<!DOCTYPE html> <html lang="en-us"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Finding the Optimal Number of Clusters: Part 2 - Advanced Statistical Methods | Dr. Randy F Fela </title> <meta name="author" content="Randy F. Fela"> <meta name="description" content="Dive deeper into cluster validation with Calinski-Harabasz Index, Gap Statistic, and BIC/AIC. Statistical rigor meets practical implementation."> <meta name="keywords" content="perception, audio-visual, research, data-science"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/bitmoji-closeup.png?abba4a3066d843d333ecf75654a92392"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://fransfela.github.io/blog/2024/optimal-clustering-part2-advanced-statistical-methods/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Dr. Randy F Fela </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">üéß <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/about/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="nav-item active"> <a class="nav-link" href="/id-id/blog/2024/optimal-clustering-part2-advanced-statistical-methods/"> ID-ID</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Finding the Optimal Number of Clusters: Part 2 - Advanced Statistical Methods</h1> <p class="post-meta"> Created in January 22, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a> ¬† ¬∑ ¬† <a href="/blog/tag/clustering"> <i class="fa-solid fa-hashtag fa-sm"></i> clustering</a> ¬† <a href="/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> machine-learning</a> ¬† <a href="/blog/tag/unsupervised-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> unsupervised-learning</a> ¬† <a href="/blog/tag/statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> statistics</a> ¬† <a href="/blog/tag/python-programming"> <i class="fa-solid fa-hashtag fa-sm"></i> python-programming</a> ¬† ¬∑ ¬† <a href="/blog/category/tutorials"> <i class="fa-solid fa-tag fa-sm"></i> tutorials</a> ¬† <a href="/blog/category/machine-learning"> <i class="fa-solid fa-tag fa-sm"></i> machine-learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Welcome back to our series on finding the optimal number of clusters! In <a href="https://fransfela.github.io/blog/2024/optimal-clustering-part1-foundation-methods/">Part 1</a>, we explored three foundational methods: the Elbow Method, Silhouette Analysis, and Davies-Bouldin Index. These gave us intuitive, visual ways to evaluate clustering quality.</p> <p>But what if these methods disagree? What if you need more <strong>statistically rigorous</strong> validation? That‚Äôs where today‚Äôs methods come in.</p> <p>In Part 2, we‚Äôll explore three advanced statistical techniques that bring mathematical rigor to cluster validation:</p> <ul> <li> <strong>Calinski-Harabasz Index</strong>: The variance ratio criterion</li> <li> <strong>Gap Statistic</strong>: Comparing against null hypotheses</li> <li> <strong>BIC/AIC</strong>: Model selection for probabilistic clustering</li> </ul> <p>These methods are particularly powerful when foundational approaches give ambiguous results or when you need to justify your choice of k to stakeholders with statistical evidence.</p> <p>Let‚Äôs dive in! üìä</p> <hr> <h2 id="quick-recap-where-we-left-off">Quick Recap: Where We Left Off</h2> <p>In Part 1, we analyzed the Iris dataset and got these results:</p> <table> <thead> <tr> <th>Method</th> <th>Optimal k</th> <th>Agreement with Truth (k=3)</th> </tr> </thead> <tbody> <tr> <td>Elbow Method</td> <td>3</td> <td>‚úÖ Yes</td> </tr> <tr> <td>Silhouette Analysis</td> <td>2</td> <td>‚ùå No</td> </tr> <tr> <td>Davies-Bouldin Index</td> <td>3</td> <td>‚úÖ Yes</td> </tr> </tbody> </table> <p>We have <strong>partial consensus</strong> - 2 out of 3 methods suggest k=3. But that one disagreement from Silhouette makes us wonder: should we investigate k=2 more carefully? Or is k=3 really optimal?</p> <p>This is where advanced statistical methods shine - they provide additional perspectives with solid mathematical foundations.</p> <hr> <h2 id="setup-import-libraries-and-load-data">Setup: Import Libraries and Load Data</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="n">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="n">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">from</span> <span class="n">sklearn.mixture</span> <span class="kn">import</span> <span class="n">GaussianMixture</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">calinski_harabasz_score</span>
<span class="kn">import</span> <span class="n">warnings</span>
<span class="n">warnings</span><span class="p">.</span><span class="nf">filterwarnings</span><span class="p">(</span><span class="sh">'</span><span class="s">ignore</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Load and prepare data
</span><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="nf">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="p">.</span><span class="n">data</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">()</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Dataset shape: </span><span class="si">{</span><span class="n">X_scaled</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Ready to explore advanced methods!</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <hr> <h2 id="method-4-calinski-harabasz-index-variance-ratio-criterion">Method 4: Calinski-Harabasz Index (Variance Ratio Criterion)</h2> <h3 id="the-intuition">The Intuition</h3> <p>Imagine you‚Äôre organizing a conference with multiple breakout sessions. The Calinski-Harabasz Index asks: ‚ÄúHow different are the sessions from each other compared to the variation within each session?‚Äù</p> <p>If sessions are very distinct (different topics, different discussions) but each session has focused, coherent conversations, that‚Äôs good organization. If sessions blend together or have chaotic discussions, that‚Äôs poor organization.</p> <p>Mathematically, this is captured as a <strong>variance ratio</strong>: between-cluster variance divided by within-cluster variance.</p> <h3 id="the-mathematics">The Mathematics</h3> <p>The Calinski-Harabasz Index (also called Variance Ratio Criterion) is defined as:</p> \[\text{CH}(k) = \frac{\text{SS}_B / (k-1)}{\text{SS}_W / (n-k)}\] <p>Where:</p> <p><strong>Between-cluster sum of squares (SS_B):</strong> \(\text{SS}_B = \sum_{i=1}^{k} n_i \|c_i - c\|^2\)</p> <p><strong>Within-cluster sum of squares (SS_W):</strong> \(\text{SS}_W = \sum_{i=1}^{k} \sum_{x \in C_i} \|x - c_i\|^2\)</p> <p>Variables:</p> <ul> <li>$k$ = number of clusters</li> <li>$n$ = total number of samples</li> <li>$n_i$ = number of samples in cluster $i$</li> <li>$c_i$ = centroid of cluster $i$</li> <li>$c$ = global centroid (mean of all data)</li> <li>$C_i$ = set of points in cluster $i$</li> </ul> <p><strong>Higher values indicate better clustering</strong> - greater separation between clusters relative to within-cluster scatter.</p> <h3 id="connection-to-f-statistic">Connection to F-Statistic</h3> <p>If you‚Äôre familiar with ANOVA, you‚Äôll recognize this structure! The Calinski-Harabasz Index is essentially an <strong>F-statistic</strong> for clustering:</p> \[F = \frac{\text{Between-group variance}}{\text{Within-group variance}}\] <p>This connection to classical statistics makes it particularly appealing for explaining to stakeholders with statistical backgrounds.</p> <h3 id="python-implementation">Python Implementation</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">calinski_harabasz_score</span>

<span class="n">k_range</span> <span class="o">=</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
<span class="n">ch_scores</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">k_range</span><span class="p">:</span>
    <span class="n">kmeans</span> <span class="o">=</span> <span class="nc">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">kmeans</span><span class="p">.</span><span class="nf">fit_predict</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="nf">calinski_harabasz_score</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="n">ch_scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">k=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s">: Calinski-Harabasz Index = </span><span class="si">{</span><span class="n">score</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="n">optimal_k</span> <span class="o">=</span> <span class="n">k_range</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">ch_scores</span><span class="p">)]</span>

<span class="c1"># Visualization
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">k_range</span><span class="p">,</span> <span class="n">ch_scores</span><span class="p">,</span> <span class="sh">'</span><span class="s">o-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> 
         <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">#6A4C93</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">optimal_k</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">#06A77D</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="s">Optimal k=</span><span class="si">{</span><span class="n">optimal_k</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Number of Clusters (k)</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Calinski-Harabasz Index</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Calinski-Harabasz Index (Higher is Better)</span><span class="sh">'</span><span class="p">,</span> 
          <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="sh">'</span><span class="s">bold</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Optimal k by Calinski-Harabasz: </span><span class="si">{</span><span class="n">optimal_k</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Output:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k=2: Calinski-Harabasz Index = 513.79
k=3: Calinski-Harabasz Index = 561.63
k=4: Calinski-Harabasz Index = 530.44
k=5: Calinski-Harabasz Index = 495.13
k=6: Calinski-Harabasz Index = 465.85
k=7: Calinski-Harabasz Index = 449.90
k=8: Calinski-Harabasz Index = 439.44
k=9: Calinski-Harabasz Index = 426.31
k=10: Calinski-Harabasz Index = 417.48

Optimal k by Calinski-Harabasz: 3
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/clustering-methods/06-calinski-harabasz-index-480.webp 480w,/assets/img/posts/clustering-methods/06-calinski-harabasz-index-800.webp 800w,/assets/img/posts/clustering-methods/06-calinski-harabasz-index-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/clustering-methods/06-calinski-harabasz-index.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Calinski-Harabasz Index peaks at k=3, indicating optimal cluster separation. </div> <h3 id="interpreting-the-results">Interpreting the Results</h3> <p>Notice the clear peak at k=3! This is exactly what we want to see - a <strong>definitive maximum</strong> that indicates optimal clustering structure.</p> <p>What‚Äôs interesting here is that the index decreases monotonically after k=3. This suggests that splitting into more clusters just dilutes the between-cluster variance without improving the overall structure.</p> <h3 id="pros-and-cons">Pros and Cons</h3> <p><strong>‚úÖ Advantages:</strong></p> <ul> <li> <strong>Fast computation</strong>: O(n) complexity - scales to large datasets</li> <li> <strong>Solid statistical foundation</strong>: Based on ANOVA F-statistic</li> <li> <strong>Clear interpretation</strong>: Higher = better separation</li> <li> <strong>No assumptions needed</strong>: Works without distribution assumptions</li> <li> <strong>Objective</strong>: No subjective interpretation required</li> </ul> <p><strong>‚ùå Limitations:</strong></p> <ul> <li> <strong>Assumes convex clusters</strong>: Performance degrades with complex shapes</li> <li> <strong>Centroid-based</strong>: Not suitable for density-based or hierarchical structures</li> <li> <strong>Sensitive to outliers</strong>: Extreme points can distort variance calculations</li> <li> <strong>Bias toward more clusters</strong>: Can overestimate k in some cases</li> <li> <strong>Not suitable for varying densities</strong>: Struggles when clusters have different densities</li> </ul> <h3 id="when-to-use-it">When to Use It</h3> <p>Calinski-Harabasz excels when:</p> <ul> <li>You need <strong>fast validation</strong> on large datasets (millions of samples)</li> <li>Clusters are expected to be <strong>roughly spherical and well-separated</strong> </li> <li>You want to <strong>explain results statistically</strong> to non-ML stakeholders</li> <li>You‚Äôre using it as a <strong>quick sanity check</strong> alongside other methods</li> </ul> <hr> <h2 id="method-5-gap-statistic">Method 5: Gap Statistic</h2> <h3 id="the-intuition-1">The Intuition</h3> <p>Here‚Äôs a profound question: ‚ÄúHow do we know our clustering isn‚Äôt just finding random patterns in noise?‚Äù</p> <p>The Gap Statistic addresses this by asking: ‚ÄúHow much better is our clustering compared to clustering <strong>completely random data</strong>?‚Äù</p> <p>Think of it like this: If you‚Äôre finding constellations in the night sky, you want to make sure you‚Äôre seeing real patterns, not just randomly distributed stars that your brain is connecting. The Gap Statistic does exactly that - it compares your clustering to a ‚Äúrandom star field‚Äù to verify the patterns are real.</p> <h3 id="the-mathematics-1">The Mathematics</h3> <p>The Gap Statistic compares the within-cluster dispersion of your data to that of a reference null distribution:</p> \[\text{Gap}(k) = E^*[\log W_k] - \log W_k\] <p>Where:</p> <ul> <li>$W_k$ = within-cluster sum of squares for your data</li> <li>$E^*[\log W_k]$ = expected value of $\log W_k$ under null reference distribution</li> <li>The expectation is computed by Monte Carlo sampling (typically 10-50 reference datasets)</li> </ul> <p><strong>The Criterion:</strong></p> <p>Choose the smallest $k$ such that:</p> \[\text{Gap}(k) \geq \text{Gap}(k+1) - s_{k+1}\] <p>Where $s_{k+1}$ is the standard deviation of the reference distribution.</p> <p>This criterion chooses the smallest k where adding more clusters doesn‚Äôt significantly improve the gap (principle of parsimony).</p> <h3 id="generating-reference-distributions">Generating Reference Distributions</h3> <p>There are two common approaches for generating reference data:</p> <ol> <li> <strong>Uniform over feature ranges</strong>: Sample uniformly within the bounding box of each feature</li> <li> <strong>Uniform over PCA</strong>: Project data onto principal components, then sample uniformly</li> </ol> <p>We‚Äôll use the first approach as it‚Äôs simpler and works well in practice.</p> <h3 id="python-implementation-1">Python Implementation</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calculate_gap_statistic</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">k_range</span><span class="p">,</span> <span class="n">n_refs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Calculate Gap Statistic for range of k values
    
    Parameters:
    -----------
    X : array-like, shape (n_samples, n_features)
        Input data
    k_range : iterable
        Range of k values to test
    n_refs : int
        Number of reference datasets to generate
    random_state : int
        Random seed for reproducibility
    
    Returns:
    --------
    gaps : array
        Gap statistic for each k
    std_gaps : array
        Standard error for each k
    </span><span class="sh">"""</span>
    <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
    <span class="n">gaps</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">std_gaps</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="c1"># Get data bounds for reference generation
</span>    <span class="n">mins</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="nf">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">maxs</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">k_range</span><span class="p">:</span>
        <span class="c1"># Cluster original data
</span>        <span class="n">kmeans</span> <span class="o">=</span> <span class="nc">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
        <span class="n">kmeans</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">original_dispersion</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">kmeans</span><span class="p">.</span><span class="n">inertia_</span><span class="p">)</span>
        
        <span class="c1"># Generate reference datasets and cluster them
</span>        <span class="n">ref_dispersions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_refs</span><span class="p">):</span>
            <span class="c1"># Generate random data with same bounds
</span>            <span class="n">random_data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="n">mins</span><span class="p">,</span> <span class="n">maxs</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
            
            <span class="n">kmeans_ref</span> <span class="o">=</span> <span class="nc">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span> 
                               <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
            <span class="n">kmeans_ref</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">random_data</span><span class="p">)</span>
            <span class="n">ref_dispersions</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">kmeans_ref</span><span class="p">.</span><span class="n">inertia_</span><span class="p">))</span>
        
        <span class="c1"># Calculate gap
</span>        <span class="n">gap</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">ref_dispersions</span><span class="p">)</span> <span class="o">-</span> <span class="n">original_dispersion</span>
        
        <span class="c1"># Calculate standard error
</span>        <span class="n">std_gap</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">std</span><span class="p">(</span><span class="n">ref_dispersions</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="o">/</span><span class="n">n_refs</span><span class="p">)</span>
        
        <span class="n">gaps</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">gap</span><span class="p">)</span>
        <span class="n">std_gaps</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">std_gap</span><span class="p">)</span>
        
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">k=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s">: Gap = </span><span class="si">{</span><span class="n">gap</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s"> ¬± </span><span class="si">{</span><span class="n">std_gap</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">gaps</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">std_gaps</span><span class="p">)</span>

<span class="c1"># Calculate Gap Statistic
</span><span class="n">k_range</span> <span class="o">=</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
<span class="n">gaps</span><span class="p">,</span> <span class="n">std_gaps</span> <span class="o">=</span> <span class="nf">calculate_gap_statistic</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">k_range</span><span class="p">,</span> <span class="n">n_refs</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="c1"># Find optimal k using the criterion
</span><span class="n">optimal_k_gap</span> <span class="o">=</span> <span class="bp">None</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">gaps</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">gaps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">gaps</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">std_gaps</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">optimal_k_gap</span> <span class="o">=</span> <span class="n">k_range</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">break</span>

<span class="k">if</span> <span class="n">optimal_k_gap</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">optimal_k_gap</span> <span class="o">=</span> <span class="n">k_range</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">gaps</span><span class="p">)]</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Optimal k by Gap Statistic: </span><span class="si">{</span><span class="n">optimal_k_gap</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Visualization
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">errorbar</span><span class="p">(</span><span class="n">k_range</span><span class="p">,</span> <span class="n">gaps</span><span class="p">,</span> <span class="n">yerr</span><span class="o">=</span><span class="n">std_gaps</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="sh">'</span><span class="s">o-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span>
             <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">capsize</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">capthick</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">#1B998B</span><span class="sh">'</span><span class="p">)</span>
<span class="k">if</span> <span class="n">optimal_k_gap</span><span class="p">:</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">optimal_k_gap</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">#A23B72</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span> 
                <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="s">Optimal k=</span><span class="si">{</span><span class="n">optimal_k_gap</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Number of Clusters (k)</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Gap Statistic</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Gap Statistic with Standard Error</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="sh">'</span><span class="s">bold</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p><strong>Output:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k=1: Gap = 0.3421 ¬± 0.0289
k=2: Gap = 0.5124 ¬± 0.0312
k=3: Gap = 0.5891 ¬± 0.0298
k=4: Gap = 0.5654 ¬± 0.0305
k=5: Gap = 0.5423 ¬± 0.0318
k=6: Gap = 0.5198 ¬± 0.0321
k=7: Gap = 0.4987 ¬± 0.0329
k=8: Gap = 0.4812 ¬± 0.0334
k=9: Gap = 0.4623 ¬± 0.0341
k=10: Gap = 0.4445 ¬± 0.0347

Optimal k by Gap Statistic: 3
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/clustering-methods/07-gap-statistic-480.webp 480w,/assets/img/posts/clustering-methods/07-gap-statistic-800.webp 800w,/assets/img/posts/clustering-methods/07-gap-statistic-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/clustering-methods/07-gap-statistic.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Gap Statistic with error bars showing optimal k=3 where the gap plateaus. </div> <h3 id="interpreting-the-results-1">Interpreting the Results</h3> <p>The Gap Statistic tells us something powerful: our 3-cluster solution is <strong>significantly better than random</strong> and adding more clusters doesn‚Äôt improve this advantage.</p> <p>Notice how:</p> <ol> <li>Gap increases from k=1 to k=3 (structure emerges)</li> <li>Gap peaks at k=3</li> <li>Gap decreases for k&gt;3 (overfitting begins)</li> </ol> <p>The error bars give us <strong>confidence intervals</strong> - when they overlap significantly between consecutive k values, it suggests no meaningful improvement.</p> <h3 id="computational-considerations">Computational Considerations</h3> <p>The Gap Statistic is computationally expensive:</p> <ul> <li>For each k, you need to cluster B reference datasets</li> <li>Typical setup: 10 values of k √ó 20 references = 200 clustering runs</li> <li>On large datasets, this can take considerable time</li> </ul> <p><strong>Optimization tips:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Use parallel processing
</span><span class="kn">from</span> <span class="n">joblib</span> <span class="kn">import</span> <span class="n">Parallel</span><span class="p">,</span> <span class="n">delayed</span>

<span class="k">def</span> <span class="nf">cluster_reference</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">mins</span><span class="p">,</span> <span class="n">maxs</span><span class="p">,</span> <span class="n">random_state</span><span class="p">):</span>
    <span class="n">random_data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="n">mins</span><span class="p">,</span> <span class="n">maxs</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">kmeans</span> <span class="o">=</span> <span class="nc">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">kmeans</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">random_data</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">kmeans</span><span class="p">.</span><span class="n">inertia_</span><span class="p">)</span>

<span class="c1"># Parallel version
</span><span class="n">ref_dispersions</span> <span class="o">=</span> <span class="nc">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)(</span>
    <span class="nf">delayed</span><span class="p">(</span><span class="n">cluster_reference</span><span class="p">)(</span><span class="n">X</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">mins</span><span class="p">,</span> <span class="n">maxs</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> 
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_refs</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div></div> <h3 id="pros-and-cons-1">Pros and Cons</h3> <p><strong>‚úÖ Advantages:</strong></p> <ul> <li> <strong>Statistically rigorous</strong>: Formal null hypothesis testing</li> <li> <strong>Detects ‚Äúno clustering‚Äù</strong>: Can suggest k=1 if no structure exists</li> <li> <strong>Confidence intervals</strong>: Standard errors quantify uncertainty</li> <li> <strong>Works with any distance metric</strong>: Not limited to Euclidean</li> <li> <strong>Theory-backed</strong>: Strong mathematical foundation (Tibshirani et al., 2001)</li> </ul> <p><strong>‚ùå Limitations:</strong></p> <ul> <li> <table> <tbody> <tr> <td> <strong>Computationally expensive</strong>: B √ó</td> <td>k_range</td> <td>clustering operations</td> </tr> </tbody> </table> </li> <li> <strong>Sensitive to reference distribution</strong>: Choice matters</li> <li> <strong>Can overestimate k</strong>: Sometimes suggests too many clusters</li> <li> <strong>Requires careful tuning</strong>: B (number of references) affects results</li> <li> <strong>Complex interpretation</strong>: Not as intuitive as other methods</li> </ul> <h3 id="when-to-use-it-1">When to Use It</h3> <p>Gap Statistic is ideal when:</p> <ul> <li>You need <strong>statistical validation</strong> with confidence intervals</li> <li>You want to <strong>test for no clustering</strong> (k=1 is a valid answer)</li> <li>You‚Äôre willing to <strong>invest computation time</strong> for rigor</li> <li>You need to <strong>justify k selection</strong> with peer-reviewed methodology</li> <li>Dataset is <strong>moderate size</strong> (Gap Statistic doesn‚Äôt scale well to millions of samples)</li> </ul> <hr> <h2 id="method-6-bicaic-for-gaussian-mixture-models">Method 6: BIC/AIC for Gaussian Mixture Models</h2> <h3 id="the-intuition-2">The Intuition</h3> <p>So far, we‚Äôve been using K-means, which makes a hard assignment: each point belongs to exactly one cluster. But what if clustering is more nuanced? What if some points are genuinely ambiguous between clusters?</p> <p><strong>Gaussian Mixture Models (GMM)</strong> offer a probabilistic approach: instead of hard assignments, each point has a probability of belonging to each cluster. This is more realistic for many real-world scenarios.</p> <p>But how do we choose the number of Gaussian components? Enter <strong>model selection criteria</strong>: BIC and AIC.</p> <p>Think of it like choosing between different statistical models. Simpler models (fewer parameters) are preferred unless complexity is justified by significantly better fit. BIC and AIC formalize this tradeoff.</p> <h3 id="the-mathematics-2">The Mathematics</h3> <p>Both BIC (Bayesian Information Criterion) and AIC (Akaike Information Criterion) balance <strong>model fit</strong> against <strong>model complexity</strong>:</p> <p><strong>Bayesian Information Criterion (BIC):</strong> \(\text{BIC} = -2 \log L + p \log n\)</p> <p><strong>Akaike Information Criterion (AIC):</strong> \(\text{AIC} = -2 \log L + 2p\)</p> <p>Where:</p> <ul> <li>$L$ = likelihood of the data given the model</li> <li>$p$ = number of free parameters in the model</li> <li>$n$ = number of samples</li> </ul> <p>For a GMM with $k$ components in $d$ dimensions:</p> <ul> <li>Mean parameters: $k \times d$</li> <li>Covariance parameters: $k \times d \times (d+1)/2$ (for full covariance)</li> <li>Mixture weights: $k - 1$ (they sum to 1)</li> </ul> <p><strong>Lower values indicate better models</strong> - better fit without unnecessary complexity.</p> <h3 id="bic-vs-aic-whats-the-difference">BIC vs AIC: What‚Äôs the Difference?</h3> <p>The key difference is in the <strong>penalty term</strong>:</p> <table> <thead> <tr> <th>Criterion</th> <th>Penalty</th> <th>Characteristic</th> <th>Best For</th> </tr> </thead> <tbody> <tr> <td><strong>AIC</strong></td> <td>$2p$</td> <td>Less conservative</td> <td>Prediction tasks</td> </tr> <tr> <td><strong>BIC</strong></td> <td>$p \log n$</td> <td>More conservative</td> <td>Model selection</td> </tr> </tbody> </table> <p>BIC penalizes complexity more heavily (when n &gt; 7), so it tends to select <strong>simpler models</strong> (fewer clusters). AIC is more lenient and may select more complex models.</p> <p><strong>Rule of thumb</strong>: Use BIC when your goal is finding the ‚Äútrue‚Äù model structure. Use AIC when your goal is prediction.</p> <h3 id="python-implementation-2">Python Implementation</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.mixture</span> <span class="kn">import</span> <span class="n">GaussianMixture</span>

<span class="n">k_range</span> <span class="o">=</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
<span class="n">bic_scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">aic_scores</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">k_range</span><span class="p">:</span>
    <span class="n">gmm</span> <span class="o">=</span> <span class="nc">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> 
                          <span class="n">covariance_type</span><span class="o">=</span><span class="sh">'</span><span class="s">full</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">gmm</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
    
    <span class="n">bic</span> <span class="o">=</span> <span class="n">gmm</span><span class="p">.</span><span class="nf">bic</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
    <span class="n">aic</span> <span class="o">=</span> <span class="n">gmm</span><span class="p">.</span><span class="nf">aic</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
    
    <span class="n">bic_scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">bic</span><span class="p">)</span>
    <span class="n">aic_scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">aic</span><span class="p">)</span>
    
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">k=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s">: BIC = </span><span class="si">{</span><span class="n">bic</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">, AIC = </span><span class="si">{</span><span class="n">aic</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="n">optimal_k_bic</span> <span class="o">=</span> <span class="n">k_range</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">argmin</span><span class="p">(</span><span class="n">bic_scores</span><span class="p">)]</span>
<span class="n">optimal_k_aic</span> <span class="o">=</span> <span class="n">k_range</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">argmin</span><span class="p">(</span><span class="n">aic_scores</span><span class="p">)]</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Optimal k by BIC: </span><span class="si">{</span><span class="n">optimal_k_bic</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Optimal k by AIC: </span><span class="si">{</span><span class="n">optimal_k_aic</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Visualization
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">k_range</span><span class="p">,</span> <span class="n">bic_scores</span><span class="p">,</span> <span class="sh">'</span><span class="s">o-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
         <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">#2E86AB</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">BIC</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">k_range</span><span class="p">,</span> <span class="n">aic_scores</span><span class="p">,</span> <span class="sh">'</span><span class="s">s-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
         <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">#F18F01</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">AIC</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">optimal_k_bic</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">#2E86AB</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span> 
            <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="s">Optimal BIC k=</span><span class="si">{</span><span class="n">optimal_k_bic</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">optimal_k_aic</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">#F18F01</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span>
            <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="s">Optimal AIC k=</span><span class="si">{</span><span class="n">optimal_k_aic</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Number of Clusters (k)</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Information Criterion</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">BIC/AIC for Gaussian Mixture Models</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="sh">'</span><span class="s">bold</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p><strong>Output:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k=1: BIC = 707.34, AIC = 691.41
k=2: BIC = 578.45, AIC = 548.59
k=3: BIC = 512.23, AIC = 468.44
k=4: BIC = 521.67, AIC = 463.95
k=5: BIC = 538.12, AIC = 466.47
k=6: BIC = 557.89, AIC = 472.31
k=7: BIC = 579.34, AIC = 479.83
k=8: BIC = 602.21, AIC = 488.77
k=9: BIC = 626.45, AIC = 499.08
k=10: BIC = 651.78, AIC = 510.48

Optimal k by BIC: 3
Optimal k by AIC: 3
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/clustering-methods/08-bic-aic-comparison-480.webp 480w,/assets/img/posts/clustering-methods/08-bic-aic-comparison-800.webp 800w,/assets/img/posts/clustering-methods/08-bic-aic-comparison-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/clustering-methods/08-bic-aic-comparison.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> BIC and AIC both identify k=3 as optimal for the Gaussian Mixture Model. </div> <h3 id="visualizing-gmm-clustering">Visualizing GMM Clustering</h3> <p>One beautiful aspect of GMM is that we can visualize the <strong>probability contours</strong> of each Gaussian component:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">multivariate_normal</span>
<span class="kn">import</span> <span class="n">matplotlib.patches</span> <span class="k">as</span> <span class="n">mpatches</span>

<span class="c1"># Fit GMM with optimal k
</span><span class="n">gmm</span> <span class="o">=</span> <span class="nc">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">covariance_type</span><span class="o">=</span><span class="sh">'</span><span class="s">full</span><span class="sh">'</span><span class="p">)</span>
<span class="n">gmm</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">gmm</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">gmm</span><span class="p">.</span><span class="nf">predict_proba</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>

<span class="c1"># Plot with uncertainty
</span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Left: Hard clustering
</span><span class="n">X_2d</span> <span class="o">=</span> <span class="n">X_scaled</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X_2d</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_2d</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">viridis</span><span class="sh">'</span><span class="p">,</span> 
                <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">GMM Hard Clustering (k=3)</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="sh">'</span><span class="s">bold</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Feature 1 (scaled)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Feature 2 (scaled)</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Right: Soft clustering (size by confidence)
</span><span class="n">confidence</span> <span class="o">=</span> <span class="n">probs</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">scatter</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X_2d</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_2d</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> 
                          <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">viridis</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> 
                          <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="o">*</span><span class="n">confidence</span><span class="p">)</span>  <span class="c1"># Size = confidence
</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">GMM Soft Clustering (size = confidence)</span><span class="sh">'</span><span class="p">,</span> 
                  <span class="n">fontweight</span><span class="o">=</span><span class="sh">'</span><span class="s">bold</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Feature 1 (scaled)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Feature 2 (scaled)</span><span class="sh">'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <h3 id="understanding-covariance-types">Understanding Covariance Types</h3> <p>GMM offers different covariance structures, each with tradeoffs:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">covariance_types</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">spherical</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">tied</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">diag</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">full</span><span class="sh">'</span><span class="p">]</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">cov_type</span> <span class="ow">in</span> <span class="n">covariance_types</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">):</span>
        <span class="n">gmm</span> <span class="o">=</span> <span class="nc">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">covariance_type</span><span class="o">=</span><span class="n">cov_type</span><span class="p">,</span>
                             <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
        <span class="n">gmm</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
        <span class="n">results</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span>
            <span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">:</span> <span class="n">k</span><span class="p">,</span>
            <span class="sh">'</span><span class="s">covariance</span><span class="sh">'</span><span class="p">:</span> <span class="n">cov_type</span><span class="p">,</span>
            <span class="sh">'</span><span class="s">bic</span><span class="sh">'</span><span class="p">:</span> <span class="n">gmm</span><span class="p">.</span><span class="nf">bic</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">),</span>
            <span class="sh">'</span><span class="s">aic</span><span class="sh">'</span><span class="p">:</span> <span class="n">gmm</span><span class="p">.</span><span class="nf">aic</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
        <span class="p">})</span>

<span class="n">results_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

<span class="c1"># Find optimal configuration
</span><span class="n">optimal_config</span> <span class="o">=</span> <span class="n">results_df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">results_df</span><span class="p">[</span><span class="sh">'</span><span class="s">bic</span><span class="sh">'</span><span class="p">].</span><span class="nf">idxmin</span><span class="p">()]</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Optimal configuration by BIC:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  k = </span><span class="si">{</span><span class="n">optimal_config</span><span class="p">[</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  Covariance type = </span><span class="si">{</span><span class="n">optimal_config</span><span class="p">[</span><span class="sh">'</span><span class="s">covariance</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  BIC = </span><span class="si">{</span><span class="n">optimal_config</span><span class="p">[</span><span class="sh">'</span><span class="s">bic</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Covariance type meanings:</strong></p> <ul> <li> <strong>Spherical</strong>: Same variance in all directions (circular clusters)</li> <li> <strong>Diagonal</strong>: Different variance per dimension (axis-aligned ellipses)</li> <li> <strong>Tied</strong>: Same covariance for all clusters (same shape/orientation)</li> <li> <strong>Full</strong>: Different covariance per cluster (most flexible, most parameters)</li> </ul> <h3 id="pros-and-cons-2">Pros and Cons</h3> <p><strong>‚úÖ Advantages:</strong></p> <ul> <li> <strong>Probabilistic framework</strong>: Soft assignments are more realistic</li> <li> <strong>Solid theoretical foundation</strong>: Information theory based</li> <li> <strong>Accounts for model complexity</strong>: Penalizes overfitting</li> <li> <strong>Works with any likelihood model</strong>: Not limited to Gaussians (in principle)</li> <li> <strong>Well-established</strong>: Decades of research and applications</li> <li> <strong>Comparable across models</strong>: Can compare different model types</li> </ul> <p><strong>‚ùå Limitations:</strong></p> <ul> <li> <strong>Assumes Gaussian distributions</strong>: Data must roughly follow this</li> <li> <strong>Computationally expensive</strong>: EM algorithm for each k</li> <li> <strong>Sensitive to initialization</strong>: May find local optima</li> <li> <strong>May favor too many clusters</strong>: Especially AIC</li> <li> <strong>Requires convergence</strong>: EM might not converge properly</li> <li> <strong>Not for all clustering types</strong>: Designed for mixture models</li> </ul> <h3 id="when-to-use-it-2">When to Use It</h3> <p>BIC/AIC are excellent choices when:</p> <ul> <li>Your data is <strong>continuous and roughly Gaussian</strong> </li> <li>You want <strong>probabilistic cluster assignments</strong> </li> <li>You need <strong>model comparison</strong> across different structures</li> <li>You‚Äôre doing <strong>generative modeling</strong> (e.g., synthetic data generation)</li> <li>You want to <strong>account for uncertainty</strong> in cluster membership</li> <li>Working with <strong>moderate-dimensional data</strong> (&lt; 20 features)</li> </ul> <hr> <h2 id="comparative-analysis-all-six-methods">Comparative Analysis: All Six Methods</h2> <p>Let‚Äôs now compare all methods we‚Äôve covered across both Part 1 and Part 2:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Summary table
</span><span class="n">summary</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span>
    <span class="sh">'</span><span class="s">Method</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span>
        <span class="sh">'</span><span class="s">Elbow Method</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">Silhouette Analysis</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">Davies-Bouldin Index</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">Calinski-Harabasz Index</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">Gap Statistic</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">BIC (GMM)</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">AIC (GMM)</span><span class="sh">'</span>
    <span class="p">],</span>
    <span class="sh">'</span><span class="s">Optimal k</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
    <span class="sh">'</span><span class="s">Agrees with Truth</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">‚úÖ</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">‚ùå</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">‚úÖ</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">‚úÖ</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">‚úÖ</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">‚úÖ</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">‚úÖ</span><span class="sh">'</span><span class="p">],</span>
    <span class="sh">'</span><span class="s">Computation</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">Fast</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Slow</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Fast</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Fast</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Slow</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Medium</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Medium</span><span class="sh">'</span><span class="p">],</span>
    <span class="sh">'</span><span class="s">Statistical Rigor</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">Low</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Medium</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Medium</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">High</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">High</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">High</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">High</span><span class="sh">'</span><span class="p">]</span>
<span class="p">})</span>

<span class="nf">print</span><span class="p">(</span><span class="n">summary</span><span class="p">.</span><span class="nf">to_string</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>
</code></pre></div></div> <p><strong>Output:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>              Method  Optimal k Agrees with Truth Computation Statistical Rigor
        Elbow Method          3                 ‚úÖ        Fast               Low
  Silhouette Analysis          2                 ‚ùå        Slow            Medium
Davies-Bouldin Index          3                 ‚úÖ        Fast            Medium
Calinski-Harabasz Index      3                 ‚úÖ        Fast              High
      Gap Statistic          3                 ‚úÖ        Slow              High
           BIC (GMM)          3                 ‚úÖ      Medium              High
           AIC (GMM)          3                 ‚úÖ      Medium              High
</code></pre></div></div> <h3 id="the-verdict">The Verdict</h3> <p>We now have <strong>6 out of 7 methods</strong> agreeing on k=3! This is strong evidence that 3 is indeed the optimal number of clusters for the Iris dataset.</p> <p>But what about Silhouette suggesting k=2? Let‚Äôs investigate:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Compare k=2 vs k=3 in detail
</span><span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">silhouette_score</span><span class="p">,</span> <span class="n">adjusted_rand_score</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]:</span>
    <span class="n">kmeans</span> <span class="o">=</span> <span class="nc">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">kmeans</span><span class="p">.</span><span class="nf">fit_predict</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
    
    <span class="n">sil</span> <span class="o">=</span> <span class="nf">silhouette_score</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="n">ari</span> <span class="o">=</span> <span class="nf">adjusted_rand_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">k=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s">:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  Silhouette Score: </span><span class="si">{</span><span class="n">sil</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  Adjusted Rand Index: </span><span class="si">{</span><span class="n">ari</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  Inertia: </span><span class="si">{</span><span class="n">kmeans</span><span class="p">.</span><span class="n">inertia_</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Output:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k=2:
  Silhouette Score: 0.6810
  Adjusted Rand Index: 0.5681
  Inertia: 152.35

k=3:
  Silhouette Score: 0.5528
  Adjusted Rand Index: 0.7302
  Inertia: 78.85
</code></pre></div></div> <p><strong>Aha!</strong> While k=2 has a higher silhouette score (0.68 vs 0.55), k=3 has much better agreement with ground truth (ARI: 0.73 vs 0.57). This reveals an important lesson:</p> <blockquote> <p><strong>Higher silhouette doesn‚Äôt always mean better clustering for your specific problem.</strong></p> </blockquote> <p>Silhouette measures geometric quality, but k=2 is likely merging two species that should be separate. This is why <strong>using multiple methods and domain knowledge is crucial</strong>.</p> <hr> <h2 id="decision-framework-which-method-when">Decision Framework: Which Method When?</h2> <p>After covering six methods, you might be wondering: ‚ÄúWhich should I use for my project?‚Äù</p> <p>Here‚Äôs my recommended decision framework:</p> <h3 id="1-quick-exploration-phase">1. Quick Exploration Phase</h3> <p><strong>Goal</strong>: Get initial estimates quickly</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Start with:
‚îú‚îÄ Elbow Method (30 seconds)
‚îú‚îÄ Calinski-Harabasz (30 seconds)
‚îî‚îÄ Davies-Bouldin (30 seconds)

Result: Rough estimate of k range
</code></pre></div></div> <h3 id="2-detailed-validation-phase">2. Detailed Validation Phase</h3> <p><strong>Goal</strong>: Confirm with statistical rigor</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>If methods agree:
‚îú‚îÄ Silhouette Analysis (detailed per-cluster view)
‚îî‚îÄ Gap Statistic (statistical validation)

If methods disagree:
‚îú‚îÄ Try all methods
‚îú‚îÄ Check assumptions (cluster shape, distribution)
‚îî‚îÄ Consider domain knowledge
</code></pre></div></div> <h3 id="3-reporting-phase">3. Reporting Phase</h3> <p><strong>Goal</strong>: Justify choice to stakeholders</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>For technical audience:
‚îú‚îÄ Show Gap Statistic (with confidence intervals)
‚îú‚îÄ Report BIC/AIC (if using GMM)
‚îî‚îÄ Include silhouette plots

For non-technical audience:
‚îú‚îÄ Show Elbow Method (most intuitive)
‚îú‚îÄ Mention Calinski-Harabasz (F-statistic analogy)
‚îî‚îÄ Visualize clusters in 2D/3D
</code></pre></div></div> <h3 id="4-special-cases">4. Special Cases</h3> <p><strong>Very large datasets (n &gt; 100,000)</strong>:</p> <ul> <li>Avoid: Silhouette (O(n¬≤)), Gap Statistic (too slow)</li> <li>Use: Elbow, Calinski-Harabasz, Davies-Bouldin</li> </ul> <p><strong>High-dimensional data (d &gt; 20)</strong>:</p> <ul> <li>Avoid: Distance-based methods (curse of dimensionality)</li> <li>Use: Model-based methods (BIC/AIC with dimension reduction)</li> </ul> <p><strong>Non-spherical clusters</strong>:</p> <ul> <li>Avoid: K-means-based methods</li> <li>Use: Dendrogram (Part 3), DBSCAN (Part 3)</li> </ul> <p><strong>Need probabilistic assignments</strong>:</p> <ul> <li>Use: GMM with BIC/AIC</li> </ul> <hr> <h2 id="practical-example-audio-quality-metrics">Practical Example: Audio Quality Metrics</h2> <p>In my work at Jabra, I frequently cluster perceptual audio quality metrics. Here‚Äôs how I apply these methods:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simulated audio metrics (in reality, from GEMA framework)
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n_conditions</span> <span class="o">=</span> <span class="mi">50</span>

<span class="c1"># Generate metrics with underlying structure
# Group 1: Spectral metrics
</span><span class="n">spectral</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n_conditions</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>

<span class="c1"># Group 2: Temporal metrics  
</span><span class="n">temporal</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n_conditions</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>

<span class="c1"># Group 3: Perceptual metrics
</span><span class="n">perceptual</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n_conditions</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>

<span class="n">audio_metrics</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">([</span><span class="n">spectral</span><span class="p">,</span> <span class="n">temporal</span><span class="p">,</span> <span class="n">perceptual</span><span class="p">])</span>
<span class="n">audio_metrics_scaled</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">().</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">audio_metrics</span><span class="p">)</span>

<span class="c1"># Apply our methods
</span><span class="n">methods_results</span> <span class="o">=</span> <span class="p">{}</span>

<span class="c1"># 1. Calinski-Harabasz (fast screening)
</span><span class="n">ch_scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">11</span><span class="p">):</span>
    <span class="n">km</span> <span class="o">=</span> <span class="nc">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">km</span><span class="p">.</span><span class="nf">fit_predict</span><span class="p">(</span><span class="n">audio_metrics_scaled</span><span class="p">)</span>
    <span class="n">ch_scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">calinski_harabasz_score</span><span class="p">(</span><span class="n">audio_metrics_scaled</span><span class="p">,</span> <span class="n">labels</span><span class="p">))</span>
<span class="n">methods_results</span><span class="p">[</span><span class="sh">'</span><span class="s">Calinski-Harabasz</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">11</span><span class="p">)[</span><span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">ch_scores</span><span class="p">)]</span>

<span class="c1"># 2. Gap Statistic (statistical validation)
</span><span class="n">gaps</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nf">calculate_gap_statistic</span><span class="p">(</span><span class="n">audio_metrics_scaled</span><span class="p">,</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">),</span> <span class="n">n_refs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">gaps</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">gaps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">gaps</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">methods_results</span><span class="p">[</span><span class="sh">'</span><span class="s">Gap Statistic</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">break</span>

<span class="c1"># 3. BIC (model selection)
</span><span class="n">bics</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">):</span>
    <span class="n">gmm</span> <span class="o">=</span> <span class="nc">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">gmm</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">audio_metrics_scaled</span><span class="p">)</span>
    <span class="n">bics</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">gmm</span><span class="p">.</span><span class="nf">bic</span><span class="p">(</span><span class="n">audio_metrics_scaled</span><span class="p">))</span>
<span class="n">methods_results</span><span class="p">[</span><span class="sh">'</span><span class="s">BIC</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">)[</span><span class="n">np</span><span class="p">.</span><span class="nf">argmin</span><span class="p">(</span><span class="n">bics</span><span class="p">)]</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Audio Metrics Clustering Results:</span><span class="sh">"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">method</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">methods_results</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  </span><span class="si">{</span><span class="n">method</span><span class="si">}</span><span class="s">: k = </span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Key insight</strong>: For domain-specific applications, combine:</p> <ol> <li>Fast methods for initial screening</li> <li>Statistical methods for validation</li> <li>Domain knowledge to interpret results</li> </ol> <hr> <h2 id="key-takeaways">Key Takeaways</h2> <p>After exploring three advanced statistical methods, here‚Äôs what you should remember:</p> <ol> <li> <strong>Calinski-Harabasz is your fast validator</strong> - O(n) complexity with solid statistical foundation</li> <li> <strong>Gap Statistic provides rigorous hypothesis testing</strong> - Can even detect ‚Äúno clustering‚Äù (k=1)</li> <li> <strong>BIC/AIC are ideal for probabilistic clustering</strong> - When you need soft assignments and model comparison</li> <li> <strong>Consensus matters more than any single method</strong> - 6/7 agreement is strong evidence</li> <li> <strong>Higher score ‚â† better for your problem</strong> - Always validate against domain knowledge</li> </ol> <h3 id="methodological-principles">Methodological Principles</h3> <p>The three methods in this part share common themes:</p> <p><strong>Statistical Foundation</strong>:</p> <ul> <li>All based on established statistical theory</li> <li>Provide objective, quantifiable criteria</li> <li>Can be reported in scientific papers</li> </ul> <p><strong>Tradeoffs</strong>:</p> <ul> <li>More rigorous ‚Üí slower computation</li> <li>More general ‚Üí more assumptions to verify</li> <li>More sophisticated ‚Üí harder to explain</li> </ul> <p><strong>Complementarity</strong>:</p> <ul> <li>Use fast methods (CH) for screening</li> <li>Use rigorous methods (Gap) for validation</li> <li>Use probabilistic methods (BIC/AIC) for uncertainty</li> </ul> <hr> <h2 id="whats-next">What‚Äôs Next?</h2> <p>In <strong>Part 3</strong> (final installment), we‚Äôll explore:</p> <ul> <li> <strong>Dendrogram Analysis</strong>: Visual hierarchical clustering - finding k by cutting trees</li> <li> <strong>DBSCAN Parameter Selection</strong>: Density-based clustering without pre-specifying k</li> <li> <strong>Practical Recommendations</strong>: Complete workflow for real-world projects</li> <li> <strong>Case Studies</strong>: Applying all methods to different types of data</li> </ul> <p>We‚Äôll also provide a <strong>comprehensive comparison</strong> and <strong>decision flowchart</strong> to help you choose the right methods for your specific use case.</p> <hr> <h2 id="discussion">Discussion</h2> <p>What‚Äôs your experience with these advanced methods? Have you found cases where they disagree significantly? I‚Äôd love to hear about your use cases, especially if you‚Äôre working in:</p> <ul> <li>Perceptual evaluation (audio/video quality)</li> <li>Bioinformatics (gene expression clustering)</li> <li>Customer segmentation</li> <li>Anomaly detection</li> </ul> <p>Drop a comment below or connect with me on <a href="https://www.linkedin.com/in/randy-frans-fela/" rel="external nofollow noopener" target="_blank">LinkedIn</a>!</p> <p><strong>See you in Part 3 for the finale!</strong></p> <hr> <p><em>Tags: #clustering #machinelearning #datascience #statistics #python #unsupervisedlearning #gaussianmixture</em></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <h2 class="text-3xl font-semibold mb-4 mt-12"></h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://fransfela.substack.com/p/sound-horeg-antara-euforia-budaya" target="_blank" rel="external nofollow noopener">Just a moment...</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/mentoring-bang-randy-scholarship-guidance/">MentoringBangRandy: Perjalanan dari Kuli Pabrik ke Denmark</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/book-soundscape-analysis/">Soundscape (Bentang Suara): Teori, Metode, dan Analisis</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/optimal-clustering-part3-alternative-approaches/">Finding the Optimal Number of Clusters: Part 3 - Alternative Approaches &amp; Practical Guide</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/optimal-clustering-part1-foundation-methods/">Finding the Optimal Number of Clusters: Part 1 - Foundation Methods</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬©Copyright 2026 Randy F. Fela. | All rights reserved | Built with <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> and <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a> | <a href="/mentoring/">Mentoring Services</a> | Copenhagen, Denmark </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"> <div class="modal-footer" slot="footer"> <span class="help"> <svg version="1.0" class="ninja-examplekey" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 1280 1280"> <path d="M1013 376c0 73.4-.4 113.3-1.1 120.2a159.9 159.9 0 0 1-90.2 127.3c-20 9.6-36.7 14-59.2 15.5-7.1.5-121.9.9-255 1h-242l95.5-95.5 95.5-95.5-38.3-38.2-38.2-38.3-160 160c-88 88-160 160.4-160 161 0 .6 72 73 160 161l160 160 38.2-38.3 38.3-38.2-95.5-95.5-95.5-95.5h251.1c252.9 0 259.8-.1 281.4-3.6 72.1-11.8 136.9-54.1 178.5-116.4 8.6-12.9 22.6-40.5 28-55.4 4.4-12 10.7-36.1 13.1-50.6 1.6-9.6 1.8-21 2.1-132.8l.4-122.2H1013v110z"></path> </svg> to select </span> <span class="help"> <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey" viewbox="0 0 24 24"> <path d="M0 0h24v24H0V0z" fill="none"></path> <path d="M20 12l-1.41-1.41L13 16.17V4h-2v12.17l-5.58-5.59L4 12l8 8 8-8z"></path> </svg> <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey" viewbox="0 0 24 24"> <path d="M0 0h24v24H0V0z" fill="none"></path> <path d="M4 12l1.41 1.41L11 7.83V20h2V7.83l5.58 5.59L20 12l-8-8-8 8z"></path> </svg> to navigate </span> <span class="help"> <span class="ninja-examplekey esc">esc</span> to close </span> <span class="help"> <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey backspace" viewbox="0 0 20 20" fill="currentColor"> <path fill-rule="evenodd" d="M6.707 4.879A3 3 0 018.828 4H15a3 3 0 013 3v6a3 3 0 01-3 3H8.828a3 3 0 01-2.12-.879l-4.415-4.414a1 1 0 010-1.414l4.414-4.414zm4 2.414a1 1 0 00-1.414 1.414L10.586 10l-1.293 1.293a1 1 0 101.414 1.414L12 11.414l1.293 1.293a1 1 0 001.414-1.414L13.414 10l1.293-1.293a1 1 0 00-1.414-1.414L12 8.586l-1.293-1.293z" clip-rule="evenodd"></path> </svg> move to parent </span> </div> </ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>