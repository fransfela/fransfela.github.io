<!DOCTYPE html> <html lang="en-us"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Finding the Optimal Number of Clusters: Part 1 - Foundation Methods | Dr. Randy F Fela </title> <meta name="author" content="Randy F. Fela"> <meta name="description" content="Master the art of determining optimal clusters with Elbow Method, Silhouette Analysis, and Davies-Bouldin Index. A practical guide with Python implementations."> <meta name="keywords" content="perception, audio-visual, research, data-science"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/bitmoji-closeup.png?abba4a3066d843d333ecf75654a92392"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://fransfela.github.io/blog/2024/optimal-clustering-part1-foundation-methods/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Dr. Randy F Fela </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">üéß <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/about/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/zettelkasten/">Zettelkasten </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="nav-item active"> <a class="nav-link" href="/id-id/blog/2024/optimal-clustering-part1-foundation-methods/"> ID-ID</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Finding the Optimal Number of Clusters: Part 1 - Foundation Methods</h1> <p class="post-meta"> Created in January 15, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a> ¬† ¬∑ ¬† <a href="/blog/tag/clustering"> <i class="fa-solid fa-hashtag fa-sm"></i> clustering</a> ¬† <a href="/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> machine-learning</a> ¬† <a href="/blog/tag/unsupervised-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> unsupervised-learning</a> ¬† <a href="/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> data-science</a> ¬† <a href="/blog/tag/python-programming"> <i class="fa-solid fa-hashtag fa-sm"></i> python-programming</a> ¬† ¬∑ ¬† <a href="/blog/category/tutorials"> <i class="fa-solid fa-tag fa-sm"></i> tutorials</a> ¬† <a href="/blog/category/machine-learning"> <i class="fa-solid fa-tag fa-sm"></i> machine-learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Clustering is one of the most fundamental techniques in unsupervised machine learning, but there‚Äôs one question that haunts every data scientist: <strong>‚ÄúHow many clusters should I use?‚Äù</strong></p> <p>Unlike supervised learning where the number of classes is predetermined, clustering requires us to make this crucial decision. Choose too few clusters, and you‚Äôll miss important patterns in your data. Choose too many, and you‚Äôll overfit, finding noise instead of signal.</p> <p>In this three-part series, I‚Äôll walk you through <strong>eight proven methods</strong> for finding the optimal number of clusters, complete with Python implementations and real-world examples. By the end, you‚Äôll have a comprehensive toolkit to confidently answer this question for your own projects.</p> <h2 id="series-overview">Series Overview</h2> <ul> <li> <strong>Part 1</strong> (this post): Foundation methods - Elbow, Silhouette, and Davies-Bouldin Index</li> <li> <strong>Part 2</strong>: Advanced statistical methods - Calinski-Harabasz, Gap Statistic, and BIC/AIC</li> <li> <strong>Part 3</strong>: Alternative approaches - Dendrogram analysis, DBSCAN, and practical recommendations</li> </ul> <p>Let‚Äôs dive in!</p> <hr> <h2 id="the-dataset-iris-for-demonstration">The Dataset: Iris for Demonstration</h2> <p>Throughout this series, we‚Äôll use the classic Iris dataset - not because it‚Äôs complex, but because it‚Äôs <strong>well-understood</strong> and allows us to validate our methods. The Iris dataset contains 150 samples of iris flowers with 4 features each, representing 3 species.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># Load data
</span><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="nf">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="p">.</span><span class="n">data</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">iris</span><span class="p">.</span><span class="n">target</span>  <span class="c1"># We have ground truth for validation
</span>
<span class="c1"># Always standardize your features for clustering!
</span><span class="n">scaler</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">()</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Dataset shape: </span><span class="si">{</span><span class="n">X_scaled</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">True number of clusters: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="n">y_true</span><span class="p">))</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Important</strong>: We know the true answer is 3 clusters, which lets us evaluate how well each method performs. In real-world scenarios, you won‚Äôt have this luxury.</p> <hr> <h2 id="method-1-the-elbow-method">Method 1: The Elbow Method</h2> <h3 id="the-intuition">The Intuition</h3> <p>The Elbow Method is probably the most intuitive approach to finding optimal clusters. The idea is simple: as you increase the number of clusters, the Within-Cluster Sum of Squares (WCSS) naturally decreases. But at some point, the improvement becomes marginal - that‚Äôs your ‚Äúelbow.‚Äù</p> <p>Think of it like this: if you‚Äôre organizing books on shelves, having more shelves (clusters) always helps a bit, but after a certain point, you‚Äôre just moving books around without meaningful organization.</p> <h3 id="how-it-works">How It Works</h3> <p>The metric we optimize is <strong>inertia</strong> (also called WCSS):</p> \[\text{WCSS}(k) = \sum_{i=1}^{k} \sum_{x \in C_i} \|x - \mu_i\|^2\] <p>Where:</p> <ul> <li>$k$ is the number of clusters</li> <li>$C_i$ is cluster $i$</li> <li>$\mu_i$ is the centroid of cluster $i$</li> <li>$|x - \mu_i|$ is the Euclidean distance</li> </ul> <h3 id="python-implementation">Python Implementation</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">from</span> <span class="n">kneed</span> <span class="kn">import</span> <span class="n">KneeLocator</span>  <span class="c1"># For automated elbow detection
</span><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">k_range</span> <span class="o">=</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
<span class="n">inertias</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">k_range</span><span class="p">:</span>
    <span class="n">kmeans</span> <span class="o">=</span> <span class="nc">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">kmeans</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
    <span class="n">inertias</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">kmeans</span><span class="p">.</span><span class="n">inertia_</span><span class="p">)</span>

<span class="c1"># Automated elbow detection
</span><span class="n">kl</span> <span class="o">=</span> <span class="nc">KneeLocator</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">k_range</span><span class="p">),</span> <span class="n">inertias</span><span class="p">,</span> <span class="n">curve</span><span class="o">=</span><span class="sh">'</span><span class="s">convex</span><span class="sh">'</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="sh">'</span><span class="s">decreasing</span><span class="sh">'</span><span class="p">)</span>
<span class="n">optimal_k</span> <span class="o">=</span> <span class="n">kl</span><span class="p">.</span><span class="n">elbow</span>

<span class="c1"># Visualization
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">k_range</span><span class="p">,</span> <span class="n">inertias</span><span class="p">,</span> <span class="sh">'</span><span class="s">o-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">optimal_k</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span> 
            <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="s">Elbow at k=</span><span class="si">{</span><span class="n">optimal_k</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Number of Clusters (k)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Inertia (WCSS)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Elbow Method for Optimal k</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Optimal k by Elbow Method: </span><span class="si">{</span><span class="n">optimal_k</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/clustering-methods/01-elbow-method-inertia-480.webp 480w,/assets/img/posts/clustering-methods/01-elbow-method-inertia-800.webp 800w,/assets/img/posts/clustering-methods/01-elbow-method-inertia-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/clustering-methods/01-elbow-method-inertia.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The Elbow Method plot showing the "elbow point" where adding more clusters yields diminishing returns. </div> <h3 id="pros-and-cons">Pros and Cons</h3> <p><strong>‚úÖ Advantages:</strong></p> <ul> <li>Extremely intuitive and easy to explain</li> <li>Computationally efficient</li> <li>Good starting point for exploration</li> <li>Works well when clusters are well-separated</li> </ul> <p><strong>‚ùå Limitations:</strong></p> <ul> <li>Subjective interpretation (elbow not always clear)</li> <li>Struggles with ambiguous cases</li> <li>Only considers within-cluster variance, not separation</li> <li>Can suggest different k values depending on data scaling</li> </ul> <h3 id="when-to-use-it">When to Use It</h3> <p>Use the Elbow Method as your <strong>first pass</strong> when exploring data. It‚Äôs excellent for getting a rough estimate, but don‚Äôt rely on it alone. Combine it with other methods for robust validation.</p> <hr> <h2 id="method-2-silhouette-analysis">Method 2: Silhouette Analysis</h2> <h3 id="the-intuition-1">The Intuition</h3> <p>While the Elbow Method only looks at compactness (how tight clusters are), Silhouette Analysis considers <strong>both compactness and separation</strong>. It asks: ‚ÄúIs each point closer to its own cluster than to neighboring clusters?‚Äù</p> <p>This is more aligned with what we intuitively want from clustering - groups that are both cohesive internally and distinct from each other.</p> <h3 id="how-it-works-1">How It Works</h3> <p>For each data point $i$, the silhouette coefficient is calculated as:</p> \[s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}\] <p>Where:</p> <ul> <li>$a(i)$ = average distance to other points in the same cluster</li> <li>$b(i)$ = average distance to points in the nearest neighboring cluster</li> </ul> <p>The silhouette score ranges from <strong>-1 to +1</strong>:</p> <ul> <li> <strong>+1</strong>: Point is very close to its cluster, far from others (ideal)</li> <li> <strong>0</strong>: Point is on the border between clusters</li> <li> <strong>-1</strong>: Point is probably in the wrong cluster</li> </ul> <h3 id="python-implementation-1">Python Implementation</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">silhouette_score</span><span class="p">,</span> <span class="n">silhouette_samples</span>

<span class="n">k_range</span> <span class="o">=</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
<span class="n">silhouette_scores</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">k_range</span><span class="p">:</span>
    <span class="n">kmeans</span> <span class="o">=</span> <span class="nc">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">kmeans</span><span class="p">.</span><span class="nf">fit_predict</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="nf">silhouette_score</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="n">silhouette_scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">k=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s">: Silhouette Score = </span><span class="si">{</span><span class="n">score</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="n">optimal_k</span> <span class="o">=</span> <span class="n">k_range</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">silhouette_scores</span><span class="p">)]</span>

<span class="c1"># Visualization
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">k_range</span><span class="p">,</span> <span class="n">silhouette_scores</span><span class="p">,</span> <span class="sh">'</span><span class="s">o-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">optimal_k</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="s">Optimal k=</span><span class="si">{</span><span class="n">optimal_k</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Number of Clusters (k)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Silhouette Score</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Silhouette Analysis: Score vs Number of Clusters</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/clustering-methods/03-silhouette-score-vs-k-480.webp 480w,/assets/img/posts/clustering-methods/03-silhouette-score-vs-k-800.webp 800w,/assets/img/posts/clustering-methods/03-silhouette-score-vs-k-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/clustering-methods/03-silhouette-score-vs-k.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Silhouette score across different k values. Higher scores indicate better-defined clusters. </div> <h3 id="detailed-silhouette-plots">Detailed Silhouette Plots</h3> <p>One of the most powerful features of silhouette analysis is the <strong>per-cluster visualization</strong>. This lets you see not just the average score, but how well-clustered each individual point is.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">matplotlib.cm</span> <span class="k">as</span> <span class="n">cm</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    
    <span class="n">kmeans</span> <span class="o">=</span> <span class="nc">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">kmeans</span><span class="p">.</span><span class="nf">fit_predict</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
    <span class="n">silhouette_vals</span> <span class="o">=</span> <span class="nf">silhouette_samples</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    
    <span class="n">y_lower</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
        <span class="c1"># Get silhouette values for cluster i
</span>        <span class="n">cluster_silhouette_vals</span> <span class="o">=</span> <span class="n">silhouette_vals</span><span class="p">[</span><span class="n">labels</span> <span class="o">==</span> <span class="n">i</span><span class="p">]</span>
        <span class="n">cluster_silhouette_vals</span><span class="p">.</span><span class="nf">sort</span><span class="p">()</span>
        
        <span class="n">size_cluster_i</span> <span class="o">=</span> <span class="n">cluster_silhouette_vals</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">y_upper</span> <span class="o">=</span> <span class="n">y_lower</span> <span class="o">+</span> <span class="n">size_cluster_i</span>
        
        <span class="n">color</span> <span class="o">=</span> <span class="n">cm</span><span class="p">.</span><span class="nf">viridis</span><span class="p">(</span><span class="nf">float</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">/</span> <span class="n">k</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="nf">fill_betweenx</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">y_lower</span><span class="p">,</span> <span class="n">y_upper</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> 
                         <span class="n">cluster_silhouette_vals</span><span class="p">,</span>
                         <span class="n">facecolor</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
        
        <span class="c1"># Label clusters
</span>        <span class="n">ax</span><span class="p">.</span><span class="nf">text</span><span class="p">(</span><span class="o">-</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">y_lower</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">size_cluster_i</span><span class="p">,</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
        <span class="n">y_lower</span> <span class="o">=</span> <span class="n">y_upper</span> <span class="o">+</span> <span class="mi">10</span>
    
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Silhouette Coefficient</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Cluster</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">k=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s">, Avg=</span><span class="si">{</span><span class="nf">silhouette_score</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
    
    <span class="c1"># Vertical line for average score
</span>    <span class="n">ax</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="nf">silhouette_score</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">labels</span><span class="p">),</span> 
               <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_yticks</span><span class="p">([])</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/clustering-methods/04-silhouette-detailed-plots-480.webp 480w,/assets/img/posts/clustering-methods/04-silhouette-detailed-plots-800.webp 800w,/assets/img/posts/clustering-methods/04-silhouette-detailed-plots-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/clustering-methods/04-silhouette-detailed-plots.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Detailed silhouette plots showing individual point clustering quality for k=2, 3, and 4. </div> <h3 id="interpreting-silhouette-plots">Interpreting Silhouette Plots</h3> <p>When analyzing these plots, look for:</p> <ol> <li> <strong>Uniform thickness</strong>: Clusters of similar size indicate balanced partitioning</li> <li> <strong>Values exceeding the average</strong>: All clusters should extend past the red dashed line</li> <li> <strong>No negative values</strong>: Points with negative silhouettes might be misclassified</li> <li> <strong>Clear separation</strong>: Visible gaps between clusters suggest good separation</li> </ol> <h3 id="pros-and-cons-1">Pros and Cons</h3> <p><strong>‚úÖ Advantages:</strong></p> <ul> <li>Considers both cohesion and separation</li> <li>Intuitive interpretation</li> <li>Provides per-point analysis (identify problematic assignments)</li> <li>Works with any distance metric</li> <li>Visual diagnostics reveal cluster quality issues</li> </ul> <p><strong>‚ùå Limitations:</strong></p> <ul> <li>Computationally expensive: O(n¬≤) for n samples</li> <li>Biased toward convex, spherical clusters</li> <li>Sensitive to noise and outliers</li> <li>May favor equal-sized clusters</li> <li>Not ideal for density-based clustering patterns</li> </ul> <h3 id="when-to-use-it-1">When to Use It</h3> <p>Silhouette Analysis is your <strong>go-to validation method</strong> when:</p> <ul> <li>You need detailed cluster quality assessment</li> <li>Dataset size is manageable (&lt; 10,000 samples)</li> <li>You want to identify problematic cluster assignments</li> <li>Clusters are expected to be reasonably spherical</li> </ul> <hr> <h2 id="method-3-davies-bouldin-index">Method 3: Davies-Bouldin Index</h2> <h3 id="the-intuition-2">The Intuition</h3> <p>The Davies-Bouldin Index (DBI) takes a different approach: it directly measures the <strong>ratio of within-cluster scatter to between-cluster separation</strong>. Think of it as asking: ‚ÄúHow much overlap is there between clusters?‚Äù</p> <p>A lower DBI means clusters are well-separated and compact - exactly what we want.</p> <h3 id="how-it-works-2">How It Works</h3> <p>For each cluster $i$, we find the cluster $j$ that‚Äôs ‚Äúmost similar‚Äù and calculate:</p> \[R_{ij} = \frac{\sigma_i + \sigma_j}{d(c_i, c_j)}\] <p>Where:</p> <ul> <li>$\sigma_i$ = average distance of points to their centroid in cluster $i$</li> <li>$d(c_i, c_j)$ = distance between centroids of clusters $i$ and $j$</li> </ul> <p>The Davies-Bouldin Index is then:</p> \[\text{DBI} = \frac{1}{k} \sum_{i=1}^{k} \max_{j \neq i} R_{ij}\] <p><strong>Lower values indicate better clustering</strong> (minimum is 0).</p> <h3 id="python-implementation-2">Python Implementation</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">davies_bouldin_score</span>

<span class="n">k_range</span> <span class="o">=</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
<span class="n">db_scores</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">k_range</span><span class="p">:</span>
    <span class="n">kmeans</span> <span class="o">=</span> <span class="nc">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">kmeans</span><span class="p">.</span><span class="nf">fit_predict</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="nf">davies_bouldin_score</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="n">db_scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">k=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s">: Davies-Bouldin Index = </span><span class="si">{</span><span class="n">score</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="n">optimal_k</span> <span class="o">=</span> <span class="n">k_range</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">argmin</span><span class="p">(</span><span class="n">db_scores</span><span class="p">)]</span>

<span class="c1"># Visualization
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">k_range</span><span class="p">,</span> <span class="n">db_scores</span><span class="p">,</span> <span class="sh">'</span><span class="s">o-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">optimal_k</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="s">Optimal k=</span><span class="si">{</span><span class="n">optimal_k</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Number of Clusters (k)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Davies-Bouldin Index</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Davies-Bouldin Index (Lower is Better)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/clustering-methods/05-davies-bouldin-index-480.webp 480w,/assets/img/posts/clustering-methods/05-davies-bouldin-index-800.webp 800w,/assets/img/posts/clustering-methods/05-davies-bouldin-index-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/clustering-methods/05-davies-bouldin-index.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Davies-Bouldin Index across different k values. The minimum indicates optimal clustering. </div> <h3 id="pros-and-cons-2">Pros and Cons</h3> <p><strong>‚úÖ Advantages:</strong></p> <ul> <li>Fast computation: O(n) complexity</li> <li>No assumptions about cluster distribution</li> <li>Intuitive: directly measures cluster quality ratio</li> <li>Penalizes both poor separation and high variance</li> <li>Easy to implement and interpret</li> </ul> <p><strong>‚ùå Limitations:</strong></p> <ul> <li>Assumes clusters are convex and isotropic</li> <li>Uses centroids (problematic for non-spherical clusters)</li> <li>Sensitive to outliers</li> <li>Struggles with varying density clusters</li> <li>No upper bound (makes cross-dataset comparison harder)</li> </ul> <h3 id="when-to-use-it-2">When to Use It</h3> <p>Davies-Bouldin Index excels as a <strong>quick validation check</strong> when:</p> <ul> <li>You need fast computation on large datasets</li> <li>Clusters are expected to be roughly spherical</li> <li>You want a simple metric to report</li> <li>Used alongside other methods for confirmation</li> </ul> <hr> <h2 id="comparative-analysis-which-method-should-you-choose">Comparative Analysis: Which Method Should You Choose?</h2> <p>Now that we‚Äôve covered three foundational methods, let‚Äôs compare them side-by-side:</p> <table> <thead> <tr> <th>Criterion</th> <th>Elbow Method</th> <th>Silhouette Analysis</th> <th>Davies-Bouldin Index</th> </tr> </thead> <tbody> <tr> <td><strong>Computation</strong></td> <td>Fast</td> <td>Slow</td> <td>Fast</td> </tr> <tr> <td><strong>Interpretability</strong></td> <td>High</td> <td>Medium</td> <td>Medium</td> </tr> <tr> <td><strong>Cluster Shape</strong></td> <td>Spherical</td> <td>Any</td> <td>Spherical</td> </tr> <tr> <td><strong>Noise Robustness</strong></td> <td>Medium</td> <td>Low</td> <td>Medium</td> </tr> <tr> <td><strong>Objectivity</strong></td> <td>Low (subjective)</td> <td>High</td> <td>High</td> </tr> <tr> <td><strong>Best Use Case</strong></td> <td>Quick exploration</td> <td>Detailed validation</td> <td>Fast validation</td> </tr> </tbody> </table> <h3 id="my-recommendation">My Recommendation</h3> <p>For <strong>most real-world projects</strong>, follow this workflow:</p> <ol> <li> <strong>Start with Elbow Method</strong> - Get a rough estimate quickly</li> <li> <strong>Validate with Silhouette</strong> - Confirm and get detailed insights</li> <li> <strong>Cross-check with Davies-Bouldin</strong> - Quick sanity check</li> </ol> <p>If all three methods agree, you can be reasonably confident. If they disagree, you‚Äôll need the advanced methods we‚Äôll cover in Part 2.</p> <hr> <h2 id="practical-example-putting-it-all-together">Practical Example: Putting It All Together</h2> <p>Let‚Äôs apply all three methods to our Iris dataset and compare results:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">silhouette_score</span><span class="p">,</span> <span class="n">davies_bouldin_score</span>
<span class="kn">from</span> <span class="n">kneed</span> <span class="kn">import</span> <span class="n">KneeLocator</span>

<span class="k">def</span> <span class="nf">find_optimal_k</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">k_range</span><span class="o">=</span><span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">11</span><span class="p">)):</span>
    <span class="sh">"""</span><span class="s">
    Find optimal k using Elbow, Silhouette, and Davies-Bouldin methods
    </span><span class="sh">"""</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">:</span> <span class="p">[],</span>
        <span class="sh">'</span><span class="s">inertia</span><span class="sh">'</span><span class="p">:</span> <span class="p">[],</span>
        <span class="sh">'</span><span class="s">silhouette</span><span class="sh">'</span><span class="p">:</span> <span class="p">[],</span>
        <span class="sh">'</span><span class="s">davies_bouldin</span><span class="sh">'</span><span class="p">:</span> <span class="p">[]</span>
    <span class="p">}</span>
    
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">k_range</span><span class="p">:</span>
        <span class="n">kmeans</span> <span class="o">=</span> <span class="nc">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">kmeans</span><span class="p">.</span><span class="nf">fit_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        
        <span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">inertia</span><span class="sh">'</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">kmeans</span><span class="p">.</span><span class="n">inertia_</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">silhouette</span><span class="sh">'</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="nf">silhouette_score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">labels</span><span class="p">))</span>
        <span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">davies_bouldin</span><span class="sh">'</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="nf">davies_bouldin_score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">labels</span><span class="p">))</span>
    
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
    
    <span class="c1"># Find optimal k for each method
</span>    <span class="n">kl</span> <span class="o">=</span> <span class="nc">KneeLocator</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">],</span> <span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">inertia</span><span class="sh">'</span><span class="p">],</span> 
                     <span class="n">curve</span><span class="o">=</span><span class="sh">'</span><span class="s">convex</span><span class="sh">'</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="sh">'</span><span class="s">decreasing</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">optimal_elbow</span> <span class="o">=</span> <span class="n">kl</span><span class="p">.</span><span class="n">elbow</span>
    <span class="n">optimal_silhouette</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">][</span><span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">silhouette</span><span class="sh">'</span><span class="p">])]</span>
    <span class="n">optimal_db</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">][</span><span class="n">np</span><span class="p">.</span><span class="nf">argmin</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">davies_bouldin</span><span class="sh">'</span><span class="p">])]</span>
    
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Optimal k by method:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  Elbow Method: k = </span><span class="si">{</span><span class="n">optimal_elbow</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  Silhouette:   k = </span><span class="si">{</span><span class="n">optimal_silhouette</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  Davies-Bouldin: k = </span><span class="si">{</span><span class="n">optimal_db</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">  True clusters: k = 3</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">df</span>

<span class="c1"># Run analysis
</span><span class="n">results_df</span> <span class="o">=</span> <span class="nf">find_optimal_k</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Output:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Optimal k by method:
  Elbow Method: k = 3
  Silhouette:   k = 2
  Davies-Bouldin: k = 3

  True clusters: k = 3
</code></pre></div></div> <p><strong>Interesting observation</strong>: Silhouette suggests k=2, while Elbow and Davies-Bouldin correctly identify k=3. This demonstrates why <strong>using multiple methods is crucial</strong> - no single method is perfect!</p> <hr> <h2 id="key-takeaways">Key Takeaways</h2> <p>After exploring these three foundational methods, here‚Äôs what you should remember:</p> <ol> <li> <strong>No silver bullet exists</strong> - Different methods may suggest different k values</li> <li> <strong>Always use multiple methods</strong> - Look for consensus across techniques</li> <li> <strong>Domain knowledge matters</strong> - Statistical methods should guide, not dictate</li> <li> <strong>Visualize, visualize, visualize</strong> - Plots reveal patterns metrics might miss</li> <li> <strong>Consider your data structure</strong> - Different methods suit different cluster shapes</li> </ol> <h3 id="whats-next">What‚Äôs Next?</h3> <p>In <strong>Part 2</strong>, we‚Äôll explore more sophisticated statistical methods:</p> <ul> <li> <strong>Calinski-Harabasz Index</strong>: Variance ratio criterion for well-separated clusters</li> <li> <strong>Gap Statistic</strong>: Comparing against random distributions</li> <li> <strong>BIC/AIC</strong>: Model selection for Gaussian Mixture Models</li> </ul> <p>These methods provide additional perspectives and can resolve ambiguities when foundational methods disagree.</p> <hr> <h2 id="questions-or-suggestions">Questions or Suggestions?</h2> <p>Found this helpful? Have questions about applying these methods to your specific use case? Drop a comment below or reach out on <a href="https://www.linkedin.com/in/randy-frans-fela?originalSubdomain=dk" rel="external nofollow noopener" target="_blank">LinkedIn</a>.</p> <p>In my day job at Jabra, I use these clustering techniques extensively for grouping perceptual audio quality metrics - if you‚Äôre working on similar problems in audio/video evaluation, I‚Äôd love to connect!</p> <p><strong>Coming up in Part 2</strong>: We‚Äôll tackle the Gap Statistic, one of the most statistically rigorous methods, and explore how BIC/AIC can help when you‚Äôre using Gaussian Mixture Models. Stay tuned! üéØ</p> <hr> <p><em>Tags: #clustering #machinelearning #datascience #python #unsupervisedlearning #statistics</em></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <h2 class="text-3xl font-semibold mb-4 mt-12"></h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://fransfela.substack.com/p/sound-horeg-antara-euforia-budaya" target="_blank" rel="external nofollow noopener">Just a moment...</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/mentoring-bang-randy-scholarship-guidance/">MentoringBangRandy: Perjalanan dari Kuli Pabrik ke Denmark</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://fransfela.substack.com/p/sound-horeg-antara-euforia-budaya" target="_blank" rel="external nofollow noopener">Sound Horeg: Antara Euforia Budaya Populer dan Degradasi Fungsi Pendengaran</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/evaluating-ai-generated-content-audio-visual-state-of-art/">Evaluating AI-Generated Content: The Challenge of Measuring What Machines Create</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/image-video-quality-metrics-reference/">Image and Video Quality Metrics: A Comprehensive Reference</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬©Copyright 2026 Randy F. Fela. | All rights reserved | Built with <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> and <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a> | <a href="/mentoring/">Mentoring Services</a> | Copenhagen, Denmark </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"> <div class="modal-footer" slot="footer"> <span class="help"> <svg version="1.0" class="ninja-examplekey" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 1280 1280"> <path d="M1013 376c0 73.4-.4 113.3-1.1 120.2a159.9 159.9 0 0 1-90.2 127.3c-20 9.6-36.7 14-59.2 15.5-7.1.5-121.9.9-255 1h-242l95.5-95.5 95.5-95.5-38.3-38.2-38.2-38.3-160 160c-88 88-160 160.4-160 161 0 .6 72 73 160 161l160 160 38.2-38.3 38.3-38.2-95.5-95.5-95.5-95.5h251.1c252.9 0 259.8-.1 281.4-3.6 72.1-11.8 136.9-54.1 178.5-116.4 8.6-12.9 22.6-40.5 28-55.4 4.4-12 10.7-36.1 13.1-50.6 1.6-9.6 1.8-21 2.1-132.8l.4-122.2H1013v110z"></path> </svg> to select </span> <span class="help"> <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey" viewbox="0 0 24 24"> <path d="M0 0h24v24H0V0z" fill="none"></path> <path d="M20 12l-1.41-1.41L13 16.17V4h-2v12.17l-5.58-5.59L4 12l8 8 8-8z"></path> </svg> <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey" viewbox="0 0 24 24"> <path d="M0 0h24v24H0V0z" fill="none"></path> <path d="M4 12l1.41 1.41L11 7.83V20h2V7.83l5.58 5.59L20 12l-8-8-8 8z"></path> </svg> to navigate </span> <span class="help"> <span class="ninja-examplekey esc">esc</span> to close </span> <span class="help"> <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey backspace" viewbox="0 0 20 20" fill="currentColor"> <path fill-rule="evenodd" d="M6.707 4.879A3 3 0 018.828 4H15a3 3 0 013 3v6a3 3 0 01-3 3H8.828a3 3 0 01-2.12-.879l-4.415-4.414a1 1 0 010-1.414l4.414-4.414zm4 2.414a1 1 0 00-1.414 1.414L10.586 10l-1.293 1.293a1 1 0 101.414 1.414L12 11.414l1.293 1.293a1 1 0 001.414-1.414L13.414 10l1.293-1.293a1 1 0 00-1.414-1.414L12 8.586l-1.293-1.293z" clip-rule="evenodd"></path> </svg> move to parent </span> </div> </ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>