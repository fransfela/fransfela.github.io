<!DOCTYPE html> <html lang="en-us"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Audio Quality Metrics: A Comprehensive Reference | Dr. Randy F Fela </title> <meta name="author" content="Randy F. Fela"> <meta name="description" content="Structured guide to objective and subjective metrics for speech, music, and audio quality evaluation across diverse applications"> <meta name="keywords" content="perception, audio-visual, research, data-science"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/bitmoji-closeup.png?abba4a3066d843d333ecf75654a92392"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://fransfela.github.io/blog/2025/audio-quality-metrics-reference/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Dr. Randy F Fela </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">üéß <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/about/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/zettelkasten/">Zettelkasten </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="nav-item active"> <a class="nav-link" href="/id-id/blog/2025/audio-quality-metrics-reference/"> ID-ID</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Audio Quality Metrics: A Comprehensive Reference</h1> <p class="post-meta"> Created in January 14, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a> ¬† ¬∑ ¬† <a href="/blog/tag/living-note"> <i class="fa-solid fa-hashtag fa-sm"></i> living-note</a> ¬† <a href="/blog/tag/auditory-perception"> <i class="fa-solid fa-hashtag fa-sm"></i> auditory-perception</a> ¬† <a href="/blog/tag/perceptual-evaluation"> <i class="fa-solid fa-hashtag fa-sm"></i> perceptual-evaluation</a> ¬† ¬∑ ¬† <a href="/blog/category/research"> <i class="fa-solid fa-tag fa-sm"></i> research</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <div class="authors"> <div class="author"> <strong>Randy Frans Fela</strong><br> <em>GN Group, Copenhagen</em><br> <a href="https://fransfela.github.io">fransfela.github.io</a> </div> </div> <hr> <h2 id="introduction">Introduction</h2> <p>Audio quality evaluation spans multiple domains, each with specialized metrics designed for specific acoustic scenarios. This living reference consolidates metrics across speech processing, music analysis, spatial audio, and environmental soundscapes.</p> <blockquote> <p><strong>Last updated:</strong> January 14, 2025<br> <strong>Status:</strong> üü¢ Actively maintained</p> </blockquote> <h2 id="speech-level-variation">Speech Level Variation</h2> <p>Metrics for assessing speech level consistency and dynamics.</p> <details> <summary><strong>Active Speech Level (ASL)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> **Description:** Measures the active speech level excluding pauses and silent segments. **How it works:** Applies voice activity detection (VAD) to isolate speech regions, then calculates RMS level of active segments. **Formula:** $$ \text{ASL} = 10 \log_{10} \left( \frac{1}{N} \sum_{i=1}^{N} x_i^2 \right) $$ where $$x_i$$ are active speech samples. **Libraries:** - Python: `pydub`, `librosa` - MATLAB: Audio Toolbox **References:** - [ITU-T P.56: Objective measurement of active speech level](https://www.itu.int/rec/T-REC-P.56) </div> </details> <details> <summary><strong>Loudness-html (ITU-R BS.1770)</strong></summary> <div style="padding:1rem; background-color:#f8f9fa; margin-top:0.5rem; border-left:3px solid #0d6efd;"> <p><strong>Description:</strong> Perceptually weighted loudness measurement for broadcast audio.</p> <p><strong>How it works:</strong> Applies K-weighting filter to approximate human loudness perception, integrates over time.</p> <p><strong>Libraries:</strong></p> <ul> <li>Python: <code>pyloudnorm</code> </li> <li>C++: <code>libebur128</code> </li> </ul> <p><strong>Datasets:</strong></p> <ul> <li><a href="https://tech.ebu.ch/publications/tech3343" rel="external nofollow noopener" target="_blank">EBU Loudness Test Set</a></li> </ul> <p><strong>References:</strong></p> <ul> <li><a href="https://www.itu.int/rec/R-REC-BS.1770" rel="external nofollow noopener" target="_blank">ITU-R BS.1770-4</a></li> </ul> </div> </details> <details> <summary><strong>Loudness-markdown (ITU-R BS.1770)</strong></summary> <div> <p><strong>Description:</strong> Perceptually weighted loudness measurement for broadcast audio.</p> <p><strong>How it works:</strong> Applies K-weighting filter to approximate human loudness perception, integrates over time.</p> <p><strong>Libraries:</strong></p> <ul> <li>Python: <code class="language-plaintext highlighter-rouge">pyloudnorm</code> </li> <li>C++: <code class="language-plaintext highlighter-rouge">libebur128</code> </li> </ul> <p><strong>Datasets:</strong> - <a href="https://tech.ebu.ch/publications/tech3343" rel="external nofollow noopener" target="_blank">EBU Loudness Test Set</a></p> <p><strong>References:</strong> - <a href="https://www.itu.int/rec/R-REC-BS.1770" rel="external nofollow noopener" target="_blank">ITU-R BS.1770-4: Algorithms to measure audio programme loudness</a></p> </div> </details> <hr> <h2 id="overall-audio-quality">Overall Audio Quality</h2> <p>Broad metrics for general audio fidelity.</p> <details> <summary><strong>Signal-to-Noise Ratio (SNR)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> **Description:** Ratio of signal power to noise power, expressed in dB. **How it works:** $$ \text{SNR} = 10 \log_{10} \left( \frac{P_{\text{signal}}}{P_{\text{noise}}} \right) $$ **Limitations:** Does not correlate well with perceptual quality. **Libraries:** - Python: `scipy.signal`, `numpy` - MATLAB: Built-in `snr()` function **References:** - [IEEE Standard for Audio Quality Measurement](https://ieeexplore.ieee.org/document/8999518) </div> </details> <details> <summary><strong>Perceptual Evaluation of Audio Quality (PEAQ)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> **Description:** ITU standard for objective audio quality measurement, designed for codec evaluation. **How it works:** Psychoacoustic model comparing reference and degraded signals across frequency bands. **Libraries:** - C: [GstPEAQ (GStreamer plugin)](https://github.com/HSU-ANT/gstpeaq) - MATLAB: Third-party implementations available **Datasets:** - [MUSHRA Test Corpus](https://www.audiolabs-erlangen.de/resources/MIR/2019-WASPAA-MUSHRA) **References:** - [ITU-R BS.1387-1: Method for objective measurements of perceived audio quality](https://www.itu.int/rec/R-REC-BS.1387) </div> </details> <hr> <h2 id="speech-quality">Speech Quality</h2> <p>Metrics specifically for telephony and VoIP.</p> <details> <summary><strong>Perceptual Evaluation of Speech Quality (PESQ)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> **Description:** ITU standard for predicting speech quality in telecom networks. **How it works:** Time-aligned comparison of reference and degraded signals through perceptual model. Output: MOS-LQO (0.5 to 4.5 scale). **Libraries:** - Python: `pesq` (via pip) - C: [Official ITU implementation](https://www.itu.int/rec/T-REC-P.862) **Datasets:** - [NOIZEUS Speech Corpus](https://ecs.utdallas.edu/loizou/speech/noizeus/) - [TIMIT Acoustic-Phonetic Corpus](https://catalog.ldc.upenn.edu/LDC93S1) **References:** - [ITU-T P.862: Perceptual evaluation of speech quality (PESQ)](https://www.itu.int/rec/T-REC-P.862) - Rix, A.W., et al. (2001). "Perceptual evaluation of speech quality (PESQ) ‚Äì a new method for speech quality assessment of telephone networks and codecs" </div> </details> <details> <summary><strong>Perceptual Objective Listening Quality Assessment (POLQA)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> **Description:** Successor to PESQ, supporting wideband and super-wideband speech. **How it works:** Advanced perceptual model with improved handling of time warping and codec artifacts. **Libraries:** - Commercial: [POLQA by OPTICOM](https://www.polqa.info/) - Python: Limited open-source implementations **References:** - [ITU-T P.863: Perceptual objective listening quality assessment](https://www.itu.int/rec/T-REC-P.863) </div> </details> <hr> <h2 id="speech-enhancement">Speech Enhancement</h2> <p>Metrics for evaluating noise suppression and enhancement algorithms.</p> <details> <summary><strong>Short-Time Objective Intelligibility (STOI)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> **Description:** Predicts speech intelligibility in noisy conditions. **How it works:** Correlates time-frequency representations of clean and processed speech. **Libraries:** - Python: `pystoi` - MATLAB: [STOI Toolbox](http://www.ceestaal.nl/code/) **Datasets:** - [LibriSpeech](https://www.openslr.org/12) - [DNS Challenge Dataset](https://github.com/microsoft/DNS-Challenge) **References:** - Taal, C.H., et al. (2011). "An Algorithm for Intelligibility Prediction of Time‚ÄìFrequency Weighted Noisy Speech" </div> </details> <details> <summary><strong>Perceptual Contrast Using Spectrograms (PCSS)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> **Description:** Measures perceptual contrast enhancement in processed speech spectrograms. **How it works:** Computes contrast ratio in time-frequency domain weighted by auditory masking. **Libraries:** - Custom implementations (research-specific) **References:** - Healy, E.W., et al. (2013). "An algorithm to increase speech intelligibility for hearing-impaired listeners" </div> </details> <details> <summary><strong>DNSMOS (Deep Noise Suppression MOS)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> **Description:** Deep learning-based predictor of subjective MOS for noise suppression systems. **How it works:** Neural network trained on large-scale listening tests to predict MOS directly from audio. **Libraries:** - Python: [DNSMOS by Microsoft](https://github.com/microsoft/DNS-Challenge/tree/master/DNSMOS) **Datasets:** - [Microsoft DNS Challenge Dataset](https://github.com/microsoft/DNS-Challenge) **References:** - Reddy, C.K.A., et al. (2021). "DNSMOS: A Non-Intrusive Perceptual Objective Speech Quality metric" </div> </details> <hr> <h2 id="speech-intelligibility">Speech Intelligibility</h2> <p>Metrics correlating with human speech understanding.</p> <details> <summary><strong>Speech Intelligibility Index (SII)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> **Description:** ANSI standard for predicting speech intelligibility based on audibility. **How it works:** Weights audible speech bands according to their importance for intelligibility. **Libraries:** - MATLAB: [SII Toolbox](https://www.mathworks.com/matlabcentral/fileexchange/51413-speech-intelligibility-index) **References:** - [ANSI S3.5-1997: Methods for Calculation of the Speech Intelligibility Index](https://webstore.ansi.org/standards/asa/ansis31997r2017) </div> </details> <details> <summary><strong>Extended Short-Time Objective Intelligibility (ESTOI)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> **Description:** Extension of STOI handling non-linear processing like spectral subtraction. **How it works:** Applies intermediate intelligibility measure to improve correlation with subjective scores. **Libraries:** - Python: `pystoi` (includes ESTOI) - MATLAB: [ESTOI Toolbox](http://www.ceestaal.nl/code/) **References:** - Jensen, J., &amp; Taal, C.H. (2016). "An Algorithm for Predicting the Intelligibility of Speech Masked by Modulated Noise Maskers" </div> </details> <hr> <h2 id="speech-in-reverberation">Speech in Reverberation</h2> <p>Metrics for reverberant environments.</p> <details> <summary><strong>Speech-to-Reverberation Modulation Energy Ratio (SRMR)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> **Description:** Non-intrusive metric estimating intelligibility degradation due to reverberation. **How it works:** Analyzes modulation spectrum energy ratio across frequency bands. **Libraries:** - MATLAB: [SRMR Toolbox](https://www.mathworks.com/matlabcentral/fileexchange/42869-srmr-toolbox) **Datasets:** - [ACE Challenge Corpus](http://www.ee.ic.ac.uk/naylor/ACEweb/) **References:** - Falk, T.H., et al. (2010). "A non-intrusive quality and intelligibility measure of reverberant and dereverberated speech" </div> </details> <hr> <h2 id="room-acoustics-quality">Room Acoustics Quality</h2> <p>Metrics related to spatial and architectural acoustics.</p> <details> <summary><strong>Reverberation Time (RT60)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> **Description:** Time for sound to decay by 60 dB after source stops. **How it works:** Measures decay slope of impulse response in frequency bands. **Libraries:** - Python: `pyroomacoustics` - MATLAB: [ITA-Toolbox](http://www.ita-toolbox.org/) **Datasets:** - [ACE Corpus](http://www.ee.ic.ac.uk/naylor/ACEweb/) - [BUT Speech@FIT Reverb Database](https://speech.fit.vutbr.cz/software/but-speech-fit-reverb-database) **References:** - [ISO 3382-1: Measurement of room acoustic parameters](https://www.iso.org/standard/40979.html) </div> </details> <details> <summary><strong>Clarity (C50, C80)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> **Description:** Ratio of early to late arriving sound energy. **How it works:** $$ C_{50} = 10 \log_{10} \left( \frac{\int_0^{50ms} p^2(t) dt}{\int_{50ms}^{\infty} p^2(t) dt} \right) $$ **Applications:** Speech clarity (C50), music clarity (C80). **Libraries:** - Python: `pyroomacoustics` **References:** - [ISO 3382-1: Measurement of room acoustic parameters](https://www.iso.org/standard/40979.html) </div> </details> <hr> <h2 id="speech-in-noise">Speech in Noise</h2> <p>Metrics for speech masked by background noise.</p> <details> <summary><strong>Hearing Aid Speech Perception Index (HASPI)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> **Description:** Predicts speech intelligibility for hearing-impaired listeners with and without hearing aids. **How it works:** Models auditory processing including hearing loss and amplification. **Libraries:** - MATLAB: [HASPI Toolbox](https://www.colorado.edu/lab/hearlab/resources) **References:** - Kates, J.M., &amp; Arehart, K.H. (2014). "The Hearing-Aid Speech Perception Index (HASPI)" </div> </details> <hr> <h2 id="blind-source-separation">Blind Source Separation</h2> <p>Metrics for evaluating source separation quality.</p> <details> <summary><strong>Signal-to-Distortion Ratio (SDR)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> **Description:** Measures separation quality as ratio of target signal to artifacts. **How it works:** Decomposes error into target distortion, interference, and noise. **Libraries:** - Python: `mir_eval.separation` **Datasets:** - [MUSDB18](https://sigsep.github.io/datasets/musdb.html) - [WSJ0-2mix](https://www.merl.com/demos/deep-clustering) **References:** - Vincent, E., et al. (2006). "Performance measurement in blind audio source separation" </div> </details> <details> <summary><strong>Scale-Invariant SDR (SI-SDR)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> **Description:** Scale-invariant version of SDR, more robust to amplitude differences. **How it works:** Projects estimated signal onto reference, computes distortion ratio. **Libraries:** - Python: `torch_mir_eval` or custom implementation **References:** - Le Roux, J., et al. (2019). "SDR ‚Äì half-baked or well done?" </div> </details> <hr> <h2 id="music-quality">Music Quality</h2> <p>Metrics for music fidelity and artifact detection.</p> <details> <summary><strong>PEAQ (Perceptual Evaluation of Audio Quality)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> **Description:** See "Overall Audio Quality" section above. </div> </details> <details> <summary><strong>ViSQOL (Virtual Speech Quality Objective Listener)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> **Description:** Perceptual quality metric for speech and audio, supporting music mode. **How it works:** Spectrogram-based similarity using neurogram representation. **Libraries:** - C++/Python: [ViSQOL by Google](https://github.com/google/visqol) **Datasets:** - Custom music test sets (check ViSQOL repo) **References:** - Hines, A., et al. (2015). "ViSQOL: an objective speech quality model" </div> </details> <hr> <h2 id="distance-based-metrics">Distance-Based Metrics</h2> <p>Metrics measuring spectral or waveform distance.</p> <details> <summary><strong>Log-Spectral Distance (LSD)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> **Description:** Euclidean distance between log-magnitude spectra. **How it works:** $$ \text{LSD} = \sqrt{ \frac{1}{K} \sum_{k=1}^{K} \left( 10 \log_{10} |X_k| - 10 \log_{10} |\hat{X}_k| \right)^2 } $$ **Libraries:** - Python: Custom with `librosa` or `numpy` **References:** - Gray, A., &amp; Markel, J. (1976). "Distance measures for speech processing" </div> </details> <details> <summary><strong>Mel-Cepstral Distortion (MCD)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> **Description:** Distance between mel-frequency cepstral coefficients (MFCCs). **How it works:** $$ \text{MCD} = \frac{10}{\ln 10} \sqrt{2 \sum_{k=1}^{K} (c_k - \hat{c}_k)^2} $$ **Applications:** Voice conversion, TTS evaluation. **Libraries:** - Python: `librosa`, `scipy` **References:** - Kubichek, R. (1993). "Mel-cepstral distance measure for objective speech quality assessment" </div> </details> <hr> <h2 id="asr--nlp-based-metrics">ASR &amp; NLP-Based Metrics</h2> <p>Metrics using automatic speech recognition and language models.</p> <details> <summary><strong>Word Error Rate (WER)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> **Description:** Percentage of word errors (substitutions, deletions, insertions) in ASR output. **How it works:** $$ \text{WER} = \frac{S + D + I}{N} \times 100\% $$ where S=substitutions, D=deletions, I=insertions, N=total words. **Libraries:** - Python: `jiwer` **Datasets:** - [LibriSpeech](https://www.openslr.org/12) - [Common Voice](https://commonvoice.mozilla.org/) **References:** - [NIST Speech Recognition Scoring Toolkit](https://www.nist.gov/itl/iad/mig/tools) </div> </details> <details> <summary><strong>BERT Score</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> **Description:** Contextual embedding similarity between reference and hypothesis transcriptions. **How it works:** Computes cosine similarity of BERT embeddings token-by-token. **Libraries:** - Python: `bert-score` **References:** - Zhang, T., et al. (2020). "BERTScore: Evaluating Text Generation with BERT" </div> </details> <hr> <h2 id="hearing-aid-metrics">Hearing Aid Metrics</h2> <p>Metrics specific to hearing aid and assistive listening device evaluation.</p> <details> <summary><strong>Hearing Aid Speech Quality Index (HASQI)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> **Description:** Predicts speech quality (not just intelligibility) for hearing aid users. **How it works:** Models auditory processing with hearing loss, computes quality along multiple dimensions. **Libraries:** - MATLAB: [HASQI Toolbox](https://www.colorado.edu/lab/hearlab/resources) **References:** - Kates, J.M., &amp; Arehart, K.H. (2010). "The Hearing-Aid Speech Quality Index (HASQI)" </div> </details> <details> <summary><strong>Binaural Intelligibility Level Difference (BILD)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> **Description:** Improvement in intelligibility from binaural vs. monaural listening. **How it works:** Compares predicted intelligibility under binaural and monaural conditions. **Applications:** Bilateral hearing aid fitting, spatial audio benefits. **References:** - Culling, J.F., et al. (2004). "The role of head-induced interaural time and level differences" </div> </details> <hr> <h2 id="soundscape-indices">Soundscape Indices</h2> <p>Metrics for environmental and ecological acoustics.</p> <details> <summary><strong>Acoustic Complexity Index (ACI)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> **Description:** Measures temporal variability in soundscapes, correlates with biodiversity. **How it works:** Computes intensity differences across adjacent time frames in frequency bands. **Libraries:** - R: `soundecology` - Python: `scikit-maad` **Datasets:** - [Xeno-canto](https://www.xeno-canto.org/) (bird recordings) - [AudioSet](https://research.google.com/audioset/) **References:** - Pieretti, N., et al. (2011). "A new methodology to infer the singing activity of an avian community" </div> </details> <details> <summary><strong>Normalized Difference Soundscape Index (NDSI)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> **Description:** Ratio of biophony (1-2 kHz) to anthrophony (1-2 kHz and 2-11 kHz). **How it works:** $$ \text{NDSI} = \frac{\text{Biophony} - \text{Anthrophony}}{\text{Biophony} + \text{Anthrophony}} $$ **Libraries:** - R: `soundecology` - Python: `scikit-maad` **References:** - Kasten, E.P., et al. (2012). "The remote environmental assessment laboratory's acoustic library" </div> </details> <details> <summary><strong>Bioacoustic Index (BI)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> **Description:** Area under the spectrum curve within frequency range of biological sounds. **How it works:** Integrates spectral energy in 2-8 kHz range (typical for birds/insects). **Libraries:** - R: `soundecology` **References:** - Boelman, N.T., et al. (2007). "Multi-trophic invasion resistance in Hawaii" </div> </details> <details> <summary><strong>Soundscape Pleasantness (ISO 12913-3)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> **Description:** Subjective assessment of soundscape quality in urban environments. **How it works:** Perceptual attributes evaluated via listening tests (pleasantness, eventfulness, etc.). **Standards:** - [ISO 12913-3: Data analysis and reporting](https://www.iso.org/standard/75267.html) **Datasets:** - [SATP Database (Soundscapes &amp; Psychophysiology)](https://zenodo.org/record/2635759) **References:** - Aletta, F., et al. (2016). "Soundscape descriptors and a conceptual framework for developing predictive soundscape models" </div> </details> <hr> <h2 id="references">References</h2> <h3 id="standards">Standards</h3> <ul> <li>ITU-T P.862: PESQ</li> <li>ITU-T P.863: POLQA</li> <li>ITU-R BS.1387: PEAQ</li> <li>ITU-R BS.1770: Loudness</li> <li>ANSI S3.5: Speech Intelligibility Index</li> <li>ISO 3382-1: Room acoustics</li> <li>ISO 12913: Soundscape assessment</li> </ul> <h3 id="key-papers">Key Papers</h3> <ul> <li>Rix, A.W., et al. (2001). ‚ÄúPerceptual evaluation of speech quality (PESQ)‚Äù</li> <li>Taal, C.H., et al. (2011). ‚ÄúAn Algorithm for Intelligibility Prediction‚Äù</li> <li>Kates, J.M., &amp; Arehart, K.H. (2014). ‚ÄúThe Hearing-Aid Speech Perception Index‚Äù</li> <li>Vincent, E., et al. (2006). ‚ÄúPerformance measurement in blind audio source separation‚Äù</li> <li>Zhang, T., et al. (2020). ‚ÄúBERTScore‚Äù</li> </ul> <h3 id="toolboxes--libraries">Toolboxes &amp; Libraries</h3> <ul> <li><a href="https://github.com/mpariente/pystoi" rel="external nofollow noopener" target="_blank">pystoi</a></li> <li><a href="https://github.com/ludlows/python-pesq" rel="external nofollow noopener" target="_blank">pesq</a></li> <li><a href="https://github.com/LCAV/pyroomacoustics" rel="external nofollow noopener" target="_blank">pyroomacoustics</a></li> <li><a href="https://github.com/craffel/mir_eval" rel="external nofollow noopener" target="_blank">mir_eval</a></li> <li><a href="https://github.com/scikit-maad/scikit-maad" rel="external nofollow noopener" target="_blank">scikit-maad</a></li> <li><a href="https://github.com/google/visqol" rel="external nofollow noopener" target="_blank">ViSQOL</a></li> <li><a href="https://github.com/microsoft/DNS-Challenge/tree/master/DNSMOS" rel="external nofollow noopener" target="_blank">DNSMOS</a></li> </ul> <hr> <p style="text-align: center; color: var(--global-text-color-light); font-size: 0.9rem; margin-top: 3rem; font-style: italic;"> Last updated: January 14, 2025<br> This is a living document. Suggestions? <a href="mailto:randyrff@gmail.com">Email me</a>. </p> </div> </article> <div id="giscus_thread" style="max-width: 1000px; margin: 0 auto;"> <br> <script defer src="/assets/js/giscus-setup.js"></script> <noscript> Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬©Copyright 2026 Randy F. Fela. | All rights reserved | Built with <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> and <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a> | <a href="/mentoring/">Mentoring Services</a> | Copenhagen, Denmark </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"> <div class="modal-footer" slot="footer"> <span class="help"> <svg version="1.0" class="ninja-examplekey" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 1280 1280"> <path d="M1013 376c0 73.4-.4 113.3-1.1 120.2a159.9 159.9 0 0 1-90.2 127.3c-20 9.6-36.7 14-59.2 15.5-7.1.5-121.9.9-255 1h-242l95.5-95.5 95.5-95.5-38.3-38.2-38.2-38.3-160 160c-88 88-160 160.4-160 161 0 .6 72 73 160 161l160 160 38.2-38.3 38.3-38.2-95.5-95.5-95.5-95.5h251.1c252.9 0 259.8-.1 281.4-3.6 72.1-11.8 136.9-54.1 178.5-116.4 8.6-12.9 22.6-40.5 28-55.4 4.4-12 10.7-36.1 13.1-50.6 1.6-9.6 1.8-21 2.1-132.8l.4-122.2H1013v110z"></path> </svg> to select </span> <span class="help"> <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey" viewbox="0 0 24 24"> <path d="M0 0h24v24H0V0z" fill="none"></path> <path d="M20 12l-1.41-1.41L13 16.17V4h-2v12.17l-5.58-5.59L4 12l8 8 8-8z"></path> </svg> <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey" viewbox="0 0 24 24"> <path d="M0 0h24v24H0V0z" fill="none"></path> <path d="M4 12l1.41 1.41L11 7.83V20h2V7.83l5.58 5.59L20 12l-8-8-8 8z"></path> </svg> to navigate </span> <span class="help"> <span class="ninja-examplekey esc">esc</span> to close </span> <span class="help"> <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey backspace" viewbox="0 0 20 20" fill="currentColor"> <path fill-rule="evenodd" d="M6.707 4.879A3 3 0 018.828 4H15a3 3 0 013 3v6a3 3 0 01-3 3H8.828a3 3 0 01-2.12-.879l-4.415-4.414a1 1 0 010-1.414l4.414-4.414zm4 2.414a1 1 0 00-1.414 1.414L10.586 10l-1.293 1.293a1 1 0 101.414 1.414L12 11.414l1.293 1.293a1 1 0 001.414-1.414L13.414 10l1.293-1.293a1 1 0 00-1.414-1.414L12 8.586l-1.293-1.293z" clip-rule="evenodd"></path> </svg> move to parent </span> </div> </ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>