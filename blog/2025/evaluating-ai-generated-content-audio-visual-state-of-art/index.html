<!DOCTYPE html> <html lang="en-us"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Evaluating AI-Generated Content: The Challenge of Measuring What Machines Create | Dr. Randy F Fela </title> <meta name="author" content="Randy F. Fela"> <meta name="description" content="A systematic overview of objective and subjective methods for evaluating AI-generated audio, images, video, and multimodal content"> <meta name="keywords" content="perception, audio-visual, research, data-science"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/bitmoji-closeup.png?abba4a3066d843d333ecf75654a92392"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://fransfela.github.io/blog/2025/evaluating-ai-generated-content-audio-visual-state-of-art/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Dr. Randy F Fela </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">üéß <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/about/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/zettelkasten/">Zettelkasten </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="nav-item active"> <a class="nav-link" href="/id-id/blog/2025/evaluating-ai-generated-content-audio-visual-state-of-art/"> ID-ID</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Evaluating AI-Generated Content: The Challenge of Measuring What Machines Create</h1> <p class="post-meta"> Created in March 14, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a> ¬† ¬∑ ¬† <a href="/blog/tag/living-note"> <i class="fa-solid fa-hashtag fa-sm"></i> living-note</a> ¬† <a href="/blog/tag/auditory-perception"> <i class="fa-solid fa-hashtag fa-sm"></i> auditory-perception</a> ¬† <a href="/blog/tag/visual-perception"> <i class="fa-solid fa-hashtag fa-sm"></i> visual-perception</a> ¬† <a href="/blog/tag/perceptual-evaluation"> <i class="fa-solid fa-hashtag fa-sm"></i> perceptual-evaluation</a> ¬† ¬∑ ¬† <a href="/blog/category/research"> <i class="fa-solid fa-tag fa-sm"></i> research</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <blockquote> <p><strong>Last updated:</strong> March 14, 2025<br> <strong>Status:</strong> üü¢ Actively maintained</p> </blockquote> <hr> <h2 id="introduction">Introduction</h2> <p>The rapid proliferation of generative AI has created an urgent need for robust evaluation frameworks. Models like DALL¬∑E, Stable Diffusion, Midjourney (images), Sora, Runway Gen-2 (video), AudioLM, MusicGen (audio), and multimodal systems such as GPT-4V are producing content at scales and fidelities previously unimaginable. Yet evaluating this content remains one of the field‚Äôs most vexing challenges.</p> <p>Traditional quality metrics, developed for compression artifacts and transmission errors, often fail when applied to generative models. A deepfake video might score perfectly on PSNR yet be perceptually uncanny. A synthesized voice could pass PESQ checks while sounding robotic to human listeners. An AI-generated image might achieve high SSIM yet contain anatomical impossibilities that any child would notice.</p> <p>This document systematically surveys the state of the art in evaluating AI-generated content across three modalities: audio, visual (images and video), and audiovisual. For each domain, we examine objective metrics, subjective protocols, validation methodologies, and the fundamental tensions between computational convenience and perceptual validity.</p> <hr> <h2 id="the-evaluation-problem-why-ai-generated-content-is-different">The Evaluation Problem: Why AI-Generated Content Is Different</h2> <p>Evaluating AI-generated content differs fundamentally from traditional quality assessment in several ways:</p> <p><strong>1. No Ground Truth Reference</strong></p> <p>Traditional metrics (PSNR, SSIM, PESQ) assume a reference signal representing ‚Äúperfect‚Äù quality. Generative models create novel content where no such reference exists. How do you measure the quality of an image that has never existed before?</p> <p><strong>2. Perceptual Plausibility Over Fidelity</strong></p> <p>Generated content must be perceptually plausible, not necessarily accurate to a specific target. A synthesized voice should sound natural, not identical to a recording. An AI-generated face should look human, not match a particular person.</p> <p><strong>3. Semantic Coherence Matters</strong></p> <p>Beyond low-level quality (sharpness, noise), generated content must be semantically coherent. A generated image of ‚Äúa cat playing piano‚Äù should contain both a cat and a piano, in plausible spatial relationship, with consistent lighting and perspective.</p> <p><strong>4. Multi-Dimensional Quality</strong></p> <p>Quality is not unidimensional. A generated video might have excellent visual fidelity but unnatural motion. A synthesized voice might be intelligible but lack emotional expressiveness. Evaluation must capture these multiple facets.</p> <hr> <h2 id="audio-evaluating-generated-speech-music-and-soundscapes">Audio: Evaluating Generated Speech, Music, and Soundscapes</h2> <h3 id="generative-models-in-audio">Generative Models in Audio</h3> <p><strong>Speech Synthesis:</strong></p> <ul> <li>VALL-E (Microsoft): Few-shot voice cloning</li> <li>Bark (Suno AI): Text-to-audio with emotion</li> <li>Tortoise TTS: High-quality but slow synthesis</li> </ul> <p><strong>Music Generation:</strong></p> <ul> <li>MusicGen (Meta): Text-to-music generation</li> <li>AudioLM (Google): Audio continuation and infilling</li> <li>Jukebox (OpenAI): Raw audio generation</li> </ul> <p><strong>General Audio:</strong></p> <ul> <li>AudioLDM: Text-to-audio diffusion</li> <li>Stable Audio: Latent diffusion for sound effects</li> </ul> <h3 id="objective-metrics-for-generated-audio">Objective Metrics for Generated Audio</h3> <details> <summary><strong>Fr√©chet Audio Distance (FAD)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Measures distributional similarity between generated and real audio in embedding space.</p> <p><strong>How it works:</strong></p> <ol> <li>Extract embeddings using pre-trained audio classifier (VGGish)</li> <li>Fit multivariate Gaussian to real and generated distributions</li> <li>Compute Fr√©chet distance between distributions</li> </ol> <p><strong>Formula:</strong></p> \[\text{FAD} = ||\mu_r - \mu_g||^2 + \text{Tr}(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2})\] <p>where Œº is mean, Œ£ is covariance.</p> <p><strong>Strengths:</strong> Captures distributional properties, correlates with perceptual quality.</p> <p><strong>Limitations:</strong> Sensitive to embedding model choice, doesn‚Äôt capture fine-grained artifacts.</p> <p><strong>Code:</strong></p> <ul> <li><a href="https://github.com/google-research/google-research/tree/master/frechet_audio_distance" rel="external nofollow noopener" target="_blank">FAD implementation (GitHub)</a></li> </ul> <p><strong>References:</strong></p> <ul> <li>Kilgour, K., et al. (2019). ‚ÄúFr√©chet Audio Distance: A Reference-Free Metric for Evaluating Music Enhancement Algorithms‚Äù</li> </ul> </div> </details> <details> <summary><strong>Kullback-Leibler (KL) Divergence</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Measures divergence between probability distributions of acoustic features.</p> <p><strong>How it works:</strong> Extracts features (e.g., MFCCs, spectral envelopes), models distributions, computes KL divergence.</p> <p><strong>Strengths:</strong> Distribution-level comparison, interpretable.</p> <p><strong>Limitations:</strong> Assumes distributional form, sensitive to feature choice.</p> <p><strong>Code:</strong></p> <ul> <li>Standard in <code class="language-plaintext highlighter-rouge">scipy.stats.entropy</code> </li> </ul> <p><strong>References:</strong></p> <ul> <li>Kullback, S., &amp; Leibler, R.A. (1951). ‚ÄúOn information and sufficiency‚Äù</li> </ul> </div> </details> <details> <summary><strong>Mel Cepstral Distortion (MCD)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Measures spectral envelope difference between generated and reference speech.</p> <p><strong>How it works:</strong> Extracts mel-frequency cepstral coefficients (MFCCs), computes Euclidean distance.</p> <p><strong>Applications:</strong> Speech synthesis evaluation, voice conversion.</p> <p><strong>Code:</strong></p> <ul> <li> <code class="language-plaintext highlighter-rouge">librosa</code>, custom implementations</li> </ul> <p><strong>References:</strong></p> <ul> <li>Kubichek, R. (1993). ‚ÄúMel-cepstral distance measure for objective speech quality assessment‚Äù</li> </ul> </div> </details> <details> <summary><strong>DNSMOS (Deep Noise Suppression MOS)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Deep learning predictor of subjective MOS for speech quality.</p> <p><strong>How it works:</strong> Neural network trained on large-scale listening tests predicts MOS directly from audio waveform.</p> <p><strong>Applications:</strong> Evaluating speech enhancement, codec quality, TTS systems.</p> <p><strong>Code:</strong></p> <ul> <li><a href="https://github.com/microsoft/DNS-Challenge/tree/master/DNSMOS" rel="external nofollow noopener" target="_blank">DNSMOS by Microsoft</a></li> </ul> <p><strong>References:</strong></p> <ul> <li>Reddy, C.K.A., et al. (2021). ‚ÄúDNSMOS: A non-intrusive perceptual objective speech quality metric‚Äù</li> </ul> </div> </details> <h3 id="subjective-evaluation-protocols-for-audio">Subjective Evaluation Protocols for Audio</h3> <details> <summary><strong>Mean Opinion Score (MOS) for TTS</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Gold standard for TTS evaluation, rating naturalness on 1-5 scale.</p> <p><strong>Protocol:</strong></p> <ol> <li>Present generated speech samples to listeners</li> <li>Rate naturalness: 1 (very unnatural) to 5 (completely natural)</li> <li>Aggregate across listeners (typically 20-50 per condition)</li> </ol> <p><strong>Best Practices:</strong></p> <ul> <li>Use balanced corpus (phonetically diverse sentences)</li> <li>Include anchor samples (known quality references)</li> <li>Screen listeners for hearing ability, language proficiency</li> </ul> <p><strong>Validation:</strong> Inter-rater reliability (Cronbach‚Äôs Œ± &gt; 0.7), correlation with other metrics.</p> <p><strong>References:</strong></p> <ul> <li><a href="https://www.itu.int/rec/T-REC-P.800" rel="external nofollow noopener" target="_blank">ITU-T P.800: Methods for subjective determination of transmission quality</a></li> </ul> </div> </details> <details> <summary><strong>MUSHRA (MUltiple Stimuli with Hidden Reference and Anchor)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Comparative evaluation protocol for subtle quality differences.</p> <p><strong>Protocol:</strong></p> <ol> <li>Present reference audio (visible)</li> <li>Present multiple test conditions simultaneously (including hidden reference and low-quality anchor)</li> <li>Listeners rate each on 0-100 scale relative to reference</li> </ol> <p><strong>Applications:</strong> Music generation, audio codec comparison, enhancement algorithm evaluation.</p> <p><strong>Best Practices:</strong></p> <ul> <li>Hidden reference must score near 100 (validates listener attentiveness)</li> <li>Low anchor must score significantly lower (validates discrimination)</li> </ul> <p><strong>References:</strong></p> <ul> <li><a href="https://www.itu.int/rec/R-REC-BS.1534" rel="external nofollow noopener" target="_blank">ITU-R BS.1534-3: Method for the subjective assessment of intermediate quality level</a></li> </ul> </div> </details> <details> <summary><strong>Listening Test Design Considerations</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Sample Duration:</strong> 3-10 seconds for speech, longer for music (avoid listener fatigue).</p> <p><strong>Randomization:</strong> Counterbalance presentation order to mitigate bias.</p> <p><strong>Training Phase:</strong> Familiarize listeners with scale anchors before main test.</p> <p><strong>Listener Pool:</strong> Domain experts vs. naive listeners (depends on evaluation goal).</p> <p><strong>Environmental Control:</strong> Calibrated playback system, quiet listening environment.</p> <p><strong>References:</strong></p> <ul> <li><a href="https://www.itu.int/rec/R-REC-BS.1116" rel="external nofollow noopener" target="_blank">ITU-R BS.1116-3: Methods for subjective assessment of small impairments</a></li> </ul> </div> </details> <h3 id="validation-objective-subjective-correlation">Validation: Objective-Subjective Correlation</h3> <p><strong>Challenge:</strong> Do objective metrics predict human perception?</p> <p><strong>Methodology:</strong></p> <ol> <li>Conduct large-scale subjective study (collect MOS ratings)</li> <li>Compute objective metrics on same stimuli</li> <li>Calculate correlation (Pearson, Spearman, Kendall‚Äôs œÑ)</li> </ol> <p><strong>Benchmark Results:</strong></p> <ul> <li>FAD correlation with MOS: r ‚âà 0.65-0.75 (moderate-strong)</li> <li>DNSMOS correlation: r ‚âà 0.85-0.95 (very strong, by design)</li> <li>MCD correlation: r ‚âà 0.50-0.60 (moderate, limited by spectral focus)</li> </ul> <p><strong>Datasets for Validation:</strong></p> <ul> <li><a href="https://voicemos-challenge-2022.github.io/" rel="external nofollow noopener" target="_blank">VCC (Voice Conversion Challenge)</a></li> <li><a href="https://sites.google.com/view/voicemos-challenge" rel="external nofollow noopener" target="_blank">VoiceMOS Challenge Dataset</a></li> </ul> <hr> <h2 id="visual-evaluating-generated-images-and-video">Visual: Evaluating Generated Images and Video</h2> <h3 id="generative-models-in-visual-domain">Generative Models in Visual Domain</h3> <p><strong>Image Generation:</strong></p> <ul> <li>DALL¬∑E 3 (OpenAI): Text-to-image with prompt adherence</li> <li>Stable Diffusion: Open-source latent diffusion</li> <li>Midjourney: Aesthetic-focused generation</li> </ul> <p><strong>Video Generation:</strong></p> <ul> <li>Sora (OpenAI): Long-form video from text</li> <li>Runway Gen-2: Text/image-to-video</li> <li>Pika Labs: Controllable video synthesis</li> </ul> <h3 id="objective-metrics-for-generated-images">Objective Metrics for Generated Images</h3> <details> <summary><strong>Fr√©chet Inception Distance (FID)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Most widely used metric for generative image models, measuring distributional distance in Inception-v3 feature space.</p> <p><strong>How it works:</strong></p> <ol> <li>Extract features from pre-trained Inception-v3 network</li> <li>Fit Gaussian to real and generated image distributions</li> <li>Compute Fr√©chet distance</li> </ol> <p><strong>Strengths:</strong> Captures both quality and diversity, widely adopted benchmark.</p> <p><strong>Limitations:</strong></p> <ul> <li>Biased toward ImageNet-like images (Inception trained on ImageNet)</li> <li>Can be ‚Äúfooled‚Äù by memorization (mode collapse may lower FID)</li> <li>Sensitive to sample size (requires ~50k samples for stable estimate)</li> </ul> <p><strong>Code:</strong></p> <ul> <li><a href="https://github.com/mseitzer/pytorch-fid" rel="external nofollow noopener" target="_blank">pytorch-fid</a></li> <li><a href="https://github.com/bioinf-jku/TTUR" rel="external nofollow noopener" target="_blank">TensorFlow implementation</a></li> </ul> <p><strong>References:</strong></p> <ul> <li>Heusel, M., et al. (2017). ‚ÄúGANs trained by a two time-scale update rule converge to a local Nash equilibrium‚Äù</li> </ul> </div> </details> <details> <summary><strong>Inception Score (IS)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Measures quality and diversity of generated images using Inception-v3 classifier.</p> <p><strong>How it works:</strong></p> \[\text{IS} = \exp(\mathbb{E}_x [D_{KL}(p(y|x) || p(y))])\] <table> <tbody> <tr> <td>where p(y</td> <td>x) is conditional class distribution, p(y) is marginal.</td> </tr> </tbody> </table> <p><strong>Interpretation:</strong> High IS means images are confidently classified (quality) and cover many classes (diversity).</p> <p><strong>Limitations:</strong></p> <ul> <li>Only measures ImageNet-like semantic content</li> <li>Doesn‚Äôt account for intra-class diversity</li> <li>Can be gamed by generating one perfect image per class</li> </ul> <p><strong>Code:</strong></p> <ul> <li><a href="https://github.com/sbarratt/inception-score-pytorch" rel="external nofollow noopener" target="_blank">pytorch-fid</a></li> </ul> <p><strong>References:</strong></p> <ul> <li>Salimans, T., et al. (2016). ‚ÄúImproved techniques for training GANs‚Äù</li> </ul> </div> </details> <details> <summary><strong>CLIP Score</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Measures semantic alignment between image and text caption using CLIP embeddings.</p> <p><strong>How it works:</strong> Computes cosine similarity between CLIP image and text embeddings.</p> <p><strong>Applications:</strong> Text-to-image evaluation (DALL¬∑E, Stable Diffusion).</p> <p><strong>Strengths:</strong> Directly measures prompt adherence, language-agnostic via CLIP‚Äôs multilingual training.</p> <p><strong>Limitations:</strong> Doesn‚Äôt capture aesthetic quality, can be high for semantically correct but ugly images.</p> <p><strong>Code:</strong></p> <ul> <li><a href="https://github.com/openai/CLIP" rel="external nofollow noopener" target="_blank">CLIP by OpenAI</a></li> <li><a href="https://torchmetrics.readthedocs.io/en/stable/multimodal/clip_score.html" rel="external nofollow noopener" target="_blank">torchmetrics CLIPScore</a></li> </ul> <p><strong>References:</strong></p> <ul> <li>Hessel, J., et al. (2021). ‚ÄúCLIPScore: A reference-free evaluation metric for image captioning‚Äù</li> </ul> </div> </details> <details> <summary><strong>Aesthetic Predictor (LAION Aesthetics)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> CLIP-based model trained to predict human aesthetic ratings.</p> <p><strong>How it works:</strong> Linear probe on CLIP embeddings trained on aesthetic ratings from simulacrum-aesthetic-captions dataset.</p> <p><strong>Applications:</strong> Filtering training data, aesthetic quality assessment for generative models.</p> <p><strong>Code:</strong></p> <ul> <li><a href="https://github.com/LAION-AI/aesthetic-predictor" rel="external nofollow noopener" target="_blank">LAION Aesthetics Predictor</a></li> </ul> <p><strong>References:</strong></p> <ul> <li><a href="https://laion.ai/blog/laion-aesthetics/" rel="external nofollow noopener" target="_blank">LAION Aesthetics Dataset</a></li> </ul> </div> </details> <h3 id="objective-metrics-for-generated-video">Objective Metrics for Generated Video</h3> <details> <summary><strong>Fr√©chet Video Distance (FVD)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Extension of FID to video domain, using I3D (Inflated 3D ConvNet) features.</p> <p><strong>How it works:</strong> Extracts spatio-temporal features from I3D network pre-trained on Kinetics, computes Fr√©chet distance.</p> <p><strong>Applications:</strong> Video generation evaluation (Sora, Gen-2, etc.).</p> <p><strong>Strengths:</strong> Captures temporal dynamics, widely adopted for video GANs.</p> <p><strong>Limitations:</strong> Biased toward action-heavy videos (I3D trained on Kinetics actions).</p> <p><strong>Code:</strong></p> <ul> <li><a href="https://github.com/universome/stylegan-v" rel="external nofollow noopener" target="_blank">StyleGAN-V FVD implementation</a></li> </ul> <p><strong>References:</strong></p> <ul> <li>Unterthiner, T., et al. (2018). ‚ÄúTowards accurate generative models of video‚Äù</li> </ul> </div> </details> <details> <summary><strong>Temporal Coherence Metrics</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Measures frame-to-frame consistency (flicker, jitter).</p> <p><strong>Methods:</strong></p> <ul> <li> <strong>Optical Flow Smoothness:</strong> Computes optical flow between consecutive frames, measures magnitude variance (lower = more coherent).</li> <li> <strong>Temporal SSIM:</strong> Applies SSIM along temporal dimension.</li> </ul> <p><strong>Applications:</strong> Detecting video generation artifacts (flickering objects, unstable backgrounds).</p> <p><strong>Code:</strong></p> <ul> <li><a href="https://github.com/princeton-vl/RAFT" rel="external nofollow noopener" target="_blank">RAFT optical flow</a></li> </ul> <p><strong>References:</strong></p> <ul> <li>Lai, W.S., et al. (2018). ‚ÄúLearning blind video temporal consistency‚Äù</li> </ul> </div> </details> <h3 id="subjective-evaluation-for-images-and-video">Subjective Evaluation for Images and Video</h3> <details> <summary><strong>Two-Alternative Forced Choice (2AFC)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Pairwise comparison protocol presenting two images/videos, asking which is better.</p> <p><strong>Protocol:</strong></p> <ol> <li>Show image A and image B side-by-side</li> <li>Ask: ‚ÄúWhich image looks more realistic/aesthetic?‚Äù (forced choice)</li> <li>Aggregate preference rates across comparisons</li> </ol> <p><strong>Advantages:</strong> Simple task, high inter-rater agreement, works for AMT/crowdsourcing.</p> <p><strong>Limitations:</strong> Doesn‚Äôt provide absolute quality scores, requires many comparisons for ranking.</p> <p><strong>Analysis:</strong> Elo ratings, Bradley-Terry model to derive relative rankings.</p> <p><strong>References:</strong></p> <ul> <li>Kirstain, Y., et al. (2023). ‚ÄúPick-a-Pic: An open dataset of user preferences for text-to-image generation‚Äù</li> </ul> </div> </details> <details> <summary><strong>Human Evaluation on Naturalness/Realism</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Direct rating of perceptual realism.</p> <p><strong>Protocol:</strong></p> <ol> <li>Present generated image/video</li> <li>Ask: ‚ÄúHow realistic does this image appear?‚Äù (Likert scale 1-7 or 1-10)</li> <li>Aggregate ratings</li> </ol> <p><strong>Best Practices:</strong></p> <ul> <li>Include real images as controls (to calibrate rater sensitivity)</li> <li>Balanced presentation of real vs. generated (avoid response bias)</li> <li>Ask specific questions: realism, aesthetic quality, semantic coherence</li> </ul> <p><strong>Datasets:</strong></p> <ul> <li><a href="https://github.com/google-research/hype" rel="external nofollow noopener" target="_blank">HYPE (Human eYe Perceptual Evaluation)</a></li> </ul> <p><strong>References:</strong></p> <ul> <li>Zhou, S., et al. (2019). ‚ÄúHYPE: A benchmark for human eye perceptual evaluation of generative models‚Äù</li> </ul> </div> </details> <details> <summary><strong>Prompt Adherence Evaluation</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Evaluates whether generated content matches text prompt.</p> <p><strong>Protocol:</strong></p> <ol> <li>Show generated image + text prompt</li> <li>Ask: ‚ÄúDoes this image accurately represent the prompt?‚Äù (Yes/No or Likert scale)</li> <li>Optionally: ‚ÄúWhich elements are missing/incorrect?‚Äù</li> </ol> <p><strong>Applications:</strong> Text-to-image model evaluation (DALL¬∑E, Midjourney).</p> <p><strong>Validation:</strong> Compare with CLIP Score (objective proxy).</p> <p><strong>References:</strong></p> <ul> <li>Cho, J., et al. (2023). ‚ÄúDall-eval: Probing the reasoning skills and social biases of text-to-image generation models‚Äù</li> </ul> </div> </details> <h3 id="validation-datasets">Validation Datasets</h3> <p><strong>Image Generation Benchmarks:</strong></p> <ul> <li> <a href="https://cocodataset.org/#captions-2015" rel="external nofollow noopener" target="_blank">COCO Captions</a>: Caption-to-image</li> <li> <a href="https://github.com/google-research/parti" rel="external nofollow noopener" target="_blank">DrawBench</a>: Challenging prompts for text-to-image</li> <li> <a href="https://github.com/google-research/hype" rel="external nofollow noopener" target="_blank">HYPE Dataset</a>: Human evaluation benchmark</li> </ul> <p><strong>Video Generation Benchmarks:</strong></p> <ul> <li> <a href="https://www.crcv.ucf.edu/data/UCF101.php" rel="external nofollow noopener" target="_blank">UCF-101</a>: Action recognition dataset (used for FVD)</li> <li> <a href="https://www.deepmind.com/open-source/kinetics" rel="external nofollow noopener" target="_blank">Kinetics</a>: Large-scale video dataset</li> </ul> <hr> <h2 id="audiovisual-evaluating-multimodal-generated-content">Audiovisual: Evaluating Multimodal Generated Content</h2> <h3 id="generative-models-in-audiovisual-domain">Generative Models in Audiovisual Domain</h3> <p><strong>Talking Head Synthesis:</strong></p> <ul> <li>SadTalker: Audio-driven facial animation</li> <li>Wav2Lip: Lip-syncing to arbitrary audio</li> </ul> <p><strong>Text-to-Video with Audio:</strong></p> <ul> <li>VideoPoet (Google): Multimodal video generation</li> <li>Make-A-Video (Meta): Text-to-video with audio</li> </ul> <h3 id="objective-metrics-for-audiovisual-content">Objective Metrics for Audiovisual Content</h3> <details> <summary><strong>Audio-Visual Synchronization (Lip Sync Error)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Measures temporal alignment between audio speech and lip movements.</p> <p><strong>Methods:</strong></p> <ul> <li> <strong>SyncNet:</strong> Pre-trained network detecting sync/async pairs, outputs confidence score</li> <li> <strong>Landmark-Based:</strong> Extracts lip landmarks, correlates with audio envelope</li> </ul> <p><strong>Applications:</strong> Talking head evaluation, dubbing quality assessment.</p> <p><strong>Code:</strong></p> <ul> <li><a href="https://github.com/joonson/syncnet_python" rel="external nofollow noopener" target="_blank">SyncNet</a></li> </ul> <p><strong>References:</strong></p> <ul> <li>Chung, J.S., &amp; Zisserman, A. (2016). ‚ÄúOut of time: automated lip sync in the wild‚Äù</li> </ul> </div> </details> <details> <summary><strong>Semantic Audio-Visual Alignment</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Measures whether audio and visual content are semantically consistent.</p> <p><strong>Methods:</strong></p> <ul> <li> <strong>CLIP-based:</strong> Compute cosine similarity between CLIP audio and visual embeddings</li> <li> <strong>Cross-modal retrieval:</strong> Audio-to-video retrieval accuracy as proxy for alignment</li> </ul> <p><strong>Applications:</strong> Generated video-with-audio evaluation.</p> <p><strong>Code:</strong></p> <ul> <li><a href="https://github.com/AndreyGuzhov/AudioCLIP" rel="external nofollow noopener" target="_blank">AudioCLIP</a></li> </ul> <p><strong>References:</strong></p> <ul> <li>Guzhov, A., et al. (2021). ‚ÄúAudioCLIP: Extending CLIP to image, text and audio‚Äù</li> </ul> </div> </details> <h3 id="subjective-evaluation-for-audiovisual-content">Subjective Evaluation for Audiovisual Content</h3> <details> <summary><strong>Quality of Experience (QoE) for Talking Heads</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Holistic quality assessment considering realism, sync, and naturalness.</p> <p><strong>Protocol:</strong></p> <ol> <li>Present talking head video</li> <li>Rate dimensions independently: <ul> <li>Visual realism (1-5)</li> <li>Audio quality (1-5)</li> <li>Lip sync accuracy (1-5)</li> <li>Overall naturalness (1-5)</li> </ul> </li> </ol> <p><strong>Best Practices:</strong> Use real videos as anchors, balanced gender/ethnicity in stimuli.</p> <p><strong>References:</strong></p> <ul> <li><a href="https://www.itu.int/rec/T-REC-P.910" rel="external nofollow noopener" target="_blank">ITU-T P.910: Subjective video quality assessment methods</a></li> </ul> </div> </details> <hr> <h2 id="cross-cutting-challenges">Cross-Cutting Challenges</h2> <h3 id="the-perception-distortion-tradeoff">The Perception-Distortion Tradeoff</h3> <p><strong>Problem:</strong> High perceptual quality (realism) often comes at cost of distortion (deviation from reference).</p> <p><strong>Example:</strong> Super-resolution models producing sharp but hallucinated details score well perceptually but poorly on PSNR.</p> <p><strong>Implication:</strong> Need separate metrics for fidelity vs. perceptual quality.</p> <p><strong>References:</strong></p> <ul> <li>Blau, Y., &amp; Michaeli, T. (2018). ‚ÄúThe perception-distortion tradeoff‚Äù</li> </ul> <h3 id="adversarial-examples-and-metric-gaming">Adversarial Examples and Metric Gaming</h3> <p><strong>Problem:</strong> Generative models can be optimized to ‚Äúcheat‚Äù metrics.</p> <p><strong>Examples:</strong></p> <ul> <li>GAN trained to maximize IS by memorizing one perfect image per class</li> <li>Text-to-image model overfitting to CLIP Score</li> </ul> <p><strong>Mitigation:</strong> Use diverse metric suite, emphasize human evaluation for final validation.</p> <h3 id="bias-and-fairness">Bias and Fairness</h3> <p><strong>Problem:</strong> Evaluation datasets and metrics may encode demographic biases.</p> <p><strong>Examples:</strong></p> <ul> <li>FID biased toward Western/ImageNet aesthetics</li> <li>Speech quality models trained predominantly on English</li> </ul> <p><strong>Mitigation:</strong> Diverse evaluation datasets, stratified human studies, fairness-aware metrics.</p> <p><strong>References:</strong></p> <ul> <li>Wang, A., et al. (2023). ‚ÄúMeasuring and mitigating bias in text-to-image models‚Äù</li> </ul> <hr> <h2 id="best-practices-for-evaluation">Best Practices for Evaluation</h2> <h3 id="objective-evaluation">Objective Evaluation</h3> <ol> <li> <strong>Use Multiple Metrics:</strong> No single metric captures all quality dimensions</li> <li> <strong>Report Confidence Intervals:</strong> Especially for small sample sizes</li> <li> <strong>Validate Against Human Perception:</strong> Establish objective-subjective correlation</li> <li> <strong>Consider Task-Specific Metrics:</strong> TTS needs different metrics than music generation</li> </ol> <h3 id="subjective-evaluation">Subjective Evaluation</h3> <ol> <li> <strong>Pre-Register Studies:</strong> Define protocols before data collection (avoid p-hacking)</li> <li> <strong>Power Analysis:</strong> Ensure sufficient sample size for statistical significance</li> <li> <strong>Balanced Stimuli:</strong> Control for confounds (content, duration, order effects)</li> <li> <strong>Transparent Reporting:</strong> Include listener demographics, environment, equipment</li> </ol> <h3 id="combined-approach">Combined Approach</h3> <p><strong>Gold Standard:</strong> Objective metrics for rapid iteration + subjective studies for final validation.</p> <p><strong>Workflow:</strong></p> <ol> <li>Development: Optimize objective metrics (FID, FAD, etc.)</li> <li>Milestone Evaluation: Run subjective studies on key checkpoints</li> <li>Final Validation: Comprehensive human evaluation before deployment</li> </ol> <hr> <h2 id="resources-and-tools">Resources and Tools</h2> <h3 id="evaluation-toolkits">Evaluation Toolkits</h3> <ul> <li> <a href="https://torchmetrics.readthedocs.io/" rel="external nofollow noopener" target="_blank">torchmetrics</a>: PyTorch metrics library (FID, IS, CLIP Score, etc.)</li> <li> <a href="https://github.com/mseitzer/pytorch-fid" rel="external nofollow noopener" target="_blank">pytorch-fid</a>: Standard FID implementation</li> <li> <a href="https://github.com/GaParmar/clean-fid" rel="external nofollow noopener" target="_blank">cleanfid</a>: Improved FID with better preprocessing</li> <li> <a href="https://github.com/openai/CLIP" rel="external nofollow noopener" target="_blank">CLIP</a>: Multimodal embeddings for semantic evaluation</li> </ul> <h3 id="subjective-study-platforms">Subjective Study Platforms</h3> <ul> <li> <a href="https://www.mturk.com/" rel="external nofollow noopener" target="_blank">Amazon Mechanical Turk</a>: Crowdsourced evaluations</li> <li> <a href="https://www.prolific.co/" rel="external nofollow noopener" target="_blank">Prolific</a>: Higher-quality participant pool</li> <li> <a href="https://github.com/HSU-ANT/beaqlejs" rel="external nofollow noopener" target="_blank">BeaqleJS</a>: Browser-based listening test framework</li> <li> <a href="https://github.com/audiolabs/webMUSHRA" rel="external nofollow noopener" target="_blank">WebMUSHRA</a>: Online MUSHRA implementation</li> </ul> <h3 id="datasets-for-validation">Datasets for Validation</h3> <p><strong>Audio:</strong></p> <ul> <li><a href="https://github.com/nii-yamagishilab/VoiceMOS-Challenge-2023" rel="external nofollow noopener" target="_blank">VoiceMOS Challenge</a></li> <li><a href="https://github.com/nii-yamagishilab/VoiceMOS-Challenge-2022" rel="external nofollow noopener" target="_blank">BVCC Dataset</a></li> </ul> <p><strong>Images:</strong></p> <ul> <li><a href="https://cocodataset.org/" rel="external nofollow noopener" target="_blank">COCO</a></li> <li><a href="https://github.com/google-research/hype" rel="external nofollow noopener" target="_blank">HYPE Benchmark</a></li> <li><a href="https://github.com/google-research/parti" rel="external nofollow noopener" target="_blank">DrawBench</a></li> </ul> <p><strong>Video:</strong></p> <ul> <li><a href="https://www.crcv.ucf.edu/data/UCF101.php" rel="external nofollow noopener" target="_blank">UCF-101</a></li> <li><a href="https://www.deepmind.com/open-source/kinetics" rel="external nofollow noopener" target="_blank">Kinetics</a></li> </ul> <hr> <h2 id="future-directions">Future Directions</h2> <h3 id="perceptual-metrics-grounded-in-neuroscience">Perceptual Metrics Grounded in Neuroscience</h3> <p>Move beyond hand-crafted features to metrics informed by human perceptual mechanisms (e.g., models of visual attention, auditory scene analysis).</p> <h3 id="adaptive-evaluation">Adaptive Evaluation</h3> <p>Metrics that adjust to content type, user preferences, or application context (e.g., different standards for artistic vs. photorealistic generation).</p> <h3 id="multimodal-holistic-evaluation">Multimodal Holistic Evaluation</h3> <p>Unified frameworks evaluating cross-modal coherence (does the sound match the visual action?).</p> <h3 id="real-time-evaluation-for-interactive-systems">Real-Time Evaluation for Interactive Systems</h3> <p>Low-latency metrics for conversational AI, live video generation, interactive art.</p> <hr> <h2 id="conclusion">Conclusion</h2> <p>Evaluating AI-generated content remains an open research challenge. Objective metrics provide scalability and reproducibility but often miss perceptual nuances. Subjective evaluation captures human perception but is expensive and time-consuming. The field is converging on hybrid approaches: objective metrics for rapid iteration validated against carefully designed human studies.</p> <p>As generative models continue to improve, evaluation methods must evolve. We need metrics that capture semantic coherence, cultural sensitivity, and long-term engagement, not just instantaneous quality. The ultimate test is not whether AI can fool a metric, but whether it creates content that humans find valuable, trustworthy, and worth their attention.</p> <hr> <p style="text-align: center; color: var(--global-text-color-light); font-size: 0.9rem; margin-top: 3rem; font-style: italic;"> This is a living document. As new methods emerge and validation studies accumulate, this guide will be updated. Suggestions? <a href="mailto:randyrff@gmail.com">Email me</a>. </p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <h2 class="text-3xl font-semibold mb-4 mt-12"></h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://fransfela.substack.com/p/sound-horeg-antara-euforia-budaya" target="_blank" rel="external nofollow noopener">Just a moment...</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/mentoring-bang-randy-scholarship-guidance/">MentoringBangRandy: Perjalanan dari Kuli Pabrik ke Denmark</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/image-video-quality-metrics-reference/">Image and Video Quality Metrics: A Comprehensive Reference</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/audio-quality-metrics-reference/">Audio Quality Metrics: A Comprehensive Reference</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/book-soundscape-analysis/">Soundscape (Bentang Suara): Teori, Metode, dan Analisis</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬©Copyright 2026 Randy F. Fela. | All rights reserved | Built with <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> and <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a> | <a href="/mentoring/">Mentoring Services</a> | Copenhagen, Denmark </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"> <div class="modal-footer" slot="footer"> <span class="help"> <svg version="1.0" class="ninja-examplekey" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 1280 1280"> <path d="M1013 376c0 73.4-.4 113.3-1.1 120.2a159.9 159.9 0 0 1-90.2 127.3c-20 9.6-36.7 14-59.2 15.5-7.1.5-121.9.9-255 1h-242l95.5-95.5 95.5-95.5-38.3-38.2-38.2-38.3-160 160c-88 88-160 160.4-160 161 0 .6 72 73 160 161l160 160 38.2-38.3 38.3-38.2-95.5-95.5-95.5-95.5h251.1c252.9 0 259.8-.1 281.4-3.6 72.1-11.8 136.9-54.1 178.5-116.4 8.6-12.9 22.6-40.5 28-55.4 4.4-12 10.7-36.1 13.1-50.6 1.6-9.6 1.8-21 2.1-132.8l.4-122.2H1013v110z"></path> </svg> to select </span> <span class="help"> <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey" viewbox="0 0 24 24"> <path d="M0 0h24v24H0V0z" fill="none"></path> <path d="M20 12l-1.41-1.41L13 16.17V4h-2v12.17l-5.58-5.59L4 12l8 8 8-8z"></path> </svg> <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey" viewbox="0 0 24 24"> <path d="M0 0h24v24H0V0z" fill="none"></path> <path d="M4 12l1.41 1.41L11 7.83V20h2V7.83l5.58 5.59L20 12l-8-8-8 8z"></path> </svg> to navigate </span> <span class="help"> <span class="ninja-examplekey esc">esc</span> to close </span> <span class="help"> <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey backspace" viewbox="0 0 20 20" fill="currentColor"> <path fill-rule="evenodd" d="M6.707 4.879A3 3 0 018.828 4H15a3 3 0 013 3v6a3 3 0 01-3 3H8.828a3 3 0 01-2.12-.879l-4.415-4.414a1 1 0 010-1.414l4.414-4.414zm4 2.414a1 1 0 00-1.414 1.414L10.586 10l-1.293 1.293a1 1 0 101.414 1.414L12 11.414l1.293 1.293a1 1 0 001.414-1.414L13.414 10l1.293-1.293a1 1 0 00-1.414-1.414L12 8.586l-1.293-1.293z" clip-rule="evenodd"></path> </svg> move to parent </span> </div> </ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>