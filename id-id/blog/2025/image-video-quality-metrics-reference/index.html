<!DOCTYPE html> <html lang="id-id"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Image and Video Quality Metrics: A Comprehensive Reference | Dr. Randy F Fela </title> <meta name="author" content="Randy F. Fela"> <meta name="description" content="Structured guide to objective and subjective metrics for image and video quality evaluation across diverse applications and content types"> <meta name="keywords" content="perception, audio-visual, research, data-science"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/bitmoji-closeup.png?abba4a3066d843d333ecf75654a92392"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://fransfela.github.io/blog/2025/image-video-quality-metrics-reference/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/id-id/"> Dr. Randy F Fela </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/id-id/">üéß <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/id-id/about/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/id-id/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/id-id/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/id-id/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/id-id/zettelkasten/">Zettelkasten </a> </li> <li class="nav-item "> <a class="nav-link" href="/id-id/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/2025/image-video-quality-metrics-reference/"> EN-US</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Image and Video Quality Metrics: A Comprehensive Reference</h1> <p class="post-meta"> Created in 14 January 2025 </p> <p class="post-tags"> <a href="/id-id/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a> ¬† ¬∑ ¬† <a href="/id-id/blog/tag/living-note"> <i class="fa-solid fa-hashtag fa-sm"></i> living-note</a> ¬† <a href="/id-id/blog/tag/visual-perception"> <i class="fa-solid fa-hashtag fa-sm"></i> visual-perception</a> ¬† <a href="/id-id/blog/tag/perceptual-evaluation"> <i class="fa-solid fa-hashtag fa-sm"></i> perceptual-evaluation</a> ¬† ¬∑ ¬† <a href="/id-id/blog/category/research"> <i class="fa-solid fa-tag fa-sm"></i> research</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <blockquote> <p><strong>Last updated:</strong> January 14, 2025<br> <strong>Status:</strong> üü¢ Actively maintained</p> </blockquote> <hr> <h2 id="introduction">Introduction</h2> <p>Image and video quality assessment spans multiple methodologies, from full-reference (intrusive) metrics requiring pristine originals to no-reference (blind) approaches that evaluate quality without any reference. This living reference consolidates metrics across traditional media, omnidirectional content, HDR, and specialized applications.</p> <hr> <h2 id="intrusive-full-reference-metrics">Intrusive (Full-Reference) Metrics</h2> <p>Metrics requiring access to both reference (original) and distorted signals.</p> <details> <summary><strong>Peak Signal-to-Noise Ratio (PSNR)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Most widely used objective metric measuring pixel-wise difference between reference and distorted images.</p> <p><strong>How it works:</strong></p> \[\text{PSNR} = 10 \log_{10} \left( \frac{\text{MAX}^2}{\text{MSE}} \right)\] <p>where MAX is maximum possible pixel value (255 for 8-bit images), MSE is mean squared error.</p> <p><strong>Limitations:</strong> Poor correlation with human perception.</p> <p><strong>Libraries:</strong></p> <ul> <li>Python: <code class="language-plaintext highlighter-rouge">skimage.metrics.peak_signal_noise_ratio</code>, <code class="language-plaintext highlighter-rouge">cv2.PSNR</code> </li> <li>MATLAB: Built-in <code class="language-plaintext highlighter-rouge">psnr()</code> function</li> </ul> <p><strong>Open Source:</strong></p> <ul> <li><a href="https://github.com/scikit-image/scikit-image" rel="external nofollow noopener" target="_blank">scikit-image metrics module</a></li> </ul> <p><strong>Datasets:</strong></p> <ul> <li><a href="http://www.ponomarenko.info/tid2013.htm" rel="external nofollow noopener" target="_blank">TID2013</a></li> <li><a href="https://live.ece.utexas.edu/research/Quality/subjective.htm" rel="external nofollow noopener" target="_blank">LIVE Image Quality Database</a></li> </ul> <p><strong>References:</strong></p> <ul> <li><a href="https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio" rel="external nofollow noopener" target="_blank">Wikipedia: Peak signal-to-noise ratio</a></li> </ul> </div> </details> <details> <summary><strong>Structural Similarity Index (SSIM)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Perceptual metric considering luminance, contrast, and structure similarities.</p> <p><strong>How it works:</strong></p> \[\text{SSIM}(x,y) = \frac{(2\mu_x\mu_y + C_1)(2\sigma_{xy} + C_2)}{(\mu_x^2 + \mu_y^2 + C_1)(\sigma_x^2 + \sigma_y^2 + C_2)}\] <p>where Œº is mean, œÉ is variance/covariance, C are constants.</p> <p><strong>Libraries:</strong></p> <ul> <li>Python: <code class="language-plaintext highlighter-rouge">skimage.metrics.structural_similarity</code>, <code class="language-plaintext highlighter-rouge">pytorch-msssim</code> </li> <li>MATLAB: Built-in <code class="language-plaintext highlighter-rouge">ssim()</code> function</li> </ul> <p><strong>Open Source:</strong></p> <ul> <li><a href="https://github.com/VainF/pytorch-msssim" rel="external nofollow noopener" target="_blank">SSIM PyTorch implementation</a></li> <li><a href="https://www.cns.nyu.edu/~lcv/ssim/" rel="external nofollow noopener" target="_blank">Original MATLAB code</a></li> </ul> <p><strong>Datasets:</strong></p> <ul> <li><a href="https://live.ece.utexas.edu/research/Quality/" rel="external nofollow noopener" target="_blank">LIVE IQA Database</a></li> <li><a href="http://vision.eng.shizuoka.ac.jp/mod/page/view.php?id=23" rel="external nofollow noopener" target="_blank">CSIQ Database</a></li> </ul> <p><strong>References:</strong></p> <ul> <li>Wang, Z., et al. (2004). ‚ÄúImage quality assessment: from error visibility to structural similarity‚Äù</li> <li><a href="https://ieeexplore.ieee.org/document/1284395" rel="external nofollow noopener" target="_blank">IEEE Xplore paper</a></li> </ul> </div> </details> <details> <summary><strong>Multi-Scale SSIM (MS-SSIM)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Extension of SSIM evaluating structure at multiple scales via downsampling.</p> <p><strong>How it works:</strong> Applies SSIM at multiple resolutions, combines results with weights.</p> <p><strong>Libraries:</strong></p> <ul> <li>Python: <code class="language-plaintext highlighter-rouge">pytorch-msssim</code>, <code class="language-plaintext highlighter-rouge">piq</code> </li> <li>MATLAB: Available via File Exchange</li> </ul> <p><strong>Open Source:</strong></p> <ul> <li><a href="https://github.com/VainF/pytorch-msssim" rel="external nofollow noopener" target="_blank">MS-SSIM PyTorch</a></li> </ul> <p><strong>References:</strong></p> <ul> <li>Wang, Z., et al. (2003). ‚ÄúMultiscale structural similarity for image quality assessment‚Äù</li> </ul> </div> </details> <details> <summary><strong>Video Multi-Method Assessment Fusion (VMAF)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Machine learning-based video quality metric developed by Netflix, fusing multiple elementary metrics.</p> <p><strong>How it works:</strong> Combines VIF, DLM, motion, and temporal features via SVR (Support Vector Regression).</p> <p><strong>Libraries:</strong></p> <ul> <li>C: <a href="https://github.com/Netflix/vmaf" rel="external nofollow noopener" target="_blank">libvmaf (Netflix)</a> </li> <li>Python: <code class="language-plaintext highlighter-rouge">vmaf</code> (pip installable)</li> <li>FFmpeg: Built-in VMAF filter</li> </ul> <p><strong>Open Source:</strong></p> <ul> <li><a href="https://github.com/Netflix/vmaf" rel="external nofollow noopener" target="_blank">Netflix VMAF GitHub</a></li> </ul> <p><strong>Datasets:</strong></p> <ul> <li><a href="https://github.com/Netflix/vmaf/tree/master/resource/doc" rel="external nofollow noopener" target="_blank">VMAF Dataset (NFLX-TEST)</a></li> <li><a href="https://github.com/Netflix/vmaf/blob/master/resource/doc/datasets.md" rel="external nofollow noopener" target="_blank">VideoSet</a></li> </ul> <p><strong>References:</strong></p> <ul> <li>Li, Z., et al. (2016). ‚ÄúToward a practical perceptual video quality metric‚Äù</li> <li><a href="https://netflixtechblog.com/toward-a-practical-perceptual-video-quality-metric-653f208b9652" rel="external nofollow noopener" target="_blank">Netflix Tech Blog</a></li> </ul> </div> </details> <details> <summary><strong>Visual Information Fidelity (VIF)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Information-theoretic metric quantifying shared information between reference and distorted images.</p> <p><strong>How it works:</strong> Models image as natural scene statistics passing through distortion channel, computes mutual information.</p> <p><strong>Libraries:</strong></p> <ul> <li>Python: <code class="language-plaintext highlighter-rouge">piq</code> library</li> <li>MATLAB: <a href="https://live.ece.utexas.edu/research/Quality/VIF.htm" rel="external nofollow noopener" target="_blank">VIF Toolbox</a> </li> </ul> <p><strong>Open Source:</strong></p> <ul> <li><a href="https://github.com/chaofengc/IQA-PyTorch" rel="external nofollow noopener" target="_blank">PyIQA implementation</a></li> </ul> <p><strong>References:</strong></p> <ul> <li>Sheikh, H.R., &amp; Bovik, A.C. (2006). ‚ÄúImage information and visual quality‚Äù</li> </ul> </div> </details> <details> <summary><strong>Feature Similarity Index (FSIM)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Low-level feature-based metric using phase congruency and gradient magnitude.</p> <p><strong>How it works:</strong> Extracts phase congruency (PC) and gradient magnitude (GM) as features, computes similarity.</p> <p><strong>Libraries:</strong></p> <ul> <li>Python: <code class="language-plaintext highlighter-rouge">piq</code>, <code class="language-plaintext highlighter-rouge">sewar</code> </li> <li>MATLAB: <a href="http://www4.comp.polyu.edu.hk/~cslzhang/IQA/FSIM/FSIM.htm" rel="external nofollow noopener" target="_blank">FSIM Code</a> </li> </ul> <p><strong>Open Source:</strong></p> <ul> <li><a href="http://www4.comp.polyu.edu.hk/~cslzhang/IQA/FSIM/Files/FeatureSIM.m" rel="external nofollow noopener" target="_blank">FSIM MATLAB code</a></li> </ul> <p><strong>References:</strong></p> <ul> <li>Zhang, L., et al. (2011). ‚ÄúFSIM: A feature similarity index for image quality assessment‚Äù</li> </ul> </div> </details> <hr> <h2 id="semi-intrusive-metrics">Semi-Intrusive Metrics</h2> <p>Metrics using partial reference information (e.g., extracted features).</p> <details> <summary><strong>Reduced-Reference Entropic Differencing (RRED)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Uses wavelet-based entropy features transmitted as side information.</p> <p><strong>How it works:</strong> Extracts entropy of wavelet subbands from reference, compares with distorted version.</p> <p><strong>Libraries:</strong></p> <ul> <li>MATLAB: <a href="https://live.ece.utexas.edu/research/Quality/RR_IQA_software_release.zip" rel="external nofollow noopener" target="_blank">LIVE RR-IQA toolbox</a> </li> </ul> <p><strong>References:</strong></p> <ul> <li>Soundararajan, R., &amp; Bovik, A.C. (2012). ‚ÄúRRED indices: Reduced reference entropic differencing‚Äù</li> </ul> </div> </details> <details> <summary><strong>SpEED-QA (Spatial-Spectral Entropy-based Quality)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Reduced-reference metric based on spatial and spectral entropies.</p> <p><strong>How it works:</strong> Transmits entropy statistics of DCT blocks and edges as reference features.</p> <p><strong>References:</strong></p> <ul> <li>Chandler, D.M., &amp; Hemami, S.S. (2007). ‚ÄúA57 database and VSNR metric‚Äù</li> </ul> </div> </details> <hr> <h2 id="non-intrusive-no-reference-metrics">Non-Intrusive (No-Reference) Metrics</h2> <p>Blind quality assessment without access to reference.</p> <details> <summary><strong>Blind/Referenceless Image Spatial Quality Evaluator (BRISQUE)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> No-reference metric using natural scene statistics (NSS) features and SVR.</p> <p><strong>How it works:</strong> Extracts locally normalized luminance coefficients, fits to generalized Gaussian distribution, trains SVR on features.</p> <p><strong>Libraries:</strong></p> <ul> <li>Python: <code class="language-plaintext highlighter-rouge">libsvm</code>, <code class="language-plaintext highlighter-rouge">piq</code>, <code class="language-plaintext highlighter-rouge">imquality</code> </li> <li>MATLAB: <a href="https://live.ece.utexas.edu/research/Quality/index_algorithms.htm" rel="external nofollow noopener" target="_blank">BRISQUE Code</a> </li> </ul> <p><strong>Open Source:</strong></p> <ul> <li><a href="https://github.com/krshay/BRISQUE-MATLAB" rel="external nofollow noopener" target="_blank">BRISQUE MATLAB</a></li> <li><a href="https://github.com/ocampor/image-quality" rel="external nofollow noopener" target="_blank">imquality Python</a></li> </ul> <p><strong>Datasets:</strong></p> <ul> <li><a href="https://live.ece.utexas.edu/research/Quality/" rel="external nofollow noopener" target="_blank">LIVE IQA Database</a></li> <li><a href="http://www.ponomarenko.info/tid2013.htm" rel="external nofollow noopener" target="_blank">TID2013</a></li> </ul> <p><strong>References:</strong></p> <ul> <li>Mittal, A., et al. (2012). ‚ÄúNo-reference image quality assessment in the spatial domain‚Äù</li> </ul> </div> </details> <details> <summary><strong>Natural Image Quality Evaluator (NIQE)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Opinion-unaware (no training on human scores) metric based on NSS model.</p> <p><strong>How it works:</strong> Models pristine natural images with multivariate Gaussian (MVG) in NSS feature space, measures distance of test image.</p> <p><strong>Libraries:</strong></p> <ul> <li>Python: <code class="language-plaintext highlighter-rouge">piq</code>, <code class="language-plaintext highlighter-rouge">scikit-image</code> (limited)</li> <li>MATLAB: <a href="https://live.ece.utexas.edu/research/Quality/niqe_release.zip" rel="external nofollow noopener" target="_blank">NIQE Code</a> </li> </ul> <p><strong>Open Source:</strong></p> <ul> <li><a href="https://github.com/guptapraful/niqe" rel="external nofollow noopener" target="_blank">NIQE Python implementation</a></li> </ul> <p><strong>References:</strong></p> <ul> <li>Mittal, A., et al. (2013). ‚ÄúMaking a ‚Äòcompletely blind‚Äô image quality analyzer‚Äù</li> </ul> </div> </details> <details> <summary><strong>Perception-based Image Quality Evaluator (PIQE)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> No-reference metric analyzing blockiness, noise, and spatial activity.</p> <p><strong>How it works:</strong> Divides image into blocks, evaluates distortion using perceptual features (noticeably distorted blocks, noise, spatial activity).</p> <p><strong>Libraries:</strong></p> <ul> <li>MATLAB: Built-in <code class="language-plaintext highlighter-rouge">piqe()</code> function</li> </ul> <p><strong>References:</strong></p> <ul> <li>Venkatanath, N., et al. (2015). ‚ÄúBlind image quality evaluation using perception based features‚Äù</li> </ul> </div> </details> <details> <summary><strong>Deep Learning-Based: NIMA (Neural Image Assessment)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> CNN-based aesthetic and technical quality predictor trained on AVA dataset.</p> <p><strong>How it works:</strong> Fine-tunes pre-trained CNN (e.g., MobileNet, Inception) to predict distribution of human ratings.</p> <p><strong>Libraries:</strong></p> <ul> <li>Python: TensorFlow/Keras, PyTorch</li> </ul> <p><strong>Open Source:</strong></p> <ul> <li><a href="https://github.com/kentsyx/Neural-IMage-Assessment" rel="external nofollow noopener" target="_blank">NIMA PyTorch</a></li> <li><a href="https://github.com/titu1994/neural-image-assessment" rel="external nofollow noopener" target="_blank">TensorFlow implementation</a></li> </ul> <p><strong>Datasets:</strong></p> <ul> <li><a href="https://github.com/mtobeiyf/ava_downloader" rel="external nofollow noopener" target="_blank">AVA (Aesthetic Visual Analysis)</a></li> </ul> <p><strong>References:</strong></p> <ul> <li>Talebi, H., &amp; Milanfar, P. (2018). ‚ÄúNIMA: Neural image assessment‚Äù</li> </ul> </div> </details> <details> <summary><strong>MUSIQ (Multi-Scale Image Quality Transformer)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Vision Transformer-based no-reference IQA handling arbitrary resolutions and aspect ratios.</p> <p><strong>How it works:</strong> Uses multi-scale image representation fed to Transformer encoder, predicts quality score.</p> <p><strong>Open Source:</strong></p> <ul> <li><a href="https://github.com/google-research/google-research/tree/master/musiq" rel="external nofollow noopener" target="_blank">MUSIQ GitHub</a></li> </ul> <p><strong>References:</strong></p> <ul> <li>Ke, J., et al. (2021). ‚ÄúMUSIQ: Multi-scale image quality transformer‚Äù</li> </ul> </div> </details> <hr> <h2 id="overall-image-quality-metrics">Overall Image Quality Metrics</h2> <p>General-purpose metrics for diverse distortion types.</p> <details> <summary><strong>Mean Absolute Error (MAE)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Average absolute pixel-wise difference.</p> <p><strong>How it works:</strong></p> \[\text{MAE} = \frac{1}{N} \sum_{i=1}^{N} |x_i - y_i|\] <p><strong>Libraries:</strong></p> <ul> <li>Python: <code class="language-plaintext highlighter-rouge">numpy.mean(np.abs(x - y))</code> </li> </ul> <p><strong>Limitations:</strong> Does not correlate well with perceived quality.</p> </div> </details> <details> <summary><strong>Perceptual Index (PI)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> No-reference metric combining Ma‚Äôs and NIQE scores to measure perceptual quality.</p> <p><strong>How it works:</strong></p> \[\text{PI} = \frac{1}{2} \left( 10 - \text{Ma} + \text{NIQE} \right)\] <p><strong>References:</strong></p> <ul> <li>Blau, Y., &amp; Michaeli, T. (2018). ‚ÄúThe perception-distortion tradeoff‚Äù</li> </ul> </div> </details> <hr> <h2 id="image-attribute-specific-metrics">Image Attribute-Specific Metrics</h2> <p>Metrics targeting specific visual attributes.</p> <details> <summary><strong>Sharpness (Laplacian Variance)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Measures image sharpness via Laplacian operator variance.</p> <p><strong>How it works:</strong> Applies Laplacian filter, computes variance (higher = sharper).</p> <p><strong>Libraries:</strong></p> <ul> <li>Python: <code class="language-plaintext highlighter-rouge">cv2.Laplacian()</code> + <code class="language-plaintext highlighter-rouge">numpy.var()</code> </li> </ul> <p><strong>Open Source:</strong></p> <ul> <li><a href="https://docs.opencv.org/4.x/d4/d86/group__imgproc__filter.html" rel="external nofollow noopener" target="_blank">OpenCV Laplacian sharpness</a></li> </ul> <p><strong>References:</strong></p> <ul> <li>Pech-Pacheco, J.L., et al. (2000). ‚ÄúDiatom autofocusing in brightfield microscopy‚Äù</li> </ul> </div> </details> <details> <summary><strong>Colorfulness Metric</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Quantifies perceived colorfulness based on opponent color space.</p> <p><strong>How it works:</strong> Computes standard deviation and mean of rg and yb channels in opponent space.</p> <p><strong>Open Source:</strong></p> <ul> <li><a href="https://github.com/hmsch/image-colorfulness" rel="external nofollow noopener" target="_blank">Colorfulness Python implementation</a></li> </ul> <p><strong>References:</strong></p> <ul> <li>Hasler, D., &amp; Suesstrunk, S. (2003). ‚ÄúMeasuring colorfulness in natural images‚Äù</li> </ul> </div> </details> <details> <summary><strong>Contrast Metric (Michelson, RMS)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Measures luminance contrast.</p> <p><strong>How it works:</strong></p> <p>Michelson: \(C = \frac{L_{\max} - L_{\min}}{L_{\max} + L_{\min}}\)</p> <p>RMS: Standard deviation of pixel intensities.</p> <p><strong>Libraries:</strong></p> <ul> <li>Python: Custom with <code class="language-plaintext highlighter-rouge">numpy</code> </li> </ul> <p><strong>References:</strong></p> <ul> <li>Peli, E. (1990). ‚ÄúContrast in complex images‚Äù</li> </ul> </div> </details> <details> <summary><strong>Blockiness/Blurring Detection</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Detects compression artifacts like blocking and blur.</p> <p><strong>How it works:</strong> Analyzes edge discontinuities (blocking) and high-frequency attenuation (blur).</p> <p><strong>Open Source:</strong></p> <ul> <li>Part of BRISQUE, PIQE implementations</li> </ul> <p><strong>References:</strong></p> <ul> <li>Wang, Z., et al. (2002). ‚ÄúA universal image quality index‚Äù</li> </ul> </div> </details> <hr> <h2 id="omnidirectional-360-imagevideo-metrics">Omnidirectional (360¬∞) Image/Video Metrics</h2> <p>Metrics for spherical content.</p> <details> <summary><strong>Spherical PSNR (S-PSNR)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> PSNR adapted for equirectangular projection, accounting for latitude-dependent sampling density.</p> <p><strong>How it works:</strong> Weights pixel errors by spherical area (cos of latitude).</p> <p><strong>Open Source:</strong></p> <ul> <li><a href="https://github.com/facebook/transform360" rel="external nofollow noopener" target="_blank">360Lib (spherical video tools)</a></li> </ul> <p><strong>References:</strong></p> <ul> <li>Yu, M., et al. (2015). ‚ÄúA framework to evaluate omnidirectional video coding schemes‚Äù</li> </ul> </div> </details> <details> <summary><strong>Weighted-to-Spherically-Uniform PSNR (WS-PSNR)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Improved S-PSNR with uniform sampling on sphere via resampling.</p> <p><strong>How it works:</strong> Projects equirectangular to uniform spherical grid, computes PSNR.</p> <p><strong>Open Source:</strong></p> <ul> <li><a href="https://github.com/facebook/transform360" rel="external nofollow noopener" target="_blank">360Lib GitHub</a></li> </ul> <p><strong>References:</strong></p> <ul> <li>Sun, Y., et al. (2017). ‚ÄúWeighted-to-spherically-uniform quality evaluation for omnidirectional video‚Äù</li> </ul> </div> </details> <details> <summary><strong>Craster Parabolic Projection PSNR (CPP-PSNR)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Uses Craster projection minimizing area distortion for quality measurement.</p> <p><strong>How it works:</strong> Converts to Craster parabolic projection before computing PSNR.</p> <p><strong>References:</strong></p> <ul> <li>Yu, M., et al. (2017). ‚ÄúA framework to evaluate omnidirectional video coding schemes‚Äù</li> </ul> </div> </details> <details> <summary><strong>Viewport-based Quality Assessment</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Evaluates quality based on user‚Äôs viewing direction/viewport.</p> <p><strong>How it works:</strong> Samples viewports according to head movement patterns, computes quality per viewport.</p> <p><strong>Datasets:</strong></p> <ul> <li><a href="https://github.com/360VidStr/A_large_dataset_of_360_video_user_behaviour" rel="external nofollow noopener" target="_blank">360¬∞ Video Head Movement Dataset</a></li> </ul> <p><strong>References:</strong></p> <ul> <li>Xu, M., et al. (2018). ‚ÄúPredicting head movement in panoramic video‚Äù</li> </ul> </div> </details> <hr> <h2 id="natural-image-quality">Natural Image Quality</h2> <p>Metrics for photographic/natural scenes.</p> <details> <summary><strong>NIQE (Natural Image Quality Evaluator)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> See ‚ÄúNon-Intrusive Metrics‚Äù section above.</p> </div> </details> <details> <summary><strong>IL-NIQE (Integrated Local NIQE)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Improved NIQE with local quality assessment and integration.</p> <p><strong>How it works:</strong> Computes local NIQE scores in patches, aggregates for global score.</p> <p><strong>Open Source:</strong></p> <ul> <li><a href="https://github.com/christosbampis/ILNIQE" rel="external nofollow noopener" target="_blank">IL-NIQE MATLAB</a></li> </ul> <p><strong>References:</strong></p> <ul> <li>Zhang, L., et al. (2015). ‚ÄúA feature-enriched completely blind image quality evaluator‚Äù</li> </ul> </div> </details> <hr> <h2 id="hdr-image-quality">HDR Image Quality</h2> <p>Metrics for high dynamic range content.</p> <details> <summary><strong>HDR-VDP-2 (HDR Visual Difference Predictor)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Full-reference metric modeling human visual system for HDR images.</p> <p><strong>How it works:</strong> Applies luminance masking, contrast sensitivity, spatial frequency channels.</p> <p><strong>Libraries:</strong></p> <ul> <li>MATLAB: <a href="https://sourceforge.net/projects/hdrvdp/" rel="external nofollow noopener" target="_blank">HDR-VDP-2 Toolbox</a> </li> </ul> <p><strong>Open Source:</strong></p> <ul> <li><a href="https://sourceforge.net/projects/hdrvdp/" rel="external nofollow noopener" target="_blank">HDR-VDP-2 SourceForge</a></li> </ul> <p><strong>Datasets:</strong></p> <ul> <li><a href="https://live.ece.utexas.edu/research/LIVE_HDRIDatabase/index.html" rel="external nofollow noopener" target="_blank">ESPL-LIVE HDR Database</a></li> </ul> <p><strong>References:</strong></p> <ul> <li>Mantiuk, R., et al. (2011). ‚ÄúHDR-VDP-2: A calibrated visual metric for visibility and quality predictions‚Äù</li> </ul> </div> </details> <details> <summary><strong>HDR-VQM (HDR Video Quality Metric)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Extends HDR-VDP to video with temporal modeling.</p> <p><strong>How it works:</strong> Adds temporal contrast sensitivity and motion modeling to HDR-VDP.</p> <p><strong>References:</strong></p> <ul> <li>Narwaria, M., et al. (2015). ‚ÄúHDR-VQM: An objective quality measure for high dynamic range video‚Äù</li> </ul> </div> </details> <details> <summary><strong>PU21 (Perceptual Uniformity 2021)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Perceptually uniform color space for HDR images.</p> <p><strong>How it works:</strong> Transforms HDR pixel values to perceptually uniform encoding before computing differences.</p> <p><strong>Open Source:</strong></p> <ul> <li><a href="https://github.com/gfxdisp/pu21" rel="external nofollow noopener" target="_blank">PU21 MATLAB/Python</a></li> </ul> <p><strong>References:</strong></p> <ul> <li>Mikhailiuk, A., et al. (2021). ‚ÄúA perceptually uniform color space for HDR imaging‚Äù</li> </ul> </div> </details> <hr> <h2 id="artistic-image-quality">Artistic Image Quality</h2> <p>Metrics for stylized/artistic content.</p> <details> <summary><strong>Neural Style Transfer Quality (Gatys Loss)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Measures content and style preservation in neural style transfer.</p> <p><strong>How it works:</strong> Computes content loss (feature difference in deep layers) and style loss (Gram matrix difference).</p> <p><strong>Open Source:</strong></p> <ul> <li><a href="https://github.com/pytorch/examples/tree/main/fast_neural_style" rel="external nofollow noopener" target="_blank">Neural Style Transfer PyTorch</a></li> </ul> <p><strong>References:</strong></p> <ul> <li>Gatys, L.A., et al. (2016). ‚ÄúImage style transfer using convolutional neural networks‚Äù</li> </ul> </div> </details> <details> <summary><strong>Aesthetic Quality Assessment (AVA-based models)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Predicts aesthetic appeal using deep learning trained on AVA dataset.</p> <p><strong>How it works:</strong> CNN extracts features correlating with aesthetic ratings (composition, color harmony, etc.).</p> <p><strong>Open Source:</strong></p> <ul> <li><a href="https://github.com/titu1994/neural-image-assessment" rel="external nofollow noopener" target="_blank">NIMA (see No-Reference section)</a></li> </ul> <p><strong>Datasets:</strong></p> <ul> <li><a href="https://github.com/mtobeiyf/ava_downloader" rel="external nofollow noopener" target="_blank">AVA Dataset</a></li> </ul> <p><strong>References:</strong></p> <ul> <li>Murray, N., et al. (2012). ‚ÄúAVA: A large-scale database for aesthetic visual analysis‚Äù</li> </ul> </div> </details> <hr> <h2 id="video-specific-metrics">Video-Specific Metrics</h2> <h3 id="video-movies">Video Movies</h3> <details> <summary><strong>Video Multimethod Assessment Fusion (VMAF)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> See ‚ÄúIntrusive Metrics‚Äù section above. Widely used for streaming video (Netflix, YouTube).</p> </div> </details> <details> <summary><strong>Spatial-Temporal SSIM (ST-SSIM)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Extends SSIM to temporal dimension for video.</p> <p><strong>How it works:</strong> Computes SSIM across spatial and temporal patches (3D blocks).</p> <p><strong>References:</strong></p> <ul> <li>Wang, Z., et al. (2004). ‚ÄúVideo quality assessment based on structural distortion measurement‚Äù</li> </ul> </div> </details> <details> <summary><strong>Video Quality Metric (VQM)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> ITU standard for broadcast video quality.</p> <p><strong>How it works:</strong> Extracts perceptual features (spatial/temporal activity, edge degradation, chroma spread), combines via linear model.</p> <p><strong>References:</strong></p> <ul> <li><a href="https://www.itu.int/rec/T-REC-J.144" rel="external nofollow noopener" target="_blank">ITU-T J.144: Objective perceptual video quality measurement</a></li> </ul> </div> </details> <h3 id="videoconferencing">Videoconferencing</h3> <details> <summary><strong>ViVQM (Video-over-IP Visual Quality Metric)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> No-reference metric for videoconferencing and VoIP video.</p> <p><strong>How it works:</strong> Detects packet loss artifacts, blockiness, blurring specific to real-time video transmission.</p> <p><strong>References:</strong></p> <ul> <li>Reibman, A.R., et al. (2004). ‚ÄúQuality monitoring of video over a packet network‚Äù</li> </ul> </div> </details> <details> <summary><strong>Real-Time Video Quality Assessment (QoE models)</strong></summary> <div style="padding: 1rem; background-color: var(--global-bg-color); margin-top: 0.5rem; border-left: 3px solid var(--global-theme-color);"> <p><strong>Description:</strong> Models predicting quality-of-experience considering bitrate switching, stalling, resolution changes.</p> <p><strong>How it works:</strong> Combines video quality metrics with buffering/stalling penalties.</p> <p><strong>Open Source:</strong></p> <ul> <li><a href="https://github.com/itu-p1203/itu-p1203" rel="external nofollow noopener" target="_blank">QoE Models GitHub</a></li> </ul> <p><strong>References:</strong></p> <ul> <li><a href="https://www.itu.int/rec/T-REC-P.1203" rel="external nofollow noopener" target="_blank">ITU-T P.1203: Parametric bitstream-based quality assessment</a></li> </ul> </div> </details> <hr> <h2 id="key-datasets">Key Datasets</h2> <h3 id="image-quality-databases">Image Quality Databases</h3> <ul> <li><a href="https://live.ece.utexas.edu/research/Quality/subjective.htm" rel="external nofollow noopener" target="_blank">LIVE IQA Database</a></li> <li><a href="http://www.ponomarenko.info/tid2013.htm" rel="external nofollow noopener" target="_blank">TID2013</a></li> <li><a href="http://vision.eng.shizuoka.ac.jp/mod/page/view.php?id=23" rel="external nofollow noopener" target="_blank">CSIQ</a></li> <li><a href="http://database.mmsp-kn.de/kadid-10k-database.html" rel="external nofollow noopener" target="_blank">KADID-10k</a></li> <li><a href="https://github.com/mtobeiyf/ava_downloader" rel="external nofollow noopener" target="_blank">AVA (Aesthetic Visual Analysis)</a></li> <li><a href="https://live.ece.utexas.edu/research/LIVE_HDRIDatabase/" rel="external nofollow noopener" target="_blank">ESPL-LIVE HDR Database</a></li> </ul> <h3 id="video-quality-databases">Video Quality Databases</h3> <ul> <li><a href="https://live.ece.utexas.edu/research/Quality/live_video.html" rel="external nofollow noopener" target="_blank">LIVE Video Quality Database</a></li> <li><a href="https://github.com/Netflix/vmaf/tree/master/resource/doc" rel="external nofollow noopener" target="_blank">Netflix Public Dataset</a></li> <li><a href="https://ivc.uwaterloo.ca/database/Waterloo-IVC-4K-Video.html" rel="external nofollow noopener" target="_blank">Waterloo IVC 4K Video Quality Database</a></li> <li><a href="https://media.withyoutube.com/" rel="external nofollow noopener" target="_blank">YouTube UGC Dataset</a></li> </ul> <h3 id="360-video-databases">360¬∞ Video Databases</h3> <ul> <li><a href="https://github.com/360VidStr/A_large_dataset_of_360_video_user_behaviour" rel="external nofollow noopener" target="_blank">360¬∞ Video Head Movement Dataset</a></li> <li><a href="https://ieeexplore.ieee.org/document/8463705" rel="external nofollow noopener" target="_blank">VQA-ODV (Omnidirectional Video)</a></li> </ul> <hr> <h2 id="references">References</h2> <h3 id="standards">Standards</h3> <ul> <li>ITU-T J.144: VQM</li> <li>ITU-T P.1203: QoE for adaptive streaming</li> <li>ITU-R BT.500: Subjective assessment of TV pictures</li> </ul> <h3 id="key-papers">Key Papers</h3> <ul> <li>Wang, Z., et al. (2004). ‚ÄúImage quality assessment: from error visibility to structural similarity‚Äù</li> <li>Li, Z., et al. (2016). ‚ÄúToward a practical perceptual video quality metric‚Äù (VMAF)</li> <li>Mittal, A., et al. (2012). ‚ÄúNo-reference image quality assessment in the spatial domain‚Äù (BRISQUE)</li> <li>Mantiuk, R., et al. (2011). ‚ÄúHDR-VDP-2‚Äù</li> <li>Talebi, H., &amp; Milanfar, P. (2018). ‚ÄúNIMA: Neural image assessment‚Äù</li> </ul> <h3 id="toolboxes--libraries">Toolboxes &amp; Libraries</h3> <ul> <li><a href="https://github.com/scikit-image/scikit-image" rel="external nofollow noopener" target="_blank">scikit-image</a></li> <li><a href="https://github.com/photosynthesis-team/piq" rel="external nofollow noopener" target="_blank">piq (PyTorch Image Quality)</a></li> <li><a href="https://github.com/chaofengc/IQA-PyTorch" rel="external nofollow noopener" target="_blank">IQA-PyTorch</a></li> <li><a href="https://github.com/Netflix/vmaf" rel="external nofollow noopener" target="_blank">Netflix VMAF</a></li> <li><a href="https://sourceforge.net/projects/hdrvdp/" rel="external nofollow noopener" target="_blank">HDR-VDP-2</a></li> <li><a href="https://github.com/ocampor/image-quality" rel="external nofollow noopener" target="_blank">imquality</a></li> </ul> <hr> <p style="text-align: center; color: var(--global-text-color-light); font-size: 0.9rem; margin-top: 3rem; font-style: italic;"> Last updated: January 14, 2025<br> This is a living document. Suggestions? <a href="mailto:randyrff@gmail.com">Email me</a>. </p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/id-id/blog/2026/expensive-earbuds-engineering-voice-calls-music/">Why Your Expensive Earbuds Spend More Engineering Effort on Your Zoom Calls Than Your Spotify Playlist</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/id-id/blog/2026/mentoring-bang-randy-scholarship-guidance/">MentoringBangRandy: Perjalanan dari Kuli Pabrik ke Denmark</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://fransfela.substack.com/p/sound-horeg-antara-euforia-budaya" target="_blank" rel="external nofollow noopener">Sound Horeg: Antara Euforia Budaya Populer dan Degradasi Fungsi Pendengaran</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/id-id/blog/2025/evaluating-ai-generated-content-audio-visual-state-of-art/">Evaluating AI-Generated Content: The Challenge of Measuring What Machines Create</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/id-id/blog/2025/audio-quality-metrics-reference/">Audio Quality Metrics: A Comprehensive Reference</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬©Copyright 2026 Randy F. Fela. | All rights reserved | Built with <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> and <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a> | <a href="/id-id/mentoring/">Mentoring Services</a> | Copenhagen, Denmark </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"> <div class="modal-footer" slot="footer"> <span class="help"> <svg version="1.0" class="ninja-examplekey" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 1280 1280"> <path d="M1013 376c0 73.4-.4 113.3-1.1 120.2a159.9 159.9 0 0 1-90.2 127.3c-20 9.6-36.7 14-59.2 15.5-7.1.5-121.9.9-255 1h-242l95.5-95.5 95.5-95.5-38.3-38.2-38.2-38.3-160 160c-88 88-160 160.4-160 161 0 .6 72 73 160 161l160 160 38.2-38.3 38.3-38.2-95.5-95.5-95.5-95.5h251.1c252.9 0 259.8-.1 281.4-3.6 72.1-11.8 136.9-54.1 178.5-116.4 8.6-12.9 22.6-40.5 28-55.4 4.4-12 10.7-36.1 13.1-50.6 1.6-9.6 1.8-21 2.1-132.8l.4-122.2H1013v110z"></path> </svg> to select </span> <span class="help"> <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey" viewbox="0 0 24 24"> <path d="M0 0h24v24H0V0z" fill="none"></path> <path d="M20 12l-1.41-1.41L13 16.17V4h-2v12.17l-5.58-5.59L4 12l8 8 8-8z"></path> </svg> <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey" viewbox="0 0 24 24"> <path d="M0 0h24v24H0V0z" fill="none"></path> <path d="M4 12l1.41 1.41L11 7.83V20h2V7.83l5.58 5.59L20 12l-8-8-8 8z"></path> </svg> to navigate </span> <span class="help"> <span class="ninja-examplekey esc">esc</span> to close </span> <span class="help"> <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey backspace" viewbox="0 0 20 20" fill="currentColor"> <path fill-rule="evenodd" d="M6.707 4.879A3 3 0 018.828 4H15a3 3 0 013 3v6a3 3 0 01-3 3H8.828a3 3 0 01-2.12-.879l-4.415-4.414a1 1 0 010-1.414l4.414-4.414zm4 2.414a1 1 0 00-1.414 1.414L10.586 10l-1.293 1.293a1 1 0 101.414 1.414L12 11.414l1.293 1.293a1 1 0 001.414-1.414L13.414 10l1.293-1.293a1 1 0 00-1.414-1.414L12 8.586l-1.293-1.293z" clip-rule="evenodd"></path> </svg> move to parent </span> </div> </ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/id-id/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>