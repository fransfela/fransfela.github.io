<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://fransfela.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://fransfela.github.io/" rel="alternate" type="text/html"/><updated>2026-01-10T07:16:13+00:00</updated><id>https://fransfela.github.io/feed.xml</id><title type="html">Dr. Randy F Fela</title><entry><title type="html">MentoringBangRandy: Perjalanan dari Kuli Pabrik ke Denmark</title><link href="https://fransfela.github.io/blog/2026/mentoring-bang-randy-scholarship-guidance/" rel="alternate" type="text/html" title="MentoringBangRandy: Perjalanan dari Kuli Pabrik ke Denmark"/><published>2026-01-06T08:00:00+00:00</published><updated>2026-01-06T08:00:00+00:00</updated><id>https://fransfela.github.io/blog/2026/mentoring-bang-randy-scholarship-guidance</id><content type="html" xml:base="https://fransfela.github.io/blog/2026/mentoring-bang-randy-scholarship-guidance/"><![CDATA[<h2 id="dari-kuli-pabrik-ke-denmark-mimpi-yang-menjadi-kenyataan">Dari Kuli Pabrik ke Denmark: Mimpi yang Menjadi Kenyataan</h2> <p>Saya lahir dan besar di pinggiran Jakarta, dari keluarga kelas bawah. Setelah lulus STM, saya menjalani berbagai pekerjaan serabutan‚Äîdari kuli pabrik di perusahaan otomotif hingga guru les dan penjaga warnet. Dengan upah hanya Rp 250.000 - Rp 500.000 per bulan, saya tahu satu hal: <strong>pendidikan adalah satu-satunya jalan keluar.</strong></p> <h3 id="titik-balik-di-ugm">Titik Balik di UGM</h3> <p>Di tahun 2010, saya diterima di Universitas Gadjah Mada. Ini bukan akhir perjuangan‚Äîjustru awalnya. Saya bekerja keras sambil kuliah: guru les, berjualan di pasar pagi, asisten dosen, hingga mengikuti berbagai proyek riset. Yang membuat saya bertahan? <strong>Beasiswa.</strong></p> <p>Tahun 2013, untuk pertama kalinya saya ke luar negeri‚ÄîHong Kong‚Äîdengan biaya penuh untuk konferensi pemuda. Pengalaman itu membuka mata saya: dunia jauh lebih luas dari yang saya bayangkan.</p> <h3 id="beasiswa-yang-mengubah-hidup">Beasiswa yang Mengubah Hidup</h3> <p>Saya terus berjuang dan akhirnya menerima berbagai beasiswa bergengsi:</p> <ul> <li>üéì <strong>Short Course</strong> dari Australia Awards Scholarship</li> <li>üéì <strong>International Merit Scholarship</strong> dari University of Southampton</li> <li>üéì <strong>Sandwich Program</strong> dari ITB</li> <li>üéì <strong>Marie Sk≈Çodowska-Curie Action</strong> dari Uni Eropa untuk PhD</li> </ul> <p>Kini, saya bekerja di Denmark sebagai <strong>Perceptual Audio Engineer</strong> dan <strong>Researcher</strong>, telah menjelajahi lebih dari 15 negara, dan bertemu langsung dengan reviewer, panitia seleksi, serta akademisi yang terlibat dalam proses pemberian beasiswa.</p> <hr/> <h2 id="saya-mengerti-perjuangan-mempersiapkan-beasiswa">Saya Mengerti Perjuangan Mempersiapkan Beasiswa</h2> <p>Sebagai seseorang yang pernah berjuang sendiri, saya memahami tantangan yang sering dihadapi:</p> <ul> <li>‚ùå <strong>Bingung harus mulai dari mana</strong> ‚Äì Informasi beasiswa sangat banyak, tapi sulit menentukan yang paling sesuai</li> <li>‚ùå <strong>Kesulitan menemukan mentor</strong> ‚Äì Ingin bertanya tapi tidak tahu harus ke siapa</li> <li>‚ùå <strong>Merasa tidak cukup pintar</strong> ‚Äì Takut gagal, minder, atau merasa tidak layak bersaing</li> <li>‚ùå <strong>Kendala menyiapkan dokumen</strong> ‚Äì Menulis motivation letter yang kuat bukan hal mudah</li> <li>‚ùå <strong>Ketakutan saat wawancara</strong> ‚Äì Banyak yang gagal karena kurang persiapan dan percaya diri</li> </ul> <p><strong>Saya telah mengalami semua ini.</strong> Dan saya ingin membantu kamu agar tidak perlu melalui kesulitan yang sama sendirian.</p> <hr/> <h2 id="mentoringbangrandy-pendampingan-berbasis-pengalaman">MentoringBangRandy: Pendampingan Berbasis Pengalaman</h2> <p>Saya ingin menjadi orang yang dulu saya butuhkan ketika berjuang sendiri. Itulah alasan saya membangun <strong>#MentoringBangRandy</strong>.</p> <h3 id="layanan-yang-tersedia">Layanan yang Tersedia:</h3> <ul> <li>üí° <strong>1-on-1 Konsultasi Beasiswa</strong> ‚Äì Strategi, pemilihan beasiswa, dan wawasan berdasarkan pengalaman langsung</li> <li>üìù <strong>Review &amp; Revisi Dokumen</strong> ‚Äì Meningkatkan kualitas CV, motivation letter, dan research proposal</li> <li>üé§ <strong>Simulasi Wawancara</strong> ‚Äì Latihan intensif agar lebih percaya diri</li> <li>üöÄ <strong>Mentoring Intensif</strong> ‚Äì Pendampingan dari awal hingga akhir</li> </ul> <p>üì¢ <strong>Saya hanya membuka limited seats</strong> untuk menjaga kualitas mentoring. Ini bukan sekadar layanan‚Äîini adalah misi saya untuk membantu lebih banyak orang meraih pendidikan setinggi mungkin.</p> <hr/> <h2 id="mimpi-besar-itu-mungkin--asalkan-kamu-berani-memulai">Mimpi Besar Itu Mungkin ‚Äì Asalkan Kamu Berani Memulai</h2> <p>Dulu, saya hanyalah anak STM yang bekerja serabutan. Kini, saya telah mengelilingi 15+ negara dan berkarier di Denmark.</p> <p><strong>Jika saya bisa, kamu juga bisa.</strong></p> <p>Namun, langkah pertama selalu ada di tanganmu. Jangan tunda lagi!</p> <hr/> <h3 id="-siap-memulai-perjalananmu">üìå Siap Memulai Perjalananmu?</h3> <p><strong><a href="/mentoring/">Lihat Layanan Lengkap &amp; Booking ‚Üí</a></strong></p> <p><strong>Punya pertanyaan?</strong><br/> üì≤ Follow <a href="https://instagram.com/fransfela">@fransfela</a> dan DM aja! GRATIS konsultasi via DM.</p> <p>üí¨ <strong><a href="https://ig.me/j/Aba_wtvabFvyuRX_/">Join Group Chat IG ‚ÄúMenuju Global Citizen‚Äù</a></strong> ‚Äì Update info beasiswa &amp; lowongan PhD.</p> <hr/> <p><em>Mau nraktir kopi? Boleh banget!</em> ‚òï</p>]]></content><author><name></name></author><category term="mentoring"/><category term="career-tips"/><category term="scholarship-tips"/><summary type="html"><![CDATA[Bagaimana saya membantu kamu meraih beasiswa luar negeri berdasarkan pengalaman nyata]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://fransfela.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://fransfela.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://fransfela.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[<p>May 14, 2024 We‚Äôre introducing a series of updates across the Gemini family of models, including the new 1.5 Flash, our lightweight model for speed and efficiency, and Project Astra, our vision for the future of AI assistants. In December, we launched our first natively multimodal model Gemini 1.0 in three sizes: Ultra, Pro and Nano. Just a few months later we released 1.5 Pro, with enhanced performance and a breakthrough long context window of 1 million tokens.Developers and enterprise customers have been putting 1.5 Pro to use in incredible ways and finding its long context window, multimodal reasoning capabilities and impressive overall performance incredibly useful.We know from user feedback that some applications need lower latency and a lower cost to serve. This inspired us to keep innovating, so today, we‚Äôre introducing Gemini 1.5 Flash: a model that‚Äôs lighter-weight than 1.5 Pro, and designed to be fast and efficient to serve at scale.Both 1.5 Pro and 1.5 Flash are available in public preview with a 1 million token context window in Google AI Studio and Vertex AI. And now, 1.5 Pro is also available with a 2 million token context window via waitlist to developers using the API and to Google Cloud customers.We‚Äôre also introducing updates across the Gemini family of models, announcing our next generation of open models, Gemma 2, and sharing progress on the future of AI assistants, with Project Astra.Context lengths of leading foundation models compared with Gemini 1.5‚Äôs 2 million token capability1.5 Flash is the newest addition to the Gemini model family and the fastest Gemini model served in the API. It‚Äôs optimized for high-volume, high-frequency tasks at scale, is more cost-efficient to serve and features our breakthrough long context window.While it‚Äôs a lighter weight model than 1.5 Pro, it‚Äôs highly capable of multimodal reasoning across vast amounts of information and delivers impressive quality for its size.The new Gemini 1.5 Flash model is optimized for speed and efficiency, is highly capable of multimodal reasoning and features our breakthrough long context window.1.5 Flash excels at summarization, chat applications, image and video captioning, data extraction from long documents and tables, and more. This is because it‚Äôs been trained by 1.5 Pro through a process called ‚Äúdistillation,‚Äù where the most essential knowledge and skills from a larger model are transferred to a smaller, more efficient model.Read more about 1.5 Flash in our updated Gemini 1.5 technical report, on the Gemini technology page, and learn about 1.5 Flash‚Äôs availability and pricing.Over the last few months, we‚Äôve significantly improved 1.5 Pro, our best model for general performance across a wide range of tasks.Beyond extending its context window to 2 million tokens, we‚Äôve enhanced its code generation, logical reasoning and planning, multi-turn conversation, and audio and image understanding through data and algorithmic advances. We see strong improvements on public and internal benchmarks for each of these tasks.1.5 Pro can now follow increasingly complex and nuanced instructions, including ones that specify product-level behavior involving role, format and style. We‚Äôve improved control over the model‚Äôs responses for specific use cases, like crafting the persona and response style of a chat agent or automating workflows through multiple function calls. And we‚Äôve enabled users to steer model behavior by setting system instructions.We added audio understanding in the Gemini API and Google AI Studio, so 1.5 Pro can now reason across image and audio for videos uploaded in Google AI Studio. And we‚Äôre now integrating 1.5 Pro into Google products, including Gemini Advanced and in Workspace apps.Read more about 1.5 Pro in our updated Gemini 1.5 technical report and on the Gemini technology page.Gemini Nano is expanding beyond text-only inputs to include images as well. Starting with Pixel, applications using Gemini Nano with Multimodality will be able to understand the world the way people do ‚Äî not just through text, but also through sight, sound and spoken language.Read more about Gemini 1.0 Nano on Android.Today, we‚Äôre also sharing a series of updates to Gemma, our family of open models built from the same research and technology used to create the Gemini models.We‚Äôre announcing Gemma 2, our next generation of open models for responsible AI innovation. Gemma 2 has a new architecture designed for breakthrough performance and efficiency, and will be available in new sizes.The Gemma family is also expanding with PaliGemma, our first vision-language model inspired by PaLI-3. And we‚Äôve upgraded our Responsible Generative AI Toolkit with LLM Comparator for evaluating the quality of model responses.Read more on the Developer blog.As part of Google DeepMind‚Äôs mission to build AI responsibly to benefit humanity, we‚Äôve always wanted to develop universal AI agents that can be helpful in everyday life. That‚Äôs why today, we‚Äôre sharing our progress in building the future of AI assistants with Project Astra (advanced seeing and talking responsive agent).To be truly useful, an agent needs to understand and respond to the complex and dynamic world just like people do ‚Äî and take in and remember what it sees and hears to understand context and take action. It also needs to be proactive, teachable and personal, so users can talk to it naturally and without lag or delay.While we‚Äôve made incredible progress developing AI systems that can understand multimodal information, getting response time down to something conversational is a difficult engineering challenge. Over the past few years, we‚Äôve been working to improve how our models perceive, reason and converse to make the pace and quality of interaction feel more natural.Building on Gemini, we‚Äôve developed prototype agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events, and caching this information for efficient recall.By leveraging our leading speech models, we also enhanced how they sound, giving the agents a wider range of intonations. These agents can better understand the context they‚Äôre being used in, and respond quickly, in conversation.With technology like this, it‚Äôs easy to envision a future where people could have an expert AI assistant by their side, through a phone or glasses. And some of these capabilities are coming to Google products, like the Gemini app and web experience, later this year.We‚Äôve made incredible progress so far with our family of Gemini models, and we‚Äôre always striving to advance the state-of-the-art even further. By investing in a relentless production line of innovation, we‚Äôre able to explore new ideas at the frontier, while also unlocking the possibility of new and exciting Gemini use cases.Learn more about Gemini and its capabilities. Your information will be used in accordance with Google‚Äôs privacy policy.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      Done. Just one step more.
    
      Check your inbox to confirm your subscription.
    You are already subscribed to our newsletter.
    You can also subscribe with a
    different email address
    
    .
    
  Let‚Äôs stay in touch. Get the latest news from Google in your inbox.
          Follow Us
</code></pre></div></div>]]></content><author><name></name></author><category term="external-posts"/><category term="google"/><summary type="html"><![CDATA[We‚Äôre sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">a post with advanced image components</title><link href="https://fransfela.github.io/blog/2024/advanced-images/" rel="alternate" type="text/html" title="a post with advanced image components"/><published>2024-01-27T11:46:00+00:00</published><updated>2024-01-27T11:46:00+00:00</updated><id>https://fransfela.github.io/blog/2024/advanced-images</id><content type="html" xml:base="https://fransfela.github.io/blog/2024/advanced-images/"><![CDATA[<p>This is an example post with advanced image components.</p> <h2 id="image-slider">Image Slider</h2> <p>This is a simple image slider. It uses the <a href="https://swiperjs.com/">Swiper</a> library. Check the <a href="https://swiperjs.com/demos">examples page</a> for more information of what you can achieve with it.</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/9-480.webp 480w,/assets/img/9-800.webp 800w,/assets/img/9-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/9.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/7-480.webp 480w,/assets/img/7-800.webp 800w,/assets/img/7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/8-480.webp 480w,/assets/img/8-800.webp 800w,/assets/img/8-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/8.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/10-480.webp 480w,/assets/img/10-800.webp 800w,/assets/img/10-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/10.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/12-480.webp 480w,/assets/img/12-800.webp 800w,/assets/img/12-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/12.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container> <h2 id="image-comparison-slider">Image Comparison Slider</h2> <p>This is a simple image comparison slider. It uses the <a href="https://img-comparison-slider.sneas.io/">img-comparison-slider</a> library. Check the <a href="https://img-comparison-slider.sneas.io/examples.html">examples page</a> for more information of what you can achieve with it.</p> <img-comparison-slider> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/prof_pic.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic_color-480.webp 480w,/assets/img/prof_pic_color-800.webp 800w,/assets/img/prof_pic_color-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/prof_pic_color.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </img-comparison-slider>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[this is what advanced image components could look like]]></summary></entry><entry><title type="html">Finding the Optimal Number of Clusters: Part 2 - Advanced Statistical Methods</title><link href="https://fransfela.github.io/blog/2024/optimal-clustering-part2-advanced-statistical-methods/" rel="alternate" type="text/html" title="Finding the Optimal Number of Clusters: Part 2 - Advanced Statistical Methods"/><published>2024-01-22T00:00:00+00:00</published><updated>2024-01-22T00:00:00+00:00</updated><id>https://fransfela.github.io/blog/2024/optimal-clustering-part2-advanced-statistical-methods</id><content type="html" xml:base="https://fransfela.github.io/blog/2024/optimal-clustering-part2-advanced-statistical-methods/"><![CDATA[<p>Welcome back to our series on finding the optimal number of clusters! In <a href="https://fransfela.github.io/blog/2024/optimal-clustering-part1-foundation-methods/">Part 1</a>, we explored three foundational methods: the Elbow Method, Silhouette Analysis, and Davies-Bouldin Index. These gave us intuitive, visual ways to evaluate clustering quality.</p> <p>But what if these methods disagree? What if you need more <strong>statistically rigorous</strong> validation? That‚Äôs where today‚Äôs methods come in.</p> <p>In Part 2, we‚Äôll explore three advanced statistical techniques that bring mathematical rigor to cluster validation:</p> <ul> <li><strong>Calinski-Harabasz Index</strong>: The variance ratio criterion</li> <li><strong>Gap Statistic</strong>: Comparing against null hypotheses</li> <li><strong>BIC/AIC</strong>: Model selection for probabilistic clustering</li> </ul> <p>These methods are particularly powerful when foundational approaches give ambiguous results or when you need to justify your choice of k to stakeholders with statistical evidence.</p> <p>Let‚Äôs dive in! üìä</p> <hr/> <h2 id="quick-recap-where-we-left-off">Quick Recap: Where We Left Off</h2> <p>In Part 1, we analyzed the Iris dataset and got these results:</p> <table> <thead> <tr> <th>Method</th> <th>Optimal k</th> <th>Agreement with Truth (k=3)</th> </tr> </thead> <tbody> <tr> <td>Elbow Method</td> <td>3</td> <td>‚úÖ Yes</td> </tr> <tr> <td>Silhouette Analysis</td> <td>2</td> <td>‚ùå No</td> </tr> <tr> <td>Davies-Bouldin Index</td> <td>3</td> <td>‚úÖ Yes</td> </tr> </tbody> </table> <p>We have <strong>partial consensus</strong> - 2 out of 3 methods suggest k=3. But that one disagreement from Silhouette makes us wonder: should we investigate k=2 more carefully? Or is k=3 really optimal?</p> <p>This is where advanced statistical methods shine - they provide additional perspectives with solid mathematical foundations.</p> <hr/> <h2 id="setup-import-libraries-and-load-data">Setup: Import Libraries and Load Data</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="n">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="n">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">from</span> <span class="n">sklearn.mixture</span> <span class="kn">import</span> <span class="n">GaussianMixture</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">calinski_harabasz_score</span>
<span class="kn">import</span> <span class="n">warnings</span>
<span class="n">warnings</span><span class="p">.</span><span class="nf">filterwarnings</span><span class="p">(</span><span class="sh">'</span><span class="s">ignore</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Load and prepare data
</span><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="nf">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="p">.</span><span class="n">data</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">()</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Dataset shape: </span><span class="si">{</span><span class="n">X_scaled</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Ready to explore advanced methods!</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <hr/> <h2 id="method-4-calinski-harabasz-index-variance-ratio-criterion">Method 4: Calinski-Harabasz Index (Variance Ratio Criterion)</h2> <h3 id="the-intuition">The Intuition</h3> <p>Imagine you‚Äôre organizing a conference with multiple breakout sessions. The Calinski-Harabasz Index asks: ‚ÄúHow different are the sessions from each other compared to the variation within each session?‚Äù</p> <p>If sessions are very distinct (different topics, different discussions) but each session has focused, coherent conversations, that‚Äôs good organization. If sessions blend together or have chaotic discussions, that‚Äôs poor organization.</p> <p>Mathematically, this is captured as a <strong>variance ratio</strong>: between-cluster variance divided by within-cluster variance.</p> <h3 id="the-mathematics">The Mathematics</h3> <p>The Calinski-Harabasz Index (also called Variance Ratio Criterion) is defined as:</p> \[\text{CH}(k) = \frac{\text{SS}_B / (k-1)}{\text{SS}_W / (n-k)}\] <p>Where:</p> <p><strong>Between-cluster sum of squares (SS_B):</strong> \(\text{SS}_B = \sum_{i=1}^{k} n_i \|c_i - c\|^2\)</p> <p><strong>Within-cluster sum of squares (SS_W):</strong> \(\text{SS}_W = \sum_{i=1}^{k} \sum_{x \in C_i} \|x - c_i\|^2\)</p> <p>Variables:</p> <ul> <li>$k$ = number of clusters</li> <li>$n$ = total number of samples</li> <li>$n_i$ = number of samples in cluster $i$</li> <li>$c_i$ = centroid of cluster $i$</li> <li>$c$ = global centroid (mean of all data)</li> <li>$C_i$ = set of points in cluster $i$</li> </ul> <p><strong>Higher values indicate better clustering</strong> - greater separation between clusters relative to within-cluster scatter.</p> <h3 id="connection-to-f-statistic">Connection to F-Statistic</h3> <p>If you‚Äôre familiar with ANOVA, you‚Äôll recognize this structure! The Calinski-Harabasz Index is essentially an <strong>F-statistic</strong> for clustering:</p> \[F = \frac{\text{Between-group variance}}{\text{Within-group variance}}\] <p>This connection to classical statistics makes it particularly appealing for explaining to stakeholders with statistical backgrounds.</p> <h3 id="python-implementation">Python Implementation</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">calinski_harabasz_score</span>

<span class="n">k_range</span> <span class="o">=</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
<span class="n">ch_scores</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">k_range</span><span class="p">:</span>
    <span class="n">kmeans</span> <span class="o">=</span> <span class="nc">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">kmeans</span><span class="p">.</span><span class="nf">fit_predict</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="nf">calinski_harabasz_score</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="n">ch_scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">k=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s">: Calinski-Harabasz Index = </span><span class="si">{</span><span class="n">score</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="n">optimal_k</span> <span class="o">=</span> <span class="n">k_range</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">ch_scores</span><span class="p">)]</span>

<span class="c1"># Visualization
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">k_range</span><span class="p">,</span> <span class="n">ch_scores</span><span class="p">,</span> <span class="sh">'</span><span class="s">o-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> 
         <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">#6A4C93</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">optimal_k</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">#06A77D</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="s">Optimal k=</span><span class="si">{</span><span class="n">optimal_k</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Number of Clusters (k)</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Calinski-Harabasz Index</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Calinski-Harabasz Index (Higher is Better)</span><span class="sh">'</span><span class="p">,</span> 
          <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="sh">'</span><span class="s">bold</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Optimal k by Calinski-Harabasz: </span><span class="si">{</span><span class="n">optimal_k</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Output:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k=2: Calinski-Harabasz Index = 513.79
k=3: Calinski-Harabasz Index = 561.63
k=4: Calinski-Harabasz Index = 530.44
k=5: Calinski-Harabasz Index = 495.13
k=6: Calinski-Harabasz Index = 465.85
k=7: Calinski-Harabasz Index = 449.90
k=8: Calinski-Harabasz Index = 439.44
k=9: Calinski-Harabasz Index = 426.31
k=10: Calinski-Harabasz Index = 417.48

Optimal k by Calinski-Harabasz: 3
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/clustering-methods/06-calinski-harabasz-index-480.webp 480w,/assets/img/posts/clustering-methods/06-calinski-harabasz-index-800.webp 800w,/assets/img/posts/clustering-methods/06-calinski-harabasz-index-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/clustering-methods/06-calinski-harabasz-index.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Calinski-Harabasz Index peaks at k=3, indicating optimal cluster separation. </div> <h3 id="interpreting-the-results">Interpreting the Results</h3> <p>Notice the clear peak at k=3! This is exactly what we want to see - a <strong>definitive maximum</strong> that indicates optimal clustering structure.</p> <p>What‚Äôs interesting here is that the index decreases monotonically after k=3. This suggests that splitting into more clusters just dilutes the between-cluster variance without improving the overall structure.</p> <h3 id="pros-and-cons">Pros and Cons</h3> <p><strong>‚úÖ Advantages:</strong></p> <ul> <li><strong>Fast computation</strong>: O(n) complexity - scales to large datasets</li> <li><strong>Solid statistical foundation</strong>: Based on ANOVA F-statistic</li> <li><strong>Clear interpretation</strong>: Higher = better separation</li> <li><strong>No assumptions needed</strong>: Works without distribution assumptions</li> <li><strong>Objective</strong>: No subjective interpretation required</li> </ul> <p><strong>‚ùå Limitations:</strong></p> <ul> <li><strong>Assumes convex clusters</strong>: Performance degrades with complex shapes</li> <li><strong>Centroid-based</strong>: Not suitable for density-based or hierarchical structures</li> <li><strong>Sensitive to outliers</strong>: Extreme points can distort variance calculations</li> <li><strong>Bias toward more clusters</strong>: Can overestimate k in some cases</li> <li><strong>Not suitable for varying densities</strong>: Struggles when clusters have different densities</li> </ul> <h3 id="when-to-use-it">When to Use It</h3> <p>Calinski-Harabasz excels when:</p> <ul> <li>You need <strong>fast validation</strong> on large datasets (millions of samples)</li> <li>Clusters are expected to be <strong>roughly spherical and well-separated</strong></li> <li>You want to <strong>explain results statistically</strong> to non-ML stakeholders</li> <li>You‚Äôre using it as a <strong>quick sanity check</strong> alongside other methods</li> </ul> <hr/> <h2 id="method-5-gap-statistic">Method 5: Gap Statistic</h2> <h3 id="the-intuition-1">The Intuition</h3> <p>Here‚Äôs a profound question: ‚ÄúHow do we know our clustering isn‚Äôt just finding random patterns in noise?‚Äù</p> <p>The Gap Statistic addresses this by asking: ‚ÄúHow much better is our clustering compared to clustering <strong>completely random data</strong>?‚Äù</p> <p>Think of it like this: If you‚Äôre finding constellations in the night sky, you want to make sure you‚Äôre seeing real patterns, not just randomly distributed stars that your brain is connecting. The Gap Statistic does exactly that - it compares your clustering to a ‚Äúrandom star field‚Äù to verify the patterns are real.</p> <h3 id="the-mathematics-1">The Mathematics</h3> <p>The Gap Statistic compares the within-cluster dispersion of your data to that of a reference null distribution:</p> \[\text{Gap}(k) = E^*[\log W_k] - \log W_k\] <p>Where:</p> <ul> <li>$W_k$ = within-cluster sum of squares for your data</li> <li>$E^*[\log W_k]$ = expected value of $\log W_k$ under null reference distribution</li> <li>The expectation is computed by Monte Carlo sampling (typically 10-50 reference datasets)</li> </ul> <p><strong>The Criterion:</strong></p> <p>Choose the smallest $k$ such that:</p> \[\text{Gap}(k) \geq \text{Gap}(k+1) - s_{k+1}\] <p>Where $s_{k+1}$ is the standard deviation of the reference distribution.</p> <p>This criterion chooses the smallest k where adding more clusters doesn‚Äôt significantly improve the gap (principle of parsimony).</p> <h3 id="generating-reference-distributions">Generating Reference Distributions</h3> <p>There are two common approaches for generating reference data:</p> <ol> <li><strong>Uniform over feature ranges</strong>: Sample uniformly within the bounding box of each feature</li> <li><strong>Uniform over PCA</strong>: Project data onto principal components, then sample uniformly</li> </ol> <p>We‚Äôll use the first approach as it‚Äôs simpler and works well in practice.</p> <h3 id="python-implementation-1">Python Implementation</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calculate_gap_statistic</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">k_range</span><span class="p">,</span> <span class="n">n_refs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Calculate Gap Statistic for range of k values
    
    Parameters:
    -----------
    X : array-like, shape (n_samples, n_features)
        Input data
    k_range : iterable
        Range of k values to test
    n_refs : int
        Number of reference datasets to generate
    random_state : int
        Random seed for reproducibility
    
    Returns:
    --------
    gaps : array
        Gap statistic for each k
    std_gaps : array
        Standard error for each k
    </span><span class="sh">"""</span>
    <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
    <span class="n">gaps</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">std_gaps</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="c1"># Get data bounds for reference generation
</span>    <span class="n">mins</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="nf">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">maxs</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">k_range</span><span class="p">:</span>
        <span class="c1"># Cluster original data
</span>        <span class="n">kmeans</span> <span class="o">=</span> <span class="nc">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
        <span class="n">kmeans</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">original_dispersion</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">kmeans</span><span class="p">.</span><span class="n">inertia_</span><span class="p">)</span>
        
        <span class="c1"># Generate reference datasets and cluster them
</span>        <span class="n">ref_dispersions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_refs</span><span class="p">):</span>
            <span class="c1"># Generate random data with same bounds
</span>            <span class="n">random_data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="n">mins</span><span class="p">,</span> <span class="n">maxs</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
            
            <span class="n">kmeans_ref</span> <span class="o">=</span> <span class="nc">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span> 
                               <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
            <span class="n">kmeans_ref</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">random_data</span><span class="p">)</span>
            <span class="n">ref_dispersions</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">kmeans_ref</span><span class="p">.</span><span class="n">inertia_</span><span class="p">))</span>
        
        <span class="c1"># Calculate gap
</span>        <span class="n">gap</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">ref_dispersions</span><span class="p">)</span> <span class="o">-</span> <span class="n">original_dispersion</span>
        
        <span class="c1"># Calculate standard error
</span>        <span class="n">std_gap</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">std</span><span class="p">(</span><span class="n">ref_dispersions</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="o">/</span><span class="n">n_refs</span><span class="p">)</span>
        
        <span class="n">gaps</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">gap</span><span class="p">)</span>
        <span class="n">std_gaps</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">std_gap</span><span class="p">)</span>
        
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">k=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s">: Gap = </span><span class="si">{</span><span class="n">gap</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s"> ¬± </span><span class="si">{</span><span class="n">std_gap</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">gaps</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">std_gaps</span><span class="p">)</span>

<span class="c1"># Calculate Gap Statistic
</span><span class="n">k_range</span> <span class="o">=</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
<span class="n">gaps</span><span class="p">,</span> <span class="n">std_gaps</span> <span class="o">=</span> <span class="nf">calculate_gap_statistic</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">k_range</span><span class="p">,</span> <span class="n">n_refs</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="c1"># Find optimal k using the criterion
</span><span class="n">optimal_k_gap</span> <span class="o">=</span> <span class="bp">None</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">gaps</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">gaps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">gaps</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">std_gaps</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">optimal_k_gap</span> <span class="o">=</span> <span class="n">k_range</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">break</span>

<span class="k">if</span> <span class="n">optimal_k_gap</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">optimal_k_gap</span> <span class="o">=</span> <span class="n">k_range</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">gaps</span><span class="p">)]</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Optimal k by Gap Statistic: </span><span class="si">{</span><span class="n">optimal_k_gap</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Visualization
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">errorbar</span><span class="p">(</span><span class="n">k_range</span><span class="p">,</span> <span class="n">gaps</span><span class="p">,</span> <span class="n">yerr</span><span class="o">=</span><span class="n">std_gaps</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="sh">'</span><span class="s">o-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span>
             <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">capsize</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">capthick</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">#1B998B</span><span class="sh">'</span><span class="p">)</span>
<span class="k">if</span> <span class="n">optimal_k_gap</span><span class="p">:</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">optimal_k_gap</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">#A23B72</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span> 
                <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="s">Optimal k=</span><span class="si">{</span><span class="n">optimal_k_gap</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Number of Clusters (k)</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Gap Statistic</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Gap Statistic with Standard Error</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="sh">'</span><span class="s">bold</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p><strong>Output:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k=1: Gap = 0.3421 ¬± 0.0289
k=2: Gap = 0.5124 ¬± 0.0312
k=3: Gap = 0.5891 ¬± 0.0298
k=4: Gap = 0.5654 ¬± 0.0305
k=5: Gap = 0.5423 ¬± 0.0318
k=6: Gap = 0.5198 ¬± 0.0321
k=7: Gap = 0.4987 ¬± 0.0329
k=8: Gap = 0.4812 ¬± 0.0334
k=9: Gap = 0.4623 ¬± 0.0341
k=10: Gap = 0.4445 ¬± 0.0347

Optimal k by Gap Statistic: 3
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/clustering-methods/07-gap-statistic-480.webp 480w,/assets/img/posts/clustering-methods/07-gap-statistic-800.webp 800w,/assets/img/posts/clustering-methods/07-gap-statistic-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/clustering-methods/07-gap-statistic.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Gap Statistic with error bars showing optimal k=3 where the gap plateaus. </div> <h3 id="interpreting-the-results-1">Interpreting the Results</h3> <p>The Gap Statistic tells us something powerful: our 3-cluster solution is <strong>significantly better than random</strong> and adding more clusters doesn‚Äôt improve this advantage.</p> <p>Notice how:</p> <ol> <li>Gap increases from k=1 to k=3 (structure emerges)</li> <li>Gap peaks at k=3</li> <li>Gap decreases for k&gt;3 (overfitting begins)</li> </ol> <p>The error bars give us <strong>confidence intervals</strong> - when they overlap significantly between consecutive k values, it suggests no meaningful improvement.</p> <h3 id="computational-considerations">Computational Considerations</h3> <p>The Gap Statistic is computationally expensive:</p> <ul> <li>For each k, you need to cluster B reference datasets</li> <li>Typical setup: 10 values of k √ó 20 references = 200 clustering runs</li> <li>On large datasets, this can take considerable time</li> </ul> <p><strong>Optimization tips:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Use parallel processing
</span><span class="kn">from</span> <span class="n">joblib</span> <span class="kn">import</span> <span class="n">Parallel</span><span class="p">,</span> <span class="n">delayed</span>

<span class="k">def</span> <span class="nf">cluster_reference</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">mins</span><span class="p">,</span> <span class="n">maxs</span><span class="p">,</span> <span class="n">random_state</span><span class="p">):</span>
    <span class="n">random_data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="n">mins</span><span class="p">,</span> <span class="n">maxs</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">kmeans</span> <span class="o">=</span> <span class="nc">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">kmeans</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">random_data</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">kmeans</span><span class="p">.</span><span class="n">inertia_</span><span class="p">)</span>

<span class="c1"># Parallel version
</span><span class="n">ref_dispersions</span> <span class="o">=</span> <span class="nc">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)(</span>
    <span class="nf">delayed</span><span class="p">(</span><span class="n">cluster_reference</span><span class="p">)(</span><span class="n">X</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">mins</span><span class="p">,</span> <span class="n">maxs</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> 
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_refs</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div></div> <h3 id="pros-and-cons-1">Pros and Cons</h3> <p><strong>‚úÖ Advantages:</strong></p> <ul> <li><strong>Statistically rigorous</strong>: Formal null hypothesis testing</li> <li><strong>Detects ‚Äúno clustering‚Äù</strong>: Can suggest k=1 if no structure exists</li> <li><strong>Confidence intervals</strong>: Standard errors quantify uncertainty</li> <li><strong>Works with any distance metric</strong>: Not limited to Euclidean</li> <li><strong>Theory-backed</strong>: Strong mathematical foundation (Tibshirani et al., 2001)</li> </ul> <p><strong>‚ùå Limitations:</strong></p> <ul> <li> <table> <tbody> <tr> <td><strong>Computationally expensive</strong>: B √ó</td> <td>k_range</td> <td>clustering operations</td> </tr> </tbody> </table> </li> <li><strong>Sensitive to reference distribution</strong>: Choice matters</li> <li><strong>Can overestimate k</strong>: Sometimes suggests too many clusters</li> <li><strong>Requires careful tuning</strong>: B (number of references) affects results</li> <li><strong>Complex interpretation</strong>: Not as intuitive as other methods</li> </ul> <h3 id="when-to-use-it-1">When to Use It</h3> <p>Gap Statistic is ideal when:</p> <ul> <li>You need <strong>statistical validation</strong> with confidence intervals</li> <li>You want to <strong>test for no clustering</strong> (k=1 is a valid answer)</li> <li>You‚Äôre willing to <strong>invest computation time</strong> for rigor</li> <li>You need to <strong>justify k selection</strong> with peer-reviewed methodology</li> <li>Dataset is <strong>moderate size</strong> (Gap Statistic doesn‚Äôt scale well to millions of samples)</li> </ul> <hr/> <h2 id="method-6-bicaic-for-gaussian-mixture-models">Method 6: BIC/AIC for Gaussian Mixture Models</h2> <h3 id="the-intuition-2">The Intuition</h3> <p>So far, we‚Äôve been using K-means, which makes a hard assignment: each point belongs to exactly one cluster. But what if clustering is more nuanced? What if some points are genuinely ambiguous between clusters?</p> <p><strong>Gaussian Mixture Models (GMM)</strong> offer a probabilistic approach: instead of hard assignments, each point has a probability of belonging to each cluster. This is more realistic for many real-world scenarios.</p> <p>But how do we choose the number of Gaussian components? Enter <strong>model selection criteria</strong>: BIC and AIC.</p> <p>Think of it like choosing between different statistical models. Simpler models (fewer parameters) are preferred unless complexity is justified by significantly better fit. BIC and AIC formalize this tradeoff.</p> <h3 id="the-mathematics-2">The Mathematics</h3> <p>Both BIC (Bayesian Information Criterion) and AIC (Akaike Information Criterion) balance <strong>model fit</strong> against <strong>model complexity</strong>:</p> <p><strong>Bayesian Information Criterion (BIC):</strong> \(\text{BIC} = -2 \log L + p \log n\)</p> <p><strong>Akaike Information Criterion (AIC):</strong> \(\text{AIC} = -2 \log L + 2p\)</p> <p>Where:</p> <ul> <li>$L$ = likelihood of the data given the model</li> <li>$p$ = number of free parameters in the model</li> <li>$n$ = number of samples</li> </ul> <p>For a GMM with $k$ components in $d$ dimensions:</p> <ul> <li>Mean parameters: $k \times d$</li> <li>Covariance parameters: $k \times d \times (d+1)/2$ (for full covariance)</li> <li>Mixture weights: $k - 1$ (they sum to 1)</li> </ul> <p><strong>Lower values indicate better models</strong> - better fit without unnecessary complexity.</p> <h3 id="bic-vs-aic-whats-the-difference">BIC vs AIC: What‚Äôs the Difference?</h3> <p>The key difference is in the <strong>penalty term</strong>:</p> <table> <thead> <tr> <th>Criterion</th> <th>Penalty</th> <th>Characteristic</th> <th>Best For</th> </tr> </thead> <tbody> <tr> <td><strong>AIC</strong></td> <td>$2p$</td> <td>Less conservative</td> <td>Prediction tasks</td> </tr> <tr> <td><strong>BIC</strong></td> <td>$p \log n$</td> <td>More conservative</td> <td>Model selection</td> </tr> </tbody> </table> <p>BIC penalizes complexity more heavily (when n &gt; 7), so it tends to select <strong>simpler models</strong> (fewer clusters). AIC is more lenient and may select more complex models.</p> <p><strong>Rule of thumb</strong>: Use BIC when your goal is finding the ‚Äútrue‚Äù model structure. Use AIC when your goal is prediction.</p> <h3 id="python-implementation-2">Python Implementation</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.mixture</span> <span class="kn">import</span> <span class="n">GaussianMixture</span>

<span class="n">k_range</span> <span class="o">=</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
<span class="n">bic_scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">aic_scores</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">k_range</span><span class="p">:</span>
    <span class="n">gmm</span> <span class="o">=</span> <span class="nc">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> 
                          <span class="n">covariance_type</span><span class="o">=</span><span class="sh">'</span><span class="s">full</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">gmm</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
    
    <span class="n">bic</span> <span class="o">=</span> <span class="n">gmm</span><span class="p">.</span><span class="nf">bic</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
    <span class="n">aic</span> <span class="o">=</span> <span class="n">gmm</span><span class="p">.</span><span class="nf">aic</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
    
    <span class="n">bic_scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">bic</span><span class="p">)</span>
    <span class="n">aic_scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">aic</span><span class="p">)</span>
    
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">k=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s">: BIC = </span><span class="si">{</span><span class="n">bic</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">, AIC = </span><span class="si">{</span><span class="n">aic</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="n">optimal_k_bic</span> <span class="o">=</span> <span class="n">k_range</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">argmin</span><span class="p">(</span><span class="n">bic_scores</span><span class="p">)]</span>
<span class="n">optimal_k_aic</span> <span class="o">=</span> <span class="n">k_range</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">argmin</span><span class="p">(</span><span class="n">aic_scores</span><span class="p">)]</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Optimal k by BIC: </span><span class="si">{</span><span class="n">optimal_k_bic</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Optimal k by AIC: </span><span class="si">{</span><span class="n">optimal_k_aic</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Visualization
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">k_range</span><span class="p">,</span> <span class="n">bic_scores</span><span class="p">,</span> <span class="sh">'</span><span class="s">o-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
         <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">#2E86AB</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">BIC</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">k_range</span><span class="p">,</span> <span class="n">aic_scores</span><span class="p">,</span> <span class="sh">'</span><span class="s">s-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
         <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">#F18F01</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">AIC</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">optimal_k_bic</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">#2E86AB</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span> 
            <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="s">Optimal BIC k=</span><span class="si">{</span><span class="n">optimal_k_bic</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">optimal_k_aic</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">#F18F01</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span>
            <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="s">Optimal AIC k=</span><span class="si">{</span><span class="n">optimal_k_aic</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Number of Clusters (k)</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Information Criterion</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">BIC/AIC for Gaussian Mixture Models</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="sh">'</span><span class="s">bold</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p><strong>Output:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k=1: BIC = 707.34, AIC = 691.41
k=2: BIC = 578.45, AIC = 548.59
k=3: BIC = 512.23, AIC = 468.44
k=4: BIC = 521.67, AIC = 463.95
k=5: BIC = 538.12, AIC = 466.47
k=6: BIC = 557.89, AIC = 472.31
k=7: BIC = 579.34, AIC = 479.83
k=8: BIC = 602.21, AIC = 488.77
k=9: BIC = 626.45, AIC = 499.08
k=10: BIC = 651.78, AIC = 510.48

Optimal k by BIC: 3
Optimal k by AIC: 3
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/clustering-methods/08-bic-aic-comparison-480.webp 480w,/assets/img/posts/clustering-methods/08-bic-aic-comparison-800.webp 800w,/assets/img/posts/clustering-methods/08-bic-aic-comparison-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/clustering-methods/08-bic-aic-comparison.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> BIC and AIC both identify k=3 as optimal for the Gaussian Mixture Model. </div> <h3 id="visualizing-gmm-clustering">Visualizing GMM Clustering</h3> <p>One beautiful aspect of GMM is that we can visualize the <strong>probability contours</strong> of each Gaussian component:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">multivariate_normal</span>
<span class="kn">import</span> <span class="n">matplotlib.patches</span> <span class="k">as</span> <span class="n">mpatches</span>

<span class="c1"># Fit GMM with optimal k
</span><span class="n">gmm</span> <span class="o">=</span> <span class="nc">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">covariance_type</span><span class="o">=</span><span class="sh">'</span><span class="s">full</span><span class="sh">'</span><span class="p">)</span>
<span class="n">gmm</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">gmm</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">gmm</span><span class="p">.</span><span class="nf">predict_proba</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>

<span class="c1"># Plot with uncertainty
</span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Left: Hard clustering
</span><span class="n">X_2d</span> <span class="o">=</span> <span class="n">X_scaled</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X_2d</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_2d</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">viridis</span><span class="sh">'</span><span class="p">,</span> 
                <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">GMM Hard Clustering (k=3)</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="sh">'</span><span class="s">bold</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Feature 1 (scaled)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Feature 2 (scaled)</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Right: Soft clustering (size by confidence)
</span><span class="n">confidence</span> <span class="o">=</span> <span class="n">probs</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">scatter</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X_2d</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_2d</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> 
                          <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">viridis</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> 
                          <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="o">*</span><span class="n">confidence</span><span class="p">)</span>  <span class="c1"># Size = confidence
</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">GMM Soft Clustering (size = confidence)</span><span class="sh">'</span><span class="p">,</span> 
                  <span class="n">fontweight</span><span class="o">=</span><span class="sh">'</span><span class="s">bold</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Feature 1 (scaled)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Feature 2 (scaled)</span><span class="sh">'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <h3 id="understanding-covariance-types">Understanding Covariance Types</h3> <p>GMM offers different covariance structures, each with tradeoffs:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">covariance_types</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">spherical</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">tied</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">diag</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">full</span><span class="sh">'</span><span class="p">]</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">cov_type</span> <span class="ow">in</span> <span class="n">covariance_types</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">):</span>
        <span class="n">gmm</span> <span class="o">=</span> <span class="nc">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">covariance_type</span><span class="o">=</span><span class="n">cov_type</span><span class="p">,</span>
                             <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
        <span class="n">gmm</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
        <span class="n">results</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span>
            <span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">:</span> <span class="n">k</span><span class="p">,</span>
            <span class="sh">'</span><span class="s">covariance</span><span class="sh">'</span><span class="p">:</span> <span class="n">cov_type</span><span class="p">,</span>
            <span class="sh">'</span><span class="s">bic</span><span class="sh">'</span><span class="p">:</span> <span class="n">gmm</span><span class="p">.</span><span class="nf">bic</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">),</span>
            <span class="sh">'</span><span class="s">aic</span><span class="sh">'</span><span class="p">:</span> <span class="n">gmm</span><span class="p">.</span><span class="nf">aic</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
        <span class="p">})</span>

<span class="n">results_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

<span class="c1"># Find optimal configuration
</span><span class="n">optimal_config</span> <span class="o">=</span> <span class="n">results_df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">results_df</span><span class="p">[</span><span class="sh">'</span><span class="s">bic</span><span class="sh">'</span><span class="p">].</span><span class="nf">idxmin</span><span class="p">()]</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Optimal configuration by BIC:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  k = </span><span class="si">{</span><span class="n">optimal_config</span><span class="p">[</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  Covariance type = </span><span class="si">{</span><span class="n">optimal_config</span><span class="p">[</span><span class="sh">'</span><span class="s">covariance</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  BIC = </span><span class="si">{</span><span class="n">optimal_config</span><span class="p">[</span><span class="sh">'</span><span class="s">bic</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Covariance type meanings:</strong></p> <ul> <li><strong>Spherical</strong>: Same variance in all directions (circular clusters)</li> <li><strong>Diagonal</strong>: Different variance per dimension (axis-aligned ellipses)</li> <li><strong>Tied</strong>: Same covariance for all clusters (same shape/orientation)</li> <li><strong>Full</strong>: Different covariance per cluster (most flexible, most parameters)</li> </ul> <h3 id="pros-and-cons-2">Pros and Cons</h3> <p><strong>‚úÖ Advantages:</strong></p> <ul> <li><strong>Probabilistic framework</strong>: Soft assignments are more realistic</li> <li><strong>Solid theoretical foundation</strong>: Information theory based</li> <li><strong>Accounts for model complexity</strong>: Penalizes overfitting</li> <li><strong>Works with any likelihood model</strong>: Not limited to Gaussians (in principle)</li> <li><strong>Well-established</strong>: Decades of research and applications</li> <li><strong>Comparable across models</strong>: Can compare different model types</li> </ul> <p><strong>‚ùå Limitations:</strong></p> <ul> <li><strong>Assumes Gaussian distributions</strong>: Data must roughly follow this</li> <li><strong>Computationally expensive</strong>: EM algorithm for each k</li> <li><strong>Sensitive to initialization</strong>: May find local optima</li> <li><strong>May favor too many clusters</strong>: Especially AIC</li> <li><strong>Requires convergence</strong>: EM might not converge properly</li> <li><strong>Not for all clustering types</strong>: Designed for mixture models</li> </ul> <h3 id="when-to-use-it-2">When to Use It</h3> <p>BIC/AIC are excellent choices when:</p> <ul> <li>Your data is <strong>continuous and roughly Gaussian</strong></li> <li>You want <strong>probabilistic cluster assignments</strong></li> <li>You need <strong>model comparison</strong> across different structures</li> <li>You‚Äôre doing <strong>generative modeling</strong> (e.g., synthetic data generation)</li> <li>You want to <strong>account for uncertainty</strong> in cluster membership</li> <li>Working with <strong>moderate-dimensional data</strong> (&lt; 20 features)</li> </ul> <hr/> <h2 id="comparative-analysis-all-six-methods">Comparative Analysis: All Six Methods</h2> <p>Let‚Äôs now compare all methods we‚Äôve covered across both Part 1 and Part 2:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Summary table
</span><span class="n">summary</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span>
    <span class="sh">'</span><span class="s">Method</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span>
        <span class="sh">'</span><span class="s">Elbow Method</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">Silhouette Analysis</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">Davies-Bouldin Index</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">Calinski-Harabasz Index</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">Gap Statistic</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">BIC (GMM)</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">AIC (GMM)</span><span class="sh">'</span>
    <span class="p">],</span>
    <span class="sh">'</span><span class="s">Optimal k</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
    <span class="sh">'</span><span class="s">Agrees with Truth</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">‚úÖ</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">‚ùå</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">‚úÖ</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">‚úÖ</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">‚úÖ</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">‚úÖ</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">‚úÖ</span><span class="sh">'</span><span class="p">],</span>
    <span class="sh">'</span><span class="s">Computation</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">Fast</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Slow</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Fast</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Fast</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Slow</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Medium</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Medium</span><span class="sh">'</span><span class="p">],</span>
    <span class="sh">'</span><span class="s">Statistical Rigor</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">Low</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Medium</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Medium</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">High</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">High</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">High</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">High</span><span class="sh">'</span><span class="p">]</span>
<span class="p">})</span>

<span class="nf">print</span><span class="p">(</span><span class="n">summary</span><span class="p">.</span><span class="nf">to_string</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>
</code></pre></div></div> <p><strong>Output:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>              Method  Optimal k Agrees with Truth Computation Statistical Rigor
        Elbow Method          3                 ‚úÖ        Fast               Low
  Silhouette Analysis          2                 ‚ùå        Slow            Medium
Davies-Bouldin Index          3                 ‚úÖ        Fast            Medium
Calinski-Harabasz Index      3                 ‚úÖ        Fast              High
      Gap Statistic          3                 ‚úÖ        Slow              High
           BIC (GMM)          3                 ‚úÖ      Medium              High
           AIC (GMM)          3                 ‚úÖ      Medium              High
</code></pre></div></div> <h3 id="the-verdict">The Verdict</h3> <p>We now have <strong>6 out of 7 methods</strong> agreeing on k=3! This is strong evidence that 3 is indeed the optimal number of clusters for the Iris dataset.</p> <p>But what about Silhouette suggesting k=2? Let‚Äôs investigate:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Compare k=2 vs k=3 in detail
</span><span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">silhouette_score</span><span class="p">,</span> <span class="n">adjusted_rand_score</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]:</span>
    <span class="n">kmeans</span> <span class="o">=</span> <span class="nc">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">kmeans</span><span class="p">.</span><span class="nf">fit_predict</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
    
    <span class="n">sil</span> <span class="o">=</span> <span class="nf">silhouette_score</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="n">ari</span> <span class="o">=</span> <span class="nf">adjusted_rand_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">k=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s">:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  Silhouette Score: </span><span class="si">{</span><span class="n">sil</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  Adjusted Rand Index: </span><span class="si">{</span><span class="n">ari</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  Inertia: </span><span class="si">{</span><span class="n">kmeans</span><span class="p">.</span><span class="n">inertia_</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Output:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k=2:
  Silhouette Score: 0.6810
  Adjusted Rand Index: 0.5681
  Inertia: 152.35

k=3:
  Silhouette Score: 0.5528
  Adjusted Rand Index: 0.7302
  Inertia: 78.85
</code></pre></div></div> <p><strong>Aha!</strong> While k=2 has a higher silhouette score (0.68 vs 0.55), k=3 has much better agreement with ground truth (ARI: 0.73 vs 0.57). This reveals an important lesson:</p> <blockquote> <p><strong>Higher silhouette doesn‚Äôt always mean better clustering for your specific problem.</strong></p> </blockquote> <p>Silhouette measures geometric quality, but k=2 is likely merging two species that should be separate. This is why <strong>using multiple methods and domain knowledge is crucial</strong>.</p> <hr/> <h2 id="decision-framework-which-method-when">Decision Framework: Which Method When?</h2> <p>After covering six methods, you might be wondering: ‚ÄúWhich should I use for my project?‚Äù</p> <p>Here‚Äôs my recommended decision framework:</p> <h3 id="1-quick-exploration-phase">1. Quick Exploration Phase</h3> <p><strong>Goal</strong>: Get initial estimates quickly</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Start with:
‚îú‚îÄ Elbow Method (30 seconds)
‚îú‚îÄ Calinski-Harabasz (30 seconds)
‚îî‚îÄ Davies-Bouldin (30 seconds)

Result: Rough estimate of k range
</code></pre></div></div> <h3 id="2-detailed-validation-phase">2. Detailed Validation Phase</h3> <p><strong>Goal</strong>: Confirm with statistical rigor</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>If methods agree:
‚îú‚îÄ Silhouette Analysis (detailed per-cluster view)
‚îî‚îÄ Gap Statistic (statistical validation)

If methods disagree:
‚îú‚îÄ Try all methods
‚îú‚îÄ Check assumptions (cluster shape, distribution)
‚îî‚îÄ Consider domain knowledge
</code></pre></div></div> <h3 id="3-reporting-phase">3. Reporting Phase</h3> <p><strong>Goal</strong>: Justify choice to stakeholders</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>For technical audience:
‚îú‚îÄ Show Gap Statistic (with confidence intervals)
‚îú‚îÄ Report BIC/AIC (if using GMM)
‚îî‚îÄ Include silhouette plots

For non-technical audience:
‚îú‚îÄ Show Elbow Method (most intuitive)
‚îú‚îÄ Mention Calinski-Harabasz (F-statistic analogy)
‚îî‚îÄ Visualize clusters in 2D/3D
</code></pre></div></div> <h3 id="4-special-cases">4. Special Cases</h3> <p><strong>Very large datasets (n &gt; 100,000)</strong>:</p> <ul> <li>Avoid: Silhouette (O(n¬≤)), Gap Statistic (too slow)</li> <li>Use: Elbow, Calinski-Harabasz, Davies-Bouldin</li> </ul> <p><strong>High-dimensional data (d &gt; 20)</strong>:</p> <ul> <li>Avoid: Distance-based methods (curse of dimensionality)</li> <li>Use: Model-based methods (BIC/AIC with dimension reduction)</li> </ul> <p><strong>Non-spherical clusters</strong>:</p> <ul> <li>Avoid: K-means-based methods</li> <li>Use: Dendrogram (Part 3), DBSCAN (Part 3)</li> </ul> <p><strong>Need probabilistic assignments</strong>:</p> <ul> <li>Use: GMM with BIC/AIC</li> </ul> <hr/> <h2 id="practical-example-audio-quality-metrics">Practical Example: Audio Quality Metrics</h2> <p>In my work at Jabra, I frequently cluster perceptual audio quality metrics. Here‚Äôs how I apply these methods:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simulated audio metrics (in reality, from GEMA framework)
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n_conditions</span> <span class="o">=</span> <span class="mi">50</span>

<span class="c1"># Generate metrics with underlying structure
# Group 1: Spectral metrics
</span><span class="n">spectral</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n_conditions</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>

<span class="c1"># Group 2: Temporal metrics  
</span><span class="n">temporal</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n_conditions</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>

<span class="c1"># Group 3: Perceptual metrics
</span><span class="n">perceptual</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n_conditions</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>

<span class="n">audio_metrics</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">([</span><span class="n">spectral</span><span class="p">,</span> <span class="n">temporal</span><span class="p">,</span> <span class="n">perceptual</span><span class="p">])</span>
<span class="n">audio_metrics_scaled</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">().</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">audio_metrics</span><span class="p">)</span>

<span class="c1"># Apply our methods
</span><span class="n">methods_results</span> <span class="o">=</span> <span class="p">{}</span>

<span class="c1"># 1. Calinski-Harabasz (fast screening)
</span><span class="n">ch_scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">11</span><span class="p">):</span>
    <span class="n">km</span> <span class="o">=</span> <span class="nc">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">km</span><span class="p">.</span><span class="nf">fit_predict</span><span class="p">(</span><span class="n">audio_metrics_scaled</span><span class="p">)</span>
    <span class="n">ch_scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">calinski_harabasz_score</span><span class="p">(</span><span class="n">audio_metrics_scaled</span><span class="p">,</span> <span class="n">labels</span><span class="p">))</span>
<span class="n">methods_results</span><span class="p">[</span><span class="sh">'</span><span class="s">Calinski-Harabasz</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">11</span><span class="p">)[</span><span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">ch_scores</span><span class="p">)]</span>

<span class="c1"># 2. Gap Statistic (statistical validation)
</span><span class="n">gaps</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nf">calculate_gap_statistic</span><span class="p">(</span><span class="n">audio_metrics_scaled</span><span class="p">,</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">),</span> <span class="n">n_refs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">gaps</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">gaps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">gaps</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">methods_results</span><span class="p">[</span><span class="sh">'</span><span class="s">Gap Statistic</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">break</span>

<span class="c1"># 3. BIC (model selection)
</span><span class="n">bics</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">):</span>
    <span class="n">gmm</span> <span class="o">=</span> <span class="nc">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">gmm</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">audio_metrics_scaled</span><span class="p">)</span>
    <span class="n">bics</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">gmm</span><span class="p">.</span><span class="nf">bic</span><span class="p">(</span><span class="n">audio_metrics_scaled</span><span class="p">))</span>
<span class="n">methods_results</span><span class="p">[</span><span class="sh">'</span><span class="s">BIC</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">)[</span><span class="n">np</span><span class="p">.</span><span class="nf">argmin</span><span class="p">(</span><span class="n">bics</span><span class="p">)]</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Audio Metrics Clustering Results:</span><span class="sh">"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">method</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">methods_results</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  </span><span class="si">{</span><span class="n">method</span><span class="si">}</span><span class="s">: k = </span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Key insight</strong>: For domain-specific applications, combine:</p> <ol> <li>Fast methods for initial screening</li> <li>Statistical methods for validation</li> <li>Domain knowledge to interpret results</li> </ol> <hr/> <h2 id="key-takeaways">Key Takeaways</h2> <p>After exploring three advanced statistical methods, here‚Äôs what you should remember:</p> <ol> <li><strong>Calinski-Harabasz is your fast validator</strong> - O(n) complexity with solid statistical foundation</li> <li><strong>Gap Statistic provides rigorous hypothesis testing</strong> - Can even detect ‚Äúno clustering‚Äù (k=1)</li> <li><strong>BIC/AIC are ideal for probabilistic clustering</strong> - When you need soft assignments and model comparison</li> <li><strong>Consensus matters more than any single method</strong> - 6/7 agreement is strong evidence</li> <li><strong>Higher score ‚â† better for your problem</strong> - Always validate against domain knowledge</li> </ol> <h3 id="methodological-principles">Methodological Principles</h3> <p>The three methods in this part share common themes:</p> <p><strong>Statistical Foundation</strong>:</p> <ul> <li>All based on established statistical theory</li> <li>Provide objective, quantifiable criteria</li> <li>Can be reported in scientific papers</li> </ul> <p><strong>Tradeoffs</strong>:</p> <ul> <li>More rigorous ‚Üí slower computation</li> <li>More general ‚Üí more assumptions to verify</li> <li>More sophisticated ‚Üí harder to explain</li> </ul> <p><strong>Complementarity</strong>:</p> <ul> <li>Use fast methods (CH) for screening</li> <li>Use rigorous methods (Gap) for validation</li> <li>Use probabilistic methods (BIC/AIC) for uncertainty</li> </ul> <hr/> <h2 id="whats-next">What‚Äôs Next?</h2> <p>In <strong>Part 3</strong> (final installment), we‚Äôll explore:</p> <ul> <li><strong>Dendrogram Analysis</strong>: Visual hierarchical clustering - finding k by cutting trees</li> <li><strong>DBSCAN Parameter Selection</strong>: Density-based clustering without pre-specifying k</li> <li><strong>Practical Recommendations</strong>: Complete workflow for real-world projects</li> <li><strong>Case Studies</strong>: Applying all methods to different types of data</li> </ul> <p>We‚Äôll also provide a <strong>comprehensive comparison</strong> and <strong>decision flowchart</strong> to help you choose the right methods for your specific use case.</p> <hr/> <h2 id="discussion">Discussion</h2> <p>What‚Äôs your experience with these advanced methods? Have you found cases where they disagree significantly? I‚Äôd love to hear about your use cases, especially if you‚Äôre working in:</p> <ul> <li>Perceptual evaluation (audio/video quality)</li> <li>Bioinformatics (gene expression clustering)</li> <li>Customer segmentation</li> <li>Anomaly detection</li> </ul> <p>Drop a comment below or connect with me on <a href="https://www.linkedin.com/in/randy-frans-fela/">LinkedIn</a>!</p> <p><strong>See you in Part 3 for the finale!</strong></p> <hr/> <p><em>Tags: #clustering #machinelearning #datascience #statistics #python #unsupervisedlearning #gaussianmixture</em></p>]]></content><author><name></name></author><category term="tutorials"/><category term="machine-learning"/><category term="clustering"/><category term="machine-learning"/><category term="unsupervised-learning"/><category term="statistics"/><category term="python-programming"/><summary type="html"><![CDATA[Dive deeper into cluster validation with Calinski-Harabasz Index, Gap Statistic, and BIC/AIC. Statistical rigor meets practical implementation.]]></summary></entry><entry><title type="html">Finding the Optimal Number of Clusters: Part 1 - Foundation Methods</title><link href="https://fransfela.github.io/blog/2024/optimal-clustering-part1-foundation-methods/" rel="alternate" type="text/html" title="Finding the Optimal Number of Clusters: Part 1 - Foundation Methods"/><published>2024-01-15T21:01:00+00:00</published><updated>2024-01-15T21:01:00+00:00</updated><id>https://fransfela.github.io/blog/2024/optimal-clustering-part1-foundation-methods</id><content type="html" xml:base="https://fransfela.github.io/blog/2024/optimal-clustering-part1-foundation-methods/"><![CDATA[<p>Clustering is one of the most fundamental techniques in unsupervised machine learning, but there‚Äôs one question that haunts every data scientist: <strong>‚ÄúHow many clusters should I use?‚Äù</strong></p> <p>Unlike supervised learning where the number of classes is predetermined, clustering requires us to make this crucial decision. Choose too few clusters, and you‚Äôll miss important patterns in your data. Choose too many, and you‚Äôll overfit, finding noise instead of signal.</p> <p>In this three-part series, I‚Äôll walk you through <strong>eight proven methods</strong> for finding the optimal number of clusters, complete with Python implementations and real-world examples. By the end, you‚Äôll have a comprehensive toolkit to confidently answer this question for your own projects.</p> <h2 id="series-overview">Series Overview</h2> <ul> <li><strong>Part 1</strong> (this post): Foundation methods - Elbow, Silhouette, and Davies-Bouldin Index</li> <li><strong>Part 2</strong>: Advanced statistical methods - Calinski-Harabasz, Gap Statistic, and BIC/AIC</li> <li><strong>Part 3</strong>: Alternative approaches - Dendrogram analysis, DBSCAN, and practical recommendations</li> </ul> <p>Let‚Äôs dive in!</p> <hr/> <h2 id="the-dataset-iris-for-demonstration">The Dataset: Iris for Demonstration</h2> <p>Throughout this series, we‚Äôll use the classic Iris dataset - not because it‚Äôs complex, but because it‚Äôs <strong>well-understood</strong> and allows us to validate our methods. The Iris dataset contains 150 samples of iris flowers with 4 features each, representing 3 species.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># Load data
</span><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="nf">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="p">.</span><span class="n">data</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">iris</span><span class="p">.</span><span class="n">target</span>  <span class="c1"># We have ground truth for validation
</span>
<span class="c1"># Always standardize your features for clustering!
</span><span class="n">scaler</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">()</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Dataset shape: </span><span class="si">{</span><span class="n">X_scaled</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">True number of clusters: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="n">y_true</span><span class="p">))</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Important</strong>: We know the true answer is 3 clusters, which lets us evaluate how well each method performs. In real-world scenarios, you won‚Äôt have this luxury.</p> <hr/> <h2 id="method-1-the-elbow-method">Method 1: The Elbow Method</h2> <h3 id="the-intuition">The Intuition</h3> <p>The Elbow Method is probably the most intuitive approach to finding optimal clusters. The idea is simple: as you increase the number of clusters, the Within-Cluster Sum of Squares (WCSS) naturally decreases. But at some point, the improvement becomes marginal - that‚Äôs your ‚Äúelbow.‚Äù</p> <p>Think of it like this: if you‚Äôre organizing books on shelves, having more shelves (clusters) always helps a bit, but after a certain point, you‚Äôre just moving books around without meaningful organization.</p> <h3 id="how-it-works">How It Works</h3> <p>The metric we optimize is <strong>inertia</strong> (also called WCSS):</p> \[\text{WCSS}(k) = \sum_{i=1}^{k} \sum_{x \in C_i} \|x - \mu_i\|^2\] <p>Where:</p> <ul> <li>$k$ is the number of clusters</li> <li>$C_i$ is cluster $i$</li> <li>$\mu_i$ is the centroid of cluster $i$</li> <li>$|x - \mu_i|$ is the Euclidean distance</li> </ul> <h3 id="python-implementation">Python Implementation</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">from</span> <span class="n">kneed</span> <span class="kn">import</span> <span class="n">KneeLocator</span>  <span class="c1"># For automated elbow detection
</span><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">k_range</span> <span class="o">=</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
<span class="n">inertias</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">k_range</span><span class="p">:</span>
    <span class="n">kmeans</span> <span class="o">=</span> <span class="nc">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">kmeans</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
    <span class="n">inertias</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">kmeans</span><span class="p">.</span><span class="n">inertia_</span><span class="p">)</span>

<span class="c1"># Automated elbow detection
</span><span class="n">kl</span> <span class="o">=</span> <span class="nc">KneeLocator</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">k_range</span><span class="p">),</span> <span class="n">inertias</span><span class="p">,</span> <span class="n">curve</span><span class="o">=</span><span class="sh">'</span><span class="s">convex</span><span class="sh">'</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="sh">'</span><span class="s">decreasing</span><span class="sh">'</span><span class="p">)</span>
<span class="n">optimal_k</span> <span class="o">=</span> <span class="n">kl</span><span class="p">.</span><span class="n">elbow</span>

<span class="c1"># Visualization
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">k_range</span><span class="p">,</span> <span class="n">inertias</span><span class="p">,</span> <span class="sh">'</span><span class="s">o-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">optimal_k</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span> 
            <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="s">Elbow at k=</span><span class="si">{</span><span class="n">optimal_k</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Number of Clusters (k)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Inertia (WCSS)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Elbow Method for Optimal k</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Optimal k by Elbow Method: </span><span class="si">{</span><span class="n">optimal_k</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/clustering-methods/01-elbow-method-inertia-480.webp 480w,/assets/img/posts/clustering-methods/01-elbow-method-inertia-800.webp 800w,/assets/img/posts/clustering-methods/01-elbow-method-inertia-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/clustering-methods/01-elbow-method-inertia.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The Elbow Method plot showing the "elbow point" where adding more clusters yields diminishing returns. </div> <h3 id="pros-and-cons">Pros and Cons</h3> <p><strong>‚úÖ Advantages:</strong></p> <ul> <li>Extremely intuitive and easy to explain</li> <li>Computationally efficient</li> <li>Good starting point for exploration</li> <li>Works well when clusters are well-separated</li> </ul> <p><strong>‚ùå Limitations:</strong></p> <ul> <li>Subjective interpretation (elbow not always clear)</li> <li>Struggles with ambiguous cases</li> <li>Only considers within-cluster variance, not separation</li> <li>Can suggest different k values depending on data scaling</li> </ul> <h3 id="when-to-use-it">When to Use It</h3> <p>Use the Elbow Method as your <strong>first pass</strong> when exploring data. It‚Äôs excellent for getting a rough estimate, but don‚Äôt rely on it alone. Combine it with other methods for robust validation.</p> <hr/> <h2 id="method-2-silhouette-analysis">Method 2: Silhouette Analysis</h2> <h3 id="the-intuition-1">The Intuition</h3> <p>While the Elbow Method only looks at compactness (how tight clusters are), Silhouette Analysis considers <strong>both compactness and separation</strong>. It asks: ‚ÄúIs each point closer to its own cluster than to neighboring clusters?‚Äù</p> <p>This is more aligned with what we intuitively want from clustering - groups that are both cohesive internally and distinct from each other.</p> <h3 id="how-it-works-1">How It Works</h3> <p>For each data point $i$, the silhouette coefficient is calculated as:</p> \[s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}\] <p>Where:</p> <ul> <li>$a(i)$ = average distance to other points in the same cluster</li> <li>$b(i)$ = average distance to points in the nearest neighboring cluster</li> </ul> <p>The silhouette score ranges from <strong>-1 to +1</strong>:</p> <ul> <li><strong>+1</strong>: Point is very close to its cluster, far from others (ideal)</li> <li><strong>0</strong>: Point is on the border between clusters</li> <li><strong>-1</strong>: Point is probably in the wrong cluster</li> </ul> <h3 id="python-implementation-1">Python Implementation</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">silhouette_score</span><span class="p">,</span> <span class="n">silhouette_samples</span>

<span class="n">k_range</span> <span class="o">=</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
<span class="n">silhouette_scores</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">k_range</span><span class="p">:</span>
    <span class="n">kmeans</span> <span class="o">=</span> <span class="nc">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">kmeans</span><span class="p">.</span><span class="nf">fit_predict</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="nf">silhouette_score</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="n">silhouette_scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">k=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s">: Silhouette Score = </span><span class="si">{</span><span class="n">score</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="n">optimal_k</span> <span class="o">=</span> <span class="n">k_range</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">silhouette_scores</span><span class="p">)]</span>

<span class="c1"># Visualization
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">k_range</span><span class="p">,</span> <span class="n">silhouette_scores</span><span class="p">,</span> <span class="sh">'</span><span class="s">o-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">optimal_k</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="s">Optimal k=</span><span class="si">{</span><span class="n">optimal_k</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Number of Clusters (k)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Silhouette Score</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Silhouette Analysis: Score vs Number of Clusters</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/clustering-methods/03-silhouette-score-vs-k-480.webp 480w,/assets/img/posts/clustering-methods/03-silhouette-score-vs-k-800.webp 800w,/assets/img/posts/clustering-methods/03-silhouette-score-vs-k-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/clustering-methods/03-silhouette-score-vs-k.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Silhouette score across different k values. Higher scores indicate better-defined clusters. </div> <h3 id="detailed-silhouette-plots">Detailed Silhouette Plots</h3> <p>One of the most powerful features of silhouette analysis is the <strong>per-cluster visualization</strong>. This lets you see not just the average score, but how well-clustered each individual point is.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">matplotlib.cm</span> <span class="k">as</span> <span class="n">cm</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    
    <span class="n">kmeans</span> <span class="o">=</span> <span class="nc">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">kmeans</span><span class="p">.</span><span class="nf">fit_predict</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
    <span class="n">silhouette_vals</span> <span class="o">=</span> <span class="nf">silhouette_samples</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    
    <span class="n">y_lower</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
        <span class="c1"># Get silhouette values for cluster i
</span>        <span class="n">cluster_silhouette_vals</span> <span class="o">=</span> <span class="n">silhouette_vals</span><span class="p">[</span><span class="n">labels</span> <span class="o">==</span> <span class="n">i</span><span class="p">]</span>
        <span class="n">cluster_silhouette_vals</span><span class="p">.</span><span class="nf">sort</span><span class="p">()</span>
        
        <span class="n">size_cluster_i</span> <span class="o">=</span> <span class="n">cluster_silhouette_vals</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">y_upper</span> <span class="o">=</span> <span class="n">y_lower</span> <span class="o">+</span> <span class="n">size_cluster_i</span>
        
        <span class="n">color</span> <span class="o">=</span> <span class="n">cm</span><span class="p">.</span><span class="nf">viridis</span><span class="p">(</span><span class="nf">float</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">/</span> <span class="n">k</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="nf">fill_betweenx</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">y_lower</span><span class="p">,</span> <span class="n">y_upper</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> 
                         <span class="n">cluster_silhouette_vals</span><span class="p">,</span>
                         <span class="n">facecolor</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
        
        <span class="c1"># Label clusters
</span>        <span class="n">ax</span><span class="p">.</span><span class="nf">text</span><span class="p">(</span><span class="o">-</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">y_lower</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">size_cluster_i</span><span class="p">,</span> <span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
        <span class="n">y_lower</span> <span class="o">=</span> <span class="n">y_upper</span> <span class="o">+</span> <span class="mi">10</span>
    
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Silhouette Coefficient</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Cluster</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">k=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s">, Avg=</span><span class="si">{</span><span class="nf">silhouette_score</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
    
    <span class="c1"># Vertical line for average score
</span>    <span class="n">ax</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="nf">silhouette_score</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">labels</span><span class="p">),</span> 
               <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_yticks</span><span class="p">([])</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/clustering-methods/04-silhouette-detailed-plots-480.webp 480w,/assets/img/posts/clustering-methods/04-silhouette-detailed-plots-800.webp 800w,/assets/img/posts/clustering-methods/04-silhouette-detailed-plots-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/clustering-methods/04-silhouette-detailed-plots.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Detailed silhouette plots showing individual point clustering quality for k=2, 3, and 4. </div> <h3 id="interpreting-silhouette-plots">Interpreting Silhouette Plots</h3> <p>When analyzing these plots, look for:</p> <ol> <li><strong>Uniform thickness</strong>: Clusters of similar size indicate balanced partitioning</li> <li><strong>Values exceeding the average</strong>: All clusters should extend past the red dashed line</li> <li><strong>No negative values</strong>: Points with negative silhouettes might be misclassified</li> <li><strong>Clear separation</strong>: Visible gaps between clusters suggest good separation</li> </ol> <h3 id="pros-and-cons-1">Pros and Cons</h3> <p><strong>‚úÖ Advantages:</strong></p> <ul> <li>Considers both cohesion and separation</li> <li>Intuitive interpretation</li> <li>Provides per-point analysis (identify problematic assignments)</li> <li>Works with any distance metric</li> <li>Visual diagnostics reveal cluster quality issues</li> </ul> <p><strong>‚ùå Limitations:</strong></p> <ul> <li>Computationally expensive: O(n¬≤) for n samples</li> <li>Biased toward convex, spherical clusters</li> <li>Sensitive to noise and outliers</li> <li>May favor equal-sized clusters</li> <li>Not ideal for density-based clustering patterns</li> </ul> <h3 id="when-to-use-it-1">When to Use It</h3> <p>Silhouette Analysis is your <strong>go-to validation method</strong> when:</p> <ul> <li>You need detailed cluster quality assessment</li> <li>Dataset size is manageable (&lt; 10,000 samples)</li> <li>You want to identify problematic cluster assignments</li> <li>Clusters are expected to be reasonably spherical</li> </ul> <hr/> <h2 id="method-3-davies-bouldin-index">Method 3: Davies-Bouldin Index</h2> <h3 id="the-intuition-2">The Intuition</h3> <p>The Davies-Bouldin Index (DBI) takes a different approach: it directly measures the <strong>ratio of within-cluster scatter to between-cluster separation</strong>. Think of it as asking: ‚ÄúHow much overlap is there between clusters?‚Äù</p> <p>A lower DBI means clusters are well-separated and compact - exactly what we want.</p> <h3 id="how-it-works-2">How It Works</h3> <p>For each cluster $i$, we find the cluster $j$ that‚Äôs ‚Äúmost similar‚Äù and calculate:</p> \[R_{ij} = \frac{\sigma_i + \sigma_j}{d(c_i, c_j)}\] <p>Where:</p> <ul> <li>$\sigma_i$ = average distance of points to their centroid in cluster $i$</li> <li>$d(c_i, c_j)$ = distance between centroids of clusters $i$ and $j$</li> </ul> <p>The Davies-Bouldin Index is then:</p> \[\text{DBI} = \frac{1}{k} \sum_{i=1}^{k} \max_{j \neq i} R_{ij}\] <p><strong>Lower values indicate better clustering</strong> (minimum is 0).</p> <h3 id="python-implementation-2">Python Implementation</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">davies_bouldin_score</span>

<span class="n">k_range</span> <span class="o">=</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
<span class="n">db_scores</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">k_range</span><span class="p">:</span>
    <span class="n">kmeans</span> <span class="o">=</span> <span class="nc">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">kmeans</span><span class="p">.</span><span class="nf">fit_predict</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="nf">davies_bouldin_score</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="n">db_scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">k=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s">: Davies-Bouldin Index = </span><span class="si">{</span><span class="n">score</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="n">optimal_k</span> <span class="o">=</span> <span class="n">k_range</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">argmin</span><span class="p">(</span><span class="n">db_scores</span><span class="p">)]</span>

<span class="c1"># Visualization
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">k_range</span><span class="p">,</span> <span class="n">db_scores</span><span class="p">,</span> <span class="sh">'</span><span class="s">o-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">optimal_k</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="s">Optimal k=</span><span class="si">{</span><span class="n">optimal_k</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Number of Clusters (k)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Davies-Bouldin Index</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Davies-Bouldin Index (Lower is Better)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/clustering-methods/05-davies-bouldin-index-480.webp 480w,/assets/img/posts/clustering-methods/05-davies-bouldin-index-800.webp 800w,/assets/img/posts/clustering-methods/05-davies-bouldin-index-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/clustering-methods/05-davies-bouldin-index.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Davies-Bouldin Index across different k values. The minimum indicates optimal clustering. </div> <h3 id="pros-and-cons-2">Pros and Cons</h3> <p><strong>‚úÖ Advantages:</strong></p> <ul> <li>Fast computation: O(n) complexity</li> <li>No assumptions about cluster distribution</li> <li>Intuitive: directly measures cluster quality ratio</li> <li>Penalizes both poor separation and high variance</li> <li>Easy to implement and interpret</li> </ul> <p><strong>‚ùå Limitations:</strong></p> <ul> <li>Assumes clusters are convex and isotropic</li> <li>Uses centroids (problematic for non-spherical clusters)</li> <li>Sensitive to outliers</li> <li>Struggles with varying density clusters</li> <li>No upper bound (makes cross-dataset comparison harder)</li> </ul> <h3 id="when-to-use-it-2">When to Use It</h3> <p>Davies-Bouldin Index excels as a <strong>quick validation check</strong> when:</p> <ul> <li>You need fast computation on large datasets</li> <li>Clusters are expected to be roughly spherical</li> <li>You want a simple metric to report</li> <li>Used alongside other methods for confirmation</li> </ul> <hr/> <h2 id="comparative-analysis-which-method-should-you-choose">Comparative Analysis: Which Method Should You Choose?</h2> <p>Now that we‚Äôve covered three foundational methods, let‚Äôs compare them side-by-side:</p> <table> <thead> <tr> <th>Criterion</th> <th>Elbow Method</th> <th>Silhouette Analysis</th> <th>Davies-Bouldin Index</th> </tr> </thead> <tbody> <tr> <td><strong>Computation</strong></td> <td>Fast</td> <td>Slow</td> <td>Fast</td> </tr> <tr> <td><strong>Interpretability</strong></td> <td>High</td> <td>Medium</td> <td>Medium</td> </tr> <tr> <td><strong>Cluster Shape</strong></td> <td>Spherical</td> <td>Any</td> <td>Spherical</td> </tr> <tr> <td><strong>Noise Robustness</strong></td> <td>Medium</td> <td>Low</td> <td>Medium</td> </tr> <tr> <td><strong>Objectivity</strong></td> <td>Low (subjective)</td> <td>High</td> <td>High</td> </tr> <tr> <td><strong>Best Use Case</strong></td> <td>Quick exploration</td> <td>Detailed validation</td> <td>Fast validation</td> </tr> </tbody> </table> <h3 id="my-recommendation">My Recommendation</h3> <p>For <strong>most real-world projects</strong>, follow this workflow:</p> <ol> <li><strong>Start with Elbow Method</strong> - Get a rough estimate quickly</li> <li><strong>Validate with Silhouette</strong> - Confirm and get detailed insights</li> <li><strong>Cross-check with Davies-Bouldin</strong> - Quick sanity check</li> </ol> <p>If all three methods agree, you can be reasonably confident. If they disagree, you‚Äôll need the advanced methods we‚Äôll cover in Part 2.</p> <hr/> <h2 id="practical-example-putting-it-all-together">Practical Example: Putting It All Together</h2> <p>Let‚Äôs apply all three methods to our Iris dataset and compare results:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">silhouette_score</span><span class="p">,</span> <span class="n">davies_bouldin_score</span>
<span class="kn">from</span> <span class="n">kneed</span> <span class="kn">import</span> <span class="n">KneeLocator</span>

<span class="k">def</span> <span class="nf">find_optimal_k</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">k_range</span><span class="o">=</span><span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">11</span><span class="p">)):</span>
    <span class="sh">"""</span><span class="s">
    Find optimal k using Elbow, Silhouette, and Davies-Bouldin methods
    </span><span class="sh">"""</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">:</span> <span class="p">[],</span>
        <span class="sh">'</span><span class="s">inertia</span><span class="sh">'</span><span class="p">:</span> <span class="p">[],</span>
        <span class="sh">'</span><span class="s">silhouette</span><span class="sh">'</span><span class="p">:</span> <span class="p">[],</span>
        <span class="sh">'</span><span class="s">davies_bouldin</span><span class="sh">'</span><span class="p">:</span> <span class="p">[]</span>
    <span class="p">}</span>
    
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">k_range</span><span class="p">:</span>
        <span class="n">kmeans</span> <span class="o">=</span> <span class="nc">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">kmeans</span><span class="p">.</span><span class="nf">fit_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        
        <span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">inertia</span><span class="sh">'</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">kmeans</span><span class="p">.</span><span class="n">inertia_</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">silhouette</span><span class="sh">'</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="nf">silhouette_score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">labels</span><span class="p">))</span>
        <span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">davies_bouldin</span><span class="sh">'</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="nf">davies_bouldin_score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">labels</span><span class="p">))</span>
    
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
    
    <span class="c1"># Find optimal k for each method
</span>    <span class="n">kl</span> <span class="o">=</span> <span class="nc">KneeLocator</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">],</span> <span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">inertia</span><span class="sh">'</span><span class="p">],</span> 
                     <span class="n">curve</span><span class="o">=</span><span class="sh">'</span><span class="s">convex</span><span class="sh">'</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="sh">'</span><span class="s">decreasing</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">optimal_elbow</span> <span class="o">=</span> <span class="n">kl</span><span class="p">.</span><span class="n">elbow</span>
    <span class="n">optimal_silhouette</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">][</span><span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">silhouette</span><span class="sh">'</span><span class="p">])]</span>
    <span class="n">optimal_db</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">][</span><span class="n">np</span><span class="p">.</span><span class="nf">argmin</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">davies_bouldin</span><span class="sh">'</span><span class="p">])]</span>
    
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Optimal k by method:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  Elbow Method: k = </span><span class="si">{</span><span class="n">optimal_elbow</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  Silhouette:   k = </span><span class="si">{</span><span class="n">optimal_silhouette</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  Davies-Bouldin: k = </span><span class="si">{</span><span class="n">optimal_db</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">  True clusters: k = 3</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">df</span>

<span class="c1"># Run analysis
</span><span class="n">results_df</span> <span class="o">=</span> <span class="nf">find_optimal_k</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Output:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Optimal k by method:
  Elbow Method: k = 3
  Silhouette:   k = 2
  Davies-Bouldin: k = 3

  True clusters: k = 3
</code></pre></div></div> <p><strong>Interesting observation</strong>: Silhouette suggests k=2, while Elbow and Davies-Bouldin correctly identify k=3. This demonstrates why <strong>using multiple methods is crucial</strong> - no single method is perfect!</p> <hr/> <h2 id="key-takeaways">Key Takeaways</h2> <p>After exploring these three foundational methods, here‚Äôs what you should remember:</p> <ol> <li><strong>No silver bullet exists</strong> - Different methods may suggest different k values</li> <li><strong>Always use multiple methods</strong> - Look for consensus across techniques</li> <li><strong>Domain knowledge matters</strong> - Statistical methods should guide, not dictate</li> <li><strong>Visualize, visualize, visualize</strong> - Plots reveal patterns metrics might miss</li> <li><strong>Consider your data structure</strong> - Different methods suit different cluster shapes</li> </ol> <h3 id="whats-next">What‚Äôs Next?</h3> <p>In <strong>Part 2</strong>, we‚Äôll explore more sophisticated statistical methods:</p> <ul> <li><strong>Calinski-Harabasz Index</strong>: Variance ratio criterion for well-separated clusters</li> <li><strong>Gap Statistic</strong>: Comparing against random distributions</li> <li><strong>BIC/AIC</strong>: Model selection for Gaussian Mixture Models</li> </ul> <p>These methods provide additional perspectives and can resolve ambiguities when foundational methods disagree.</p> <hr/> <h2 id="questions-or-suggestions">Questions or Suggestions?</h2> <p>Found this helpful? Have questions about applying these methods to your specific use case? Drop a comment below or reach out on <a href="https://www.linkedin.com/in/randy-frans-fela?originalSubdomain=dk">LinkedIn</a>.</p> <p>In my day job at Jabra, I use these clustering techniques extensively for grouping perceptual audio quality metrics - if you‚Äôre working on similar problems in audio/video evaluation, I‚Äôd love to connect!</p> <p><strong>Coming up in Part 2</strong>: We‚Äôll tackle the Gap Statistic, one of the most statistically rigorous methods, and explore how BIC/AIC can help when you‚Äôre using Gaussian Mixture Models. Stay tuned! üéØ</p> <hr/> <p><em>Tags: #clustering #machinelearning #datascience #python #unsupervisedlearning #statistics</em></p>]]></content><author><name></name></author><category term="tutorials"/><category term="machine-learning"/><category term="clustering"/><category term="machine-learning"/><category term="unsupervised-learning"/><category term="data-science"/><category term="python-programming"/><summary type="html"><![CDATA[Master the art of determining optimal clusters with Elbow Method, Silhouette Analysis, and Davies-Bouldin Index. A practical guide with Python implementations.]]></summary></entry><entry><title type="html">Statistical Design of Experiment 01: An Introduction</title><link href="https://fransfela.github.io/blog/2023/statistical-design-of-experiment-01-an-introduction/" rel="alternate" type="text/html" title="Statistical Design of Experiment 01: An Introduction"/><published>2023-09-17T22:25:37+00:00</published><updated>2023-09-17T22:25:37+00:00</updated><id>https://fransfela.github.io/blog/2023/statistical-design-of-experiment-01-an-introduction</id><content type="html" xml:base="https://fransfela.github.io/blog/2023/statistical-design-of-experiment-01-an-introduction/"><![CDATA[<p>The world of experimentation can be intricate, but fear not, for there‚Äôs a guiding light known as Statistical Design of Experiments (DoE). Imagine you‚Äôre on a quest to bake the perfect batch of cookies. You have various ingredients, each with multiple options. DoE is your trusty recipe book, helping you systematically test different combinations to discover the ideal¬†recipe.</p> <p>In this blog, we‚Äôll dive into the core principles of experimental design and explore how they apply to the world of perceptual audio evaluation. We‚Äôll keep it simple and straightforward.</p> <p>Think of experimental design as the roadmap that guides researchers in conducting experiments to assess audio quality. It helps ensure that the results are meaningful, free from bias, and applicable to real-world situations.</p> <h3>Principles of Experimental Design</h3> <p>Let‚Äôs delve into the essential principles of experimental design and see how they play out in perceptual audio evaluation:</p> <p><strong>1. Clear Objectives<br/></strong>Before starting an experiment, you need to know exactly what you want to find out. Every experiment should begin with a clear understanding of what you want to achieve. What‚Äôs the research question or hypothesis you‚Äôre testing? Without a well-defined objective, your experiment lacks direction.</p> <p>Every audio evaluation must begin with a clear objective. Your objective might be to determine whether a new audio processing algorithm improves sound quality compared to an existing one, assessing the sound quality of a new audio codec or fine-tuning the spatial characteristics of a speaker¬†system.</p> <p><strong>2. Randomization<br/></strong>Randomly assigning participants or test conditions helps prevent bias and ensures that your results are representative.</p> <p>Randomization ensures that listeners‚Äô experiences are not influenced by the order in which they hear audio samples. It‚Äôs like shuffling a playlist to prevent any bias from creeping into the evaluation.</p> <p><strong>3. Control Group<br/></strong>Having a control group provides a baseline for comparison. It helps you gauge the impact of the treatment or change you‚Äôre¬†testing.</p> <p>You might have a control group of listeners who hear the audio without any enhancements, while another group assesses the enhanced audio. This allows you to measure the improvement. Also, in audio evaluation, a control group often serves as the benchmark for comparison. It‚Äôs a reference point against which the quality of other audio samples is measured. For instance, when testing the effectiveness of noise reduction algorithms, the control group represents the unprocessed sound.</p> <p><strong>4. Replication<br/></strong>Repetition is the key to consistency. Repeating your experiments multiple times and obtaining consistent results increases your confidence in the findings. If different groups of listeners consistently rate the enhanced audio as better than the unenhanced audio, it strengthens the conclusion that the enhancement is effective.</p> <p><strong>5. Blocking<br/></strong>In some experiments, external factors called confounding variables, such as ambient noise, listener fatigue, or even the audio playback equipment, can influence the results. Blocking involves grouping subjects or samples with similar characteristics, helping control these variables‚Äô impact.</p> <p><strong>6. Blinding<br/></strong>Keeping certain information hidden from participants or experimenters minimizes bias. In a double-blind test, neither the listeners nor the experimenters know which audio sample is which, reducing the chance of biased¬†ratings.</p> <p><strong>7. Sample Size Determination<br/></strong>Determining the right number of listeners or samples is crucial for the statistical power of the evaluation. A small sample may not capture nuances, while an overly large one can be overwhelming and¬†costly.</p> <p><strong>8. Data Collection and Analysis Plan<br/></strong>Before starting your experiment, outline how you‚Äôll collect and analyze data. Having a pre-defined plan ensures objectivity and reduces the temptation to cherry-pick results.</p> <h3>Real-World Application in Perceptual Audio Evaluation</h3> <p>These principles aren‚Äôt just theoretical. In perceptual audio evaluation, they help researchers design experiments that assess the quality of audio processing algorithms, audio codecs, or audio equipment. By following these principles, researchers ensure that their findings are robust and meaningful. Following are a few examples:</p> <p><strong>Audio Codecs</strong>: Evaluating the quality of audio codecs to ensure crystal-clear sound transmission.</p> <p><strong>Speaker Tuning</strong>: Fine-tuning the spatial characteristics of speakers to create immersive audio experiences.</p> <p><strong>Noise Reduction Algorithms</strong>: Assessing the effectiveness of algorithms in reducing unwanted noise in audio recordings.</p> <p><strong>Voice Recognition Systems</strong>: Testing the accuracy and quality of voice recognition systems for seamless communication.</p> <h3><strong>Important Notes</strong></h3> <p>Simplicity and Clarity in Experimental Design<br/>Experimental design doesn‚Äôt have to be complex or filled with jargon. In perceptual audio evaluation, these fundamental principles are the building blocks for conducting reliable experiments and drawing conclusions about audio quality. By keeping things simple and clear, researchers can make confident strides in improving audio experiences for all of¬†us.</p> <p><strong><em>Remember: The power of experimental design lies in its simplicity and its ability to provide meaningful insights.</em></strong></p> <p><strong>References:</strong></p> <p>Kirk, R. E. (2013). Experimental Design: Procedures for the Behavioral Sciences. SAGE Publications, Inc., <a href="https://doi.org/10.4135/9781483384733">https://doi.org/10.4135/9781483384733</a></p> <p>Box, G. E. P., Hunter, W. G., &amp; Hunter, J. S. (1978). Statistics for Experimenters: An Introduction to Design, Data Analysis, and Model Building. Wiley.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=98394af6c74c" width="1" height="1" alt=""/></p>]]></content><author><name></name></author><category term="external-posts"/><category term="medium"/></entry><entry><title type="html">FFmpeg: Apa Itu dan Bagaimana Cara Menginstalnya di Mesin Anda</title><link href="https://fransfela.github.io/blog/2023/ffmpeg-apa-itu-dan-bagaimana-cara-menginstalnya-di-mesin-anda/" rel="alternate" type="text/html" title="FFmpeg: Apa Itu dan Bagaimana Cara Menginstalnya di Mesin Anda"/><published>2023-09-17T22:06:35+00:00</published><updated>2023-09-17T22:06:35+00:00</updated><id>https://fransfela.github.io/blog/2023/ffmpeg-apa-itu-dan-bagaimana-cara-menginstalnya-di-mesin-anda</id><content type="html" xml:base="https://fransfela.github.io/blog/2023/ffmpeg-apa-itu-dan-bagaimana-cara-menginstalnya-di-mesin-anda/"><![CDATA[<p>Pernahkah kamu merasa kesal ketika mencoba memutar file video atau audio di komputermu? Rasanya bikin frustasi, ya kan? Tenang saja, ada alat yang bisa membantu mengatasi semua masalah multimedia, dan itu adalah FFmpeg. Di blog ini, saya akan membawa kamu menjelajahi dunia yang menarik dari FFmpeg, menjelaskan apa itu, apa yang bisa dilakukan, bagaimana ia berbeda dari alat serupa, dan bahkan cara menggunakannya. Mari kita¬†mulai!</p> <h3>Apa Sih FFmpeg¬†Itu?</h3> <p>Pada dasarnya, FFmpeg adalah seperti sihir dalam dunia multimedia. Ini adalah perangkat lunak <em>open source</em> yang mampu menangani berbagai tugas multimedia yang kompleks. FFmpeg digunakan melalui baris perintah, jadi kamu akan berinteraksi dengannya dengan mengetik perintah ke dalam terminal atau command¬†prompt.</p> <p>Jangan khawatir jika kamu tidak terlalu suka dengan baris perintah. Setelah kamu familiar, FFmpeg akan menjadi teman setiamu dalam hal audio dan video. Bahkan, jika kamu lebih suka menggunakan Python dan Visual Studio Code, FFmpeg bisa dengan mudah diintegrasikan ke dalam alat-alat yang kamu gunakan sehari-hari. Kami akan bahas ini lebih lanjut nanti di posting¬†lainnya.</p> <h3>Apa yang Bisa Dilakukan dengan¬†FFmpeg?</h3> <p>Nah, apa saja yang bisa kamu lakukan dengan FFmpeg? Jawabannya: hampir segalanya yang berhubungan dengan multimedia! Berikut beberapa keajaibannya:</p> <p>- <strong>Konversi Format</strong>: FFmpeg bisa mengubah format file audio dan video dari satu ke yang lain. Mau mengubah video MKV menjadi MP4? FFmpeg punya solusinya.</p> <p>- <strong>Pengeditan Video dan Audio</strong>: Kamu bisa memotong, memotong, mengubah ukuran, dan menggabungkan klip video. Selain itu, ia mengelola audio seperti seorang ahli, memungkinkan kamu memotong, mencampur, dan menerapkan filter pada trek¬†audio.</p> <p>- <strong>Streaming</strong>: FFmpeg adalah favorit di kalangan pecinta streaming. Ini bisa mengkodekan dan streaming audio serta video ke berbagai platform, menjadikannya landasan penyiaran video¬†online.</p> <p>- <strong>Perekaman</strong>: Ingin merekam layar atau webcammu? FFmpeg juga bisa melakukannya! Ini sangat berguna untuk membuat tutorial, video permainan, atau siaran langsung.</p> <p>- <strong>Transcoding</strong>: FFmpeg bisa mengubah parameter enkoding file multimedia, mengoptimalkannya untuk berbagai perangkat atau kecepatan internet.</p> <h3>Apa yang Membuat FFmpeg Istimewa?</h3> <p>Mungkin kamu bertanya-tanya, ‚ÄúEh, tapi kan ada banyak alat multimedia lain di luar sana. Apa yang bikin FFmpeg spesial?‚Äù Pertanyaan yang¬†bagus!</p> <p>- <strong><em>Open Source</em></strong>: FFmpeg adalah perangkat lunak <em>open source</em>, artinya siapa pun bisa menggunakannya, memodifikasinya, dan berkontribusi pada pengembangannya. Keterbukaan ini telah menciptakan komunitas yang aktif dan perkembangan yang berkelanjutan.</p> <p>- <strong>Dukungan Format Luas</strong>: FFmpeg mendukung berbagai format multimedia, menjadikannya salah satu alat yang paling serbaguna.</p> <p>- <strong>Skrip dan Otomatisasi</strong>: Antarmuka baris perintah FFmpeg memungkinkan untuk membuat skrip dan otomatisasi, sehingga cocok untuk pemrosesan dalam jumlah besar dan integrasi dengan perangkat lunak¬†lain.</p> <p>- <strong>Platform Independen</strong>: FFmpeg dapat digunakan di berbagai platform, termasuk Windows, macOS, dan berbagai distribusi Linux, sehingga kamu tidak akan terbatas oleh sistem operasi yang kamu¬†gunakan.</p> <h3>Siap Menggali Dunia¬†FFmpeg?</h3> <p>Apakah kamu siap untuk menjelajahi dunia FFmpeg? Apakah kamu seorang pencipta multimedia pemula, penggemar teknologi, atau hanya ingin menjelajahi keajaiban manipulasi multimedia? FFmpeg adalah alat yang siap membantu. Di blog ini, saya akan memandu kamu melalui langkah-langkah instalasi FFmpeg di Windows dan Mac, sehingga kamu siap untuk memulai petualangan dalam dunia audio dan¬†video!</p> <h3>Instalasi FFmpeg di¬†Windows</h3> <p><strong>Langkah 1: Mengunduh FFmpeg<br/></strong>1. Buka <a href="https://www.ffmpeg.org/download.html">situs web resmi FFmpeg</a><br/>2. Gulir ke bawah ke bagian ‚ÄúWindows‚Äù dan klik tautan yang bertuliskan ‚ÄúWindows Builds by shiro‚Äù (atau pilihan lain yang sesuai).<br/>3. Kamu akan diarahkan ke halaman unduhan dengan beberapa versi yang tersedia. Pilih yang sesuai dengan kebutuhanmu. Jika tidak yakin, versi ‚ÄúLatest Git‚Äù biasanya aman.<br/>4. Unduh file ZIP sesuai dengan arsitektur sistemmu (32-bit atau 64-bit). Jika ragu, pilih versi 64-bit untuk performa yang lebih¬†baik.</p> <p><strong>Langkah 2: Mengekstrak FFmpeg<br/></strong>1. Setelah selesai mengunduh, buka lokasi di mana kamu menyimpan file ZIP.<br/>2. Klik kanan pada file ZIP dan pilih ‚ÄúEkstrak Semua‚Ä¶‚Äù untuk mengekstrak isinya.<br/>3. Pilih folder tujuan untuk file yang diekstrak dan klik ‚ÄúEkstrak.‚Äù</p> <p><strong>Langkah 3: Menambahkan FFmpeg ke PATH<br/></strong>1. Sekarang FFmpeg sudah diekstrak, kamu perlu menambahkannya ke PATH sistemmu agar bisa digunakan melalui baris perintah.<br/>2. Cari ‚ÄúVariabel Lingkungan‚Äù di pencarian Windows dan klik ‚ÄúEdit variabel lingkungan sistem.‚Äù<br/>3. Di jendela ‚ÄúProperti Sistem,‚Äù klik tombol ‚ÄúVariabel Lingkungan‚Ä¶‚Äù di bagian bawah.<br/>4. Di bagian ‚ÄúVariabel Sistem,‚Äù temukan dan pilih ‚ÄúPath,‚Äù lalu klik ‚ÄúEdit.‚Äù<br/>5. Klik ‚ÄúTambahkan‚Äù dan tambahkan jalur menuju folder ‚Äúbin‚Äù dari file FFmpeg yang sudah diekstrak (misalnya, C:\path\to\ffmpeg\bin). Klik ‚ÄúOK‚Äù untuk menyimpan perubahan.<br/>6. Klik ‚ÄúOK‚Äù untuk menutup semua¬†jendela.</p> <p><strong>Langkah 4: Memeriksa Instalasi<br/></strong>1. Buka command prompt (cari ‚Äúcmd‚Äù di pencarian Windows).<br/>2. Ketik `ffmpeg -version` dan tekan Enter. Kamu akan melihat informasi tentang instalasi FFmpeg-mu, yang menandakan bahwa instalasi berhasil.</p> <h3>Instalasi FFmpeg di¬†Mac</h3> <p><strong>Langkah 1: Menggunakan Homebrew (Disarankan)<br/></strong>1. Buka aplikasi Terminal. Kamu bisa menemukannya di folder ‚ÄúUtilitas‚Äù dalam folder ‚ÄúAplikasi.‚Äù<br/>2. Jika kamu belum memiliki Homebrew, kamu bisa menginstalnya dengan menjalankan perintah¬†berikut:</p> <pre>/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot;<br />brew install ffmpeg</pre> <p>3. Setelah Homebrew terinstal, kamu bisa dengan mudah menginstal FFmpeg dengan perintah¬†berikut:</p> <pre>brew install ffmpeg</pre> <p><strong>Langkah 2: Memeriksa Instalasi<br/></strong>Setelah instalasi selesai, kamu bisa memeriksanya dengan menjalankan:</p> <pre>ffmpeg -version</pre> <p>Kamu akan melihat informasi tentang instalasi FFmpeg-mu, menandakan bahwa kamu sudah siap untuk menggunakannya.</p> <h3>Siap untuk Menjelajah!</h3> <p>Selamat! Kamu sudah berhasil menginstal FFmpeg di Windows dan Mac-mu. Sekarang, kamu punya alat multimedia serba bisa yang dapat menangani berbagai tugas audio dan video. Apakah kamu sedang mengedit video, mengonversi format audio, atau hanya ingin menjelajahi dunia multimedia, FFmpeg akan selalu ada¬†untukmu.</p> <p>Di posting blog berikutnya, kita akan mulai menjelajahi dunia manipulasi multimedia. Tapi jangan hanya menunggu saya! Lanjutkan, cari referensi lain, dan mulai bereksperimen!</p> <p><em>Catatan/Saran: Selalu patuhi hukum hak cipta dan ketentuan penggunaan saat bekerja dengan file multimedia.</em></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=17590374a7b" width="1" height="1" alt=""/></p>]]></content><author><name></name></author><category term="external-posts"/><category term="medium"/></entry><entry><title type="html">FFmpeg: What it is and how to install it on your machine</title><link href="https://fransfela.github.io/blog/2023/ffmpeg-what-it-is-and-how-to-install-it-on-your-machine/" rel="alternate" type="text/html" title="FFmpeg: What it is and how to install it on your machine"/><published>2023-09-09T23:18:04+00:00</published><updated>2023-09-09T23:18:04+00:00</updated><id>https://fransfela.github.io/blog/2023/ffmpeg-what-it-is-and-how-to-install-it-on-your-machine</id><content type="html" xml:base="https://fransfela.github.io/blog/2023/ffmpeg-what-it-is-and-how-to-install-it-on-your-machine/"><![CDATA[<p>Have you ever encountered a video or audio file that refused to play on your device? Frustrating, isn‚Äôt it? Well, fear not, for there‚Äôs a versatile and powerful tool that can help you conquer such multimedia challenges: FFmpeg. In this blog, I‚Äôll take you on a journey through the wonderful world of FFmpeg, explaining what it is, what it‚Äôs used for, how it differs from other similar software, and even how to harness its¬†magic.</p> <h3><strong>What Is¬†FFmpeg?</strong></h3> <p>At its core, FFmpeg is like the wizard of multimedia processing. It‚Äôs an open-source software suite that handles a mind-boggling array of multimedia tasks. FFmpeg is a command-line tool, which means you interact with it by typing commands into a terminal or command¬†prompt.</p> <p>But don‚Äôt let the command-line aspect scare you away. Once you get the hang of it, FFmpeg becomes your go-to companion for anything related to audio and video manipulation. Also, don‚Äôt get me wrong, if your favorite utensil is Python and your kitchen is Visual Studio Code, FFmpeg can be integrated into your daily tools too. We will talk about it later under this post category.</p> <h3><strong>What Is It Used¬†For?</strong></h3> <p>So, what can you actually do with FFmpeg? The answer is: almost anything related to multimedia! Here are just a few of its superpowers:</p> <ol><li>Format Conversion: FFmpeg can convert audio and video files from one format to another. Need that MKV video as an MP4? FFmpeg‚Äôs got you¬†covered.</li><li>Video and Audio Editing: It can trim, crop, resize, and concatenate video clips. Plus, it handles audio like a pro, letting you cut, mix, and apply filters to audio¬†tracks.</li><li>Streaming: FFmpeg is a favorite among streaming enthusiasts. It can encode and stream audio and video to various platforms, making it a cornerstone of online video broadcasting.</li><li>Recording: Want to capture your screen or record your webcam? FFmpeg can do that too! It‚Äôs perfect for creating tutorials, gameplay videos, or live¬†streams.</li><li>Transcoding: FFmpeg can change the encoding parameters of multimedia files, optimizing them for different devices or bandwidths.</li></ol> <h3><strong>How Is It Different from Other Software?</strong></h3> <p>Now, you might be thinking, ‚ÄúThere are other multimedia tools out there. What makes FFmpeg special?‚Äù Great question!</p> <ol><li>Open Source: FFmpeg is free and open-source, which means anyone can use, modify, and contribute to it. This openness has led to a vibrant community and continuous development.</li><li>Wide Range of Formats: FFmpeg supports an extensive list of multimedia formats, making it one of the most versatile tools available.</li><li>Scripting and Automation: FFmpeg‚Äôs command-line interface allows for scripting and automation, making it an excellent choice for batch processing and integration into other software.</li><li>Platform Independence: FFmpeg works on multiple platforms, including Windows, macOS, and various Linux distributions, ensuring you‚Äôre not limited by your choice of operating system.</li></ol> <p>Are you ready to unlock the powerful world of FFmpeg? Whether you‚Äôre a budding multimedia creator, a tech enthusiast, or simply looking to explore the magic of multimedia manipulation, FFmpeg is your go-to tool. In this blog, I‚Äôll walk you through the step-by-step process of installing FFmpeg on both Windows and Mac, making sure you‚Äôre equipped to dive into audio and video wizardry!</p> <h3><strong>Installing FFmpeg on¬†Windows</strong></h3> <h4>Step 1: Downloading FFmpeg</h4> <ul><li>Go to the official FFmpeg website at <a href="https://www.ffmpeg.org/download.html">https://www.ffmpeg.org/download.html</a>.</li><li>Scroll down to the ‚ÄúWindows‚Äù section and click on the link that says ‚ÄúWindows Builds by shiro‚Äù (or any other suitable¬†option).</li><li>You‚Äôll be redirected to a download page with several versions available. Choose the one that suits your needs. If you‚Äôre not sure, the ‚ÄúLatest Git‚Äù version is usually a safe¬†bet.</li><li>Download the ZIP file corresponding to your system architecture (32-bit or 64-bit). If you‚Äôre unsure, go with the 64-bit version for better performance.</li></ul> <h4>Step 2: Extracting FFmpeg</h4> <ul><li>Once the download is complete, navigate to the location where you saved the ZIP¬†file.</li><li>Right-click on the ZIP file and select ‚ÄúExtract All‚Ä¶‚Äù to unzip the contents.</li><li>Choose a destination folder for the extracted files and click ‚ÄúExtract.‚Äù</li></ul> <h4>Step 3: Adding FFmpeg to¬†PATH</h4> <ul><li>Now that FFmpeg is extracted, you need to add it to your system‚Äôs PATH so that you can use it from the command¬†line.</li><li>Search for ‚ÄúEnvironment Variables‚Äù in the Windows search bar and click ‚ÄúEdit the system environment variables.‚Äù</li><li>In the ‚ÄúSystem Properties‚Äù window, click the ‚ÄúEnvironment Variables‚Ä¶‚Äù button at the¬†bottom.</li><li>In the ‚ÄúSystem Variables‚Äù section, find and select ‚ÄúPath,‚Äù then click¬†Edit.‚Äù</li><li>Click ‚ÄúNew‚Äù and add the path to the ‚Äúbin‚Äù folder of the extracted FFmpeg files (e.g., C:\path\to\ffmpeg\bin). Click ‚ÄúOK‚Äù to save the¬†changes.</li><li>Click ‚ÄúOK‚Äù to close all the¬†windows.</li></ul> <h4>Step 4: Verifying the Installation</h4> <ul><li>Open the command prompt (you can search for ‚Äúcmd‚Äù in the Windows search¬†bar).</li><li>Type ffmpeg -version and press Enter. You should see information about your FFmpeg installation, confirming that it‚Äôs successfully installed.</li></ul> <h3>Installing FFmpeg on¬†Mac</h3> <h4>Step 1: Using Homebrew (Recommended)</h4> <ul><li>Open the Terminal app. You can find it in the ‚ÄúUtilities‚Äù folder within the ‚ÄúApplications‚Äù folder.</li><li>If you don‚Äôt have Homebrew installed, you can install it by running the following command:</li></ul> <pre>/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot;<br />brew install ffmpeg</pre> <ul><li>Once Homebrew is installed, you can easily install FFmpeg by running the following command:</li></ul> <h4>Step 2: Verifying the Installation</h4> <ul><li>After the installation is complete, you can verify it by¬†running:</li></ul> <pre>ffmpeg -version</pre> <ul><li>You should see information about your FFmpeg installation, indicating that it‚Äôs ready to¬†use.</li></ul> <h3>Ready to¬†explore!</h3> <p>Congratulations! You‚Äôve successfully installed FFmpeg on both your Windows and Mac machines. Now you‚Äôre armed with a versatile multimedia tool that can handle a wide range of audio and video tasks. Whether you‚Äôre editing videos, converting audio formats, or exploring the world of multimedia, FFmpeg has got your¬†back.</p> <p>In the next blog post, we will start exploring the world of multimedia manipulation. But don‚Äôt only wait for me! Go ahead yourself, find other references, and start experimenting!</p> <p><em>Note (or actually my wise advice): Make sure to respect copyright laws and terms of use when working with multimedia files.</em></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=fb589318d117" width="1" height="1" alt=""/></p>]]></content><author><name></name></author><category term="external-posts"/><category term="medium"/></entry><entry><title type="html">a post with table of contents on a sidebar</title><link href="https://fransfela.github.io/blog/2023/sidebar-table-of-contents/" rel="alternate" type="text/html" title="a post with table of contents on a sidebar"/><published>2023-04-25T14:14:00+00:00</published><updated>2023-04-25T14:14:00+00:00</updated><id>https://fransfela.github.io/blog/2023/sidebar-table-of-contents</id><content type="html" xml:base="https://fransfela.github.io/blog/2023/sidebar-table-of-contents/"><![CDATA[<p>This post shows how to add a table of contents as a sidebar.</p> <h2 id="adding-a-table-of-contents">Adding a Table of Contents</h2> <p>To add a table of contents to a post as a sidebar, simply add</p> <div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">toc</span><span class="pi">:</span>
  <span class="na">sidebar</span><span class="pi">:</span> <span class="s">left</span>
</code></pre></div></div> <p>to the front matter of the post. The table of contents will be automatically generated from the headings in the post. If you wish to display the sidebar to the right, simply change <code class="language-plaintext highlighter-rouge">left</code> to <code class="language-plaintext highlighter-rouge">right</code>.</p> <h3 id="example-of-sub-heading-1">Example of Sub-Heading 1</h3> <p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p> <h3 id="example-of-another-sub-heading-1">Example of another Sub-Heading 1</h3> <p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p> <h2 id="customizing-your-table-of-contents">Customizing Your Table of Contents</h2> <p data-toc-text="Customizing">If you want to learn more about how to customize the table of contents of your sidebar, you can check the <a href="https://afeld.github.io/bootstrap-toc/">bootstrap-toc</a> documentation. Notice that you can even customize the text of the heading that will be displayed on the sidebar.</p> <h3 id="example-of-sub-heading-2">Example of Sub-Heading 2</h3> <p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p> <h3 id="example-of-another-sub-heading-2">Example of another Sub-Heading 2</h3> <p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="toc"/><category term="sidebar"/><summary type="html"><![CDATA[an example of a blog post with table of contents on a sidebar]]></summary></entry><entry><title type="html">a post with audios</title><link href="https://fransfela.github.io/blog/2023/audios/" rel="alternate" type="text/html" title="a post with audios"/><published>2023-04-25T10:25:00+00:00</published><updated>2023-04-25T10:25:00+00:00</updated><id>https://fransfela.github.io/blog/2023/audios</id><content type="html" xml:base="https://fransfela.github.io/blog/2023/audios/"><![CDATA[<p>This is an example post with audios. It supports local audio files.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="/assets/audio/epicaly-short-113909.mp3" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="https://cdn.pixabay.com/download/audio/2022/06/25/audio_69a61cd6d6.mp3" controls=""/> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between video rows, after each row, or doesn't have to be there at all. </div>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="audios"/><summary type="html"><![CDATA[this is what included audios could look like]]></summary></entry></feed>